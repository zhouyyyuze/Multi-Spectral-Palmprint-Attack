[ INFO : 2022-07-26 19:30:06,418 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:30:06,419 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:30:07,904 ] - Epoch 1/500 - time: 1.49 - training_loss: -5.9944
[ INFO : 2022-07-26 19:30:07,948 ] - Epoch 2/500 - time: 0.04 - training_loss: -5.9922
[ INFO : 2022-07-26 19:30:07,989 ] - Epoch 3/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:08,029 ] - Epoch 4/500 - time: 0.04 - training_loss: -5.9917
[ INFO : 2022-07-26 19:30:08,073 ] - Epoch 5/500 - time: 0.04 - training_loss: -5.9912
[ INFO : 2022-07-26 19:30:08,113 ] - Epoch 6/500 - time: 0.04 - training_loss: -5.9911
[ INFO : 2022-07-26 19:30:08,157 ] - Epoch 7/500 - time: 0.04 - training_loss: -5.9917
[ INFO : 2022-07-26 19:30:08,200 ] - Epoch 8/500 - time: 0.04 - training_loss: -5.9914
[ INFO : 2022-07-26 19:30:08,238 ] - Epoch 9/500 - time: 0.04 - training_loss: -5.9911
[ INFO : 2022-07-26 19:30:08,276 ] - Epoch 10/500 - time: 0.04 - training_loss: -5.9906
[ INFO : 2022-07-26 19:30:08,315 ] - Epoch 11/500 - time: 0.04 - training_loss: -5.9908
[ INFO : 2022-07-26 19:30:08,357 ] - Epoch 12/500 - time: 0.04 - training_loss: -5.9910
[ INFO : 2022-07-26 19:30:08,395 ] - Epoch 13/500 - time: 0.04 - training_loss: -5.9914
[ INFO : 2022-07-26 19:30:08,435 ] - Epoch 14/500 - time: 0.04 - training_loss: -5.9917
[ INFO : 2022-07-26 19:30:08,473 ] - Epoch 15/500 - time: 0.04 - training_loss: -5.9919
[ INFO : 2022-07-26 19:30:08,513 ] - Epoch 16/500 - time: 0.04 - training_loss: -5.9919
[ INFO : 2022-07-26 19:30:08,555 ] - Epoch 17/500 - time: 0.04 - training_loss: -5.9923
[ INFO : 2022-07-26 19:30:08,596 ] - Epoch 18/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:30:08,639 ] - Epoch 19/500 - time: 0.04 - training_loss: -5.9925
[ INFO : 2022-07-26 19:30:08,678 ] - Epoch 20/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:30:08,722 ] - Epoch 21/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:30:08,763 ] - Epoch 22/500 - time: 0.04 - training_loss: -5.9925
[ INFO : 2022-07-26 19:30:08,803 ] - Epoch 23/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:30:08,842 ] - Epoch 24/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:30:08,880 ] - Epoch 25/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:08,921 ] - Epoch 26/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:08,959 ] - Epoch 27/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:08,998 ] - Epoch 28/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,036 ] - Epoch 29/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,078 ] - Epoch 30/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,118 ] - Epoch 31/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,156 ] - Epoch 32/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,198 ] - Epoch 33/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,237 ] - Epoch 34/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,278 ] - Epoch 35/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,324 ] - Epoch 36/500 - time: 0.05 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,368 ] - Epoch 37/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,405 ] - Epoch 38/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,446 ] - Epoch 39/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,485 ] - Epoch 40/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:09,525 ] - Epoch 41/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:09,564 ] - Epoch 42/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:09,602 ] - Epoch 43/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,641 ] - Epoch 44/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,679 ] - Epoch 45/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:09,717 ] - Epoch 46/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,758 ] - Epoch 47/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,800 ] - Epoch 48/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:09,839 ] - Epoch 49/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:09,877 ] - Epoch 50/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:09,916 ] - Epoch 51/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:09,957 ] - Epoch 52/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:09,998 ] - Epoch 53/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:10,037 ] - Epoch 54/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:10,077 ] - Epoch 55/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,116 ] - Epoch 56/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,153 ] - Epoch 57/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,193 ] - Epoch 58/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,232 ] - Epoch 59/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,270 ] - Epoch 60/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,309 ] - Epoch 61/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:10,348 ] - Epoch 62/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:10,386 ] - Epoch 63/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,427 ] - Epoch 64/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,470 ] - Epoch 65/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,511 ] - Epoch 66/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,555 ] - Epoch 67/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:10,593 ] - Epoch 68/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,633 ] - Epoch 69/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,673 ] - Epoch 70/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,716 ] - Epoch 71/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,756 ] - Epoch 72/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:10,794 ] - Epoch 73/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,835 ] - Epoch 74/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,877 ] - Epoch 75/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:10,917 ] - Epoch 76/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:30:10,958 ] - Epoch 77/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,000 ] - Epoch 78/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,039 ] - Epoch 79/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,077 ] - Epoch 80/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,115 ] - Epoch 81/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,153 ] - Epoch 82/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,193 ] - Epoch 83/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:30:11,234 ] - Epoch 84/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,273 ] - Epoch 85/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,315 ] - Epoch 86/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,357 ] - Epoch 87/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,400 ] - Epoch 88/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,440 ] - Epoch 89/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,487 ] - Epoch 90/500 - time: 0.05 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,529 ] - Epoch 91/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,570 ] - Epoch 92/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:11,611 ] - Epoch 93/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:11,651 ] - Epoch 94/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:11,695 ] - Epoch 95/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:11,739 ] - Epoch 96/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:11,777 ] - Epoch 97/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:11,815 ] - Epoch 98/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:11,853 ] - Epoch 99/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:11,891 ] - Epoch 100/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:11,929 ] - Epoch 101/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:11,968 ] - Epoch 102/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,007 ] - Epoch 103/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,048 ] - Epoch 104/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,085 ] - Epoch 105/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,126 ] - Epoch 106/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,167 ] - Epoch 107/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:12,211 ] - Epoch 108/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,251 ] - Epoch 109/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,291 ] - Epoch 110/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,330 ] - Epoch 111/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,370 ] - Epoch 112/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,412 ] - Epoch 113/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,452 ] - Epoch 114/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,491 ] - Epoch 115/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,532 ] - Epoch 116/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,572 ] - Epoch 117/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,611 ] - Epoch 118/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,649 ] - Epoch 119/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,690 ] - Epoch 120/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,729 ] - Epoch 121/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,772 ] - Epoch 122/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,812 ] - Epoch 123/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,854 ] - Epoch 124/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,896 ] - Epoch 125/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:30:12,937 ] - Epoch 126/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:12,979 ] - Epoch 127/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,020 ] - Epoch 128/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,060 ] - Epoch 129/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,100 ] - Epoch 130/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,142 ] - Epoch 131/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,180 ] - Epoch 132/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:30:13,222 ] - Epoch 133/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,260 ] - Epoch 134/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,301 ] - Epoch 135/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,339 ] - Epoch 136/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,381 ] - Epoch 137/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,420 ] - Epoch 138/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,459 ] - Epoch 139/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,500 ] - Epoch 140/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,541 ] - Epoch 141/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,581 ] - Epoch 142/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,622 ] - Epoch 143/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,662 ] - Epoch 144/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,701 ] - Epoch 145/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,741 ] - Epoch 146/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,782 ] - Epoch 147/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,822 ] - Epoch 148/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,865 ] - Epoch 149/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,904 ] - Epoch 150/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,945 ] - Epoch 151/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:13,985 ] - Epoch 152/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:14,025 ] - Epoch 153/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:14,064 ] - Epoch 154/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:14,102 ] - Epoch 155/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,143 ] - Epoch 156/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,185 ] - Epoch 157/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,224 ] - Epoch 158/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,266 ] - Epoch 159/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,307 ] - Epoch 160/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,348 ] - Epoch 161/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,388 ] - Epoch 162/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,429 ] - Epoch 163/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,470 ] - Epoch 164/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,517 ] - Epoch 165/500 - time: 0.05 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,555 ] - Epoch 166/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,594 ] - Epoch 167/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,637 ] - Epoch 168/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,675 ] - Epoch 169/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,721 ] - Epoch 170/500 - time: 0.05 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,760 ] - Epoch 171/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,801 ] - Epoch 172/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:14,840 ] - Epoch 173/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,882 ] - Epoch 174/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,923 ] - Epoch 175/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:14,964 ] - Epoch 176/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:15,004 ] - Epoch 177/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:15,045 ] - Epoch 178/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:15,085 ] - Epoch 179/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:15,127 ] - Epoch 180/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:15,172 ] - Epoch 181/500 - time: 0.05 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,213 ] - Epoch 182/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,252 ] - Epoch 183/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,294 ] - Epoch 184/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,333 ] - Epoch 185/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,374 ] - Epoch 186/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,414 ] - Epoch 187/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,452 ] - Epoch 188/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,489 ] - Epoch 189/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,529 ] - Epoch 190/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,570 ] - Epoch 191/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,613 ] - Epoch 192/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:15,650 ] - Epoch 193/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,689 ] - Epoch 194/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,727 ] - Epoch 195/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,770 ] - Epoch 196/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,810 ] - Epoch 197/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,849 ] - Epoch 198/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,892 ] - Epoch 199/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,931 ] - Epoch 200/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:15,968 ] - Epoch 201/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:16,007 ] - Epoch 202/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:16,047 ] - Epoch 203/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,086 ] - Epoch 204/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,125 ] - Epoch 205/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,164 ] - Epoch 206/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:30:16,204 ] - Epoch 207/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,244 ] - Epoch 208/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,285 ] - Epoch 209/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,326 ] - Epoch 210/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,364 ] - Epoch 211/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,404 ] - Epoch 212/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,442 ] - Epoch 213/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,482 ] - Epoch 214/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,520 ] - Epoch 215/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,561 ] - Epoch 216/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:16,604 ] - Epoch 217/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,647 ] - Epoch 218/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,689 ] - Epoch 219/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,728 ] - Epoch 220/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,767 ] - Epoch 221/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,809 ] - Epoch 222/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,848 ] - Epoch 223/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,888 ] - Epoch 224/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,929 ] - Epoch 225/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:16,971 ] - Epoch 226/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,009 ] - Epoch 227/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,048 ] - Epoch 228/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,088 ] - Epoch 229/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,131 ] - Epoch 230/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,170 ] - Epoch 231/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,211 ] - Epoch 232/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,253 ] - Epoch 233/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,294 ] - Epoch 234/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,335 ] - Epoch 235/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,374 ] - Epoch 236/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,413 ] - Epoch 237/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:17,452 ] - Epoch 238/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,492 ] - Epoch 239/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,532 ] - Epoch 240/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,572 ] - Epoch 241/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:17,611 ] - Epoch 242/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,650 ] - Epoch 243/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,690 ] - Epoch 244/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,728 ] - Epoch 245/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,769 ] - Epoch 246/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,818 ] - Epoch 247/500 - time: 0.05 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,857 ] - Epoch 248/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,897 ] - Epoch 249/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,947 ] - Epoch 250/500 - time: 0.05 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:17,986 ] - Epoch 251/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,027 ] - Epoch 252/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,069 ] - Epoch 253/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,112 ] - Epoch 254/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,154 ] - Epoch 255/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,193 ] - Epoch 256/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,234 ] - Epoch 257/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,278 ] - Epoch 258/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,318 ] - Epoch 259/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,358 ] - Epoch 260/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,399 ] - Epoch 261/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,440 ] - Epoch 262/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,481 ] - Epoch 263/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,520 ] - Epoch 264/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,561 ] - Epoch 265/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,603 ] - Epoch 266/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:30:18,646 ] - Epoch 267/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,686 ] - Epoch 268/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,733 ] - Epoch 269/500 - time: 0.05 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,772 ] - Epoch 270/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,812 ] - Epoch 271/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,851 ] - Epoch 272/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,889 ] - Epoch 273/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,926 ] - Epoch 274/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:18,966 ] - Epoch 275/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,005 ] - Epoch 276/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,045 ] - Epoch 277/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,084 ] - Epoch 278/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,126 ] - Epoch 279/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,167 ] - Epoch 280/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:30:19,211 ] - Epoch 281/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,257 ] - Epoch 282/500 - time: 0.05 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,306 ] - Epoch 283/500 - time: 0.05 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,352 ] - Epoch 284/500 - time: 0.05 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,393 ] - Epoch 285/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,435 ] - Epoch 286/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,477 ] - Epoch 287/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,518 ] - Epoch 288/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,557 ] - Epoch 289/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,595 ] - Epoch 290/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,631 ] - Epoch 291/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,668 ] - Epoch 292/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,706 ] - Epoch 293/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,743 ] - Epoch 294/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,782 ] - Epoch 295/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,819 ] - Epoch 296/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,855 ] - Epoch 297/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,894 ] - Epoch 298/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,931 ] - Epoch 299/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:19,970 ] - Epoch 300/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:30:20,008 ] - Epoch 301/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,047 ] - Epoch 302/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,083 ] - Epoch 303/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,121 ] - Epoch 304/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,158 ] - Epoch 305/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,196 ] - Epoch 306/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,235 ] - Epoch 307/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,273 ] - Epoch 308/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,312 ] - Epoch 309/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,352 ] - Epoch 310/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,390 ] - Epoch 311/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,432 ] - Epoch 312/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,470 ] - Epoch 313/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,507 ] - Epoch 314/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,545 ] - Epoch 315/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,583 ] - Epoch 316/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,621 ] - Epoch 317/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,660 ] - Epoch 318/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,697 ] - Epoch 319/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:30:20,735 ] - Epoch 320/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,772 ] - Epoch 321/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,808 ] - Epoch 322/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,846 ] - Epoch 323/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,887 ] - Epoch 324/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,926 ] - Epoch 325/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:20,966 ] - Epoch 326/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,005 ] - Epoch 327/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,045 ] - Epoch 328/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,084 ] - Epoch 329/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,122 ] - Epoch 330/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,161 ] - Epoch 331/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,199 ] - Epoch 332/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,239 ] - Epoch 333/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,279 ] - Epoch 334/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,318 ] - Epoch 335/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,359 ] - Epoch 336/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,399 ] - Epoch 337/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,438 ] - Epoch 338/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,477 ] - Epoch 339/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,515 ] - Epoch 340/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,552 ] - Epoch 341/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,590 ] - Epoch 342/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,628 ] - Epoch 343/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,668 ] - Epoch 344/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,706 ] - Epoch 345/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,747 ] - Epoch 346/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,787 ] - Epoch 347/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,826 ] - Epoch 348/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,866 ] - Epoch 349/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,906 ] - Epoch 350/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,945 ] - Epoch 351/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:21,985 ] - Epoch 352/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,023 ] - Epoch 353/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,063 ] - Epoch 354/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,101 ] - Epoch 355/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,139 ] - Epoch 356/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,177 ] - Epoch 357/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,216 ] - Epoch 358/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,254 ] - Epoch 359/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,291 ] - Epoch 360/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,331 ] - Epoch 361/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,368 ] - Epoch 362/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,408 ] - Epoch 363/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,447 ] - Epoch 364/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,486 ] - Epoch 365/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,525 ] - Epoch 366/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,563 ] - Epoch 367/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,600 ] - Epoch 368/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,639 ] - Epoch 369/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,676 ] - Epoch 370/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,713 ] - Epoch 371/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,751 ] - Epoch 372/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,788 ] - Epoch 373/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,825 ] - Epoch 374/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,866 ] - Epoch 375/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,904 ] - Epoch 376/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,942 ] - Epoch 377/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:22,982 ] - Epoch 378/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,019 ] - Epoch 379/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,057 ] - Epoch 380/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,094 ] - Epoch 381/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,130 ] - Epoch 382/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,168 ] - Epoch 383/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,207 ] - Epoch 384/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,245 ] - Epoch 385/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,282 ] - Epoch 386/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,318 ] - Epoch 387/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,355 ] - Epoch 388/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,392 ] - Epoch 389/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,433 ] - Epoch 390/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,472 ] - Epoch 391/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,509 ] - Epoch 392/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,546 ] - Epoch 393/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,582 ] - Epoch 394/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,619 ] - Epoch 395/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:30:23,658 ] - Epoch 396/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,696 ] - Epoch 397/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,733 ] - Epoch 398/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,770 ] - Epoch 399/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,807 ] - Epoch 400/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,844 ] - Epoch 401/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,882 ] - Epoch 402/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,921 ] - Epoch 403/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,959 ] - Epoch 404/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:23,995 ] - Epoch 405/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,033 ] - Epoch 406/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,072 ] - Epoch 407/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,110 ] - Epoch 408/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,148 ] - Epoch 409/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,186 ] - Epoch 410/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,222 ] - Epoch 411/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,259 ] - Epoch 412/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,297 ] - Epoch 413/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,338 ] - Epoch 414/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,375 ] - Epoch 415/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,412 ] - Epoch 416/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,449 ] - Epoch 417/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,488 ] - Epoch 418/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,525 ] - Epoch 419/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,563 ] - Epoch 420/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,600 ] - Epoch 421/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,638 ] - Epoch 422/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,675 ] - Epoch 423/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,713 ] - Epoch 424/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,752 ] - Epoch 425/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,791 ] - Epoch 426/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,829 ] - Epoch 427/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,866 ] - Epoch 428/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,904 ] - Epoch 429/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,942 ] - Epoch 430/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:24,980 ] - Epoch 431/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:30:25,018 ] - Epoch 432/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,056 ] - Epoch 433/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,093 ] - Epoch 434/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,129 ] - Epoch 435/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,166 ] - Epoch 436/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,204 ] - Epoch 437/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,241 ] - Epoch 438/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,281 ] - Epoch 439/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,317 ] - Epoch 440/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,353 ] - Epoch 441/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,390 ] - Epoch 442/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,429 ] - Epoch 443/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,467 ] - Epoch 444/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,508 ] - Epoch 445/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,549 ] - Epoch 446/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,588 ] - Epoch 447/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:30:25,626 ] - Epoch 448/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,664 ] - Epoch 449/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,703 ] - Epoch 450/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,740 ] - Epoch 451/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,778 ] - Epoch 452/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,815 ] - Epoch 453/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,852 ] - Epoch 454/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,892 ] - Epoch 455/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,930 ] - Epoch 456/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:25,966 ] - Epoch 457/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,004 ] - Epoch 458/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,041 ] - Epoch 459/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,078 ] - Epoch 460/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,114 ] - Epoch 461/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,152 ] - Epoch 462/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,191 ] - Epoch 463/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,228 ] - Epoch 464/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,267 ] - Epoch 465/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,305 ] - Epoch 466/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,344 ] - Epoch 467/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,384 ] - Epoch 468/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,422 ] - Epoch 469/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,458 ] - Epoch 470/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,494 ] - Epoch 471/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,531 ] - Epoch 472/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,577 ] - Epoch 473/500 - time: 0.05 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,615 ] - Epoch 474/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,652 ] - Epoch 475/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,690 ] - Epoch 476/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,728 ] - Epoch 477/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,767 ] - Epoch 478/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,804 ] - Epoch 479/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,841 ] - Epoch 480/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,880 ] - Epoch 481/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,916 ] - Epoch 482/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,953 ] - Epoch 483/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:26,992 ] - Epoch 484/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,031 ] - Epoch 485/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,070 ] - Epoch 486/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,108 ] - Epoch 487/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,145 ] - Epoch 488/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,185 ] - Epoch 489/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,224 ] - Epoch 490/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,262 ] - Epoch 491/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,301 ] - Epoch 492/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,338 ] - Epoch 493/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,375 ] - Epoch 494/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,411 ] - Epoch 495/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,448 ] - Epoch 496/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,485 ] - Epoch 497/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,524 ] - Epoch 498/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,566 ] - Epoch 499/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:30:27,604 ] - Epoch 500/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:06,468 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:31:06,468 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:31:07,859 ] - Epoch 1/500 - time: 1.39 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:07,901 ] - Epoch 2/500 - time: 0.04 - training_loss: -5.9878
[ INFO : 2022-07-26 19:31:07,940 ] - Epoch 3/500 - time: 0.04 - training_loss: -5.9881
[ INFO : 2022-07-26 19:31:07,982 ] - Epoch 4/500 - time: 0.04 - training_loss: -5.9871
[ INFO : 2022-07-26 19:31:08,023 ] - Epoch 5/500 - time: 0.04 - training_loss: -5.9883
[ INFO : 2022-07-26 19:31:08,063 ] - Epoch 6/500 - time: 0.04 - training_loss: -5.9887
[ INFO : 2022-07-26 19:31:08,104 ] - Epoch 7/500 - time: 0.04 - training_loss: -5.9894
[ INFO : 2022-07-26 19:31:08,145 ] - Epoch 8/500 - time: 0.04 - training_loss: -5.9899
[ INFO : 2022-07-26 19:31:08,189 ] - Epoch 9/500 - time: 0.04 - training_loss: -5.9905
[ INFO : 2022-07-26 19:31:08,228 ] - Epoch 10/500 - time: 0.04 - training_loss: -5.9911
[ INFO : 2022-07-26 19:31:08,268 ] - Epoch 11/500 - time: 0.04 - training_loss: -5.9915
[ INFO : 2022-07-26 19:31:08,305 ] - Epoch 12/500 - time: 0.04 - training_loss: -5.9916
[ INFO : 2022-07-26 19:31:08,346 ] - Epoch 13/500 - time: 0.04 - training_loss: -5.9918
[ INFO : 2022-07-26 19:31:08,384 ] - Epoch 14/500 - time: 0.04 - training_loss: -5.9917
[ INFO : 2022-07-26 19:31:08,422 ] - Epoch 15/500 - time: 0.04 - training_loss: -5.9919
[ INFO : 2022-07-26 19:31:08,460 ] - Epoch 16/500 - time: 0.04 - training_loss: -5.9922
[ INFO : 2022-07-26 19:31:08,497 ] - Epoch 17/500 - time: 0.04 - training_loss: -5.9923
[ INFO : 2022-07-26 19:31:08,536 ] - Epoch 18/500 - time: 0.04 - training_loss: -5.9926
[ INFO : 2022-07-26 19:31:08,575 ] - Epoch 19/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:31:08,615 ] - Epoch 20/500 - time: 0.04 - training_loss: -5.9927
[ INFO : 2022-07-26 19:31:08,655 ] - Epoch 21/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:31:08,694 ] - Epoch 22/500 - time: 0.04 - training_loss: -5.9929
[ INFO : 2022-07-26 19:31:08,735 ] - Epoch 23/500 - time: 0.04 - training_loss: -5.9928
[ INFO : 2022-07-26 19:31:08,772 ] - Epoch 24/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:31:08,810 ] - Epoch 25/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:31:08,849 ] - Epoch 26/500 - time: 0.04 - training_loss: -5.9930
[ INFO : 2022-07-26 19:31:08,889 ] - Epoch 27/500 - time: 0.04 - training_loss: -5.9931
[ INFO : 2022-07-26 19:31:08,927 ] - Epoch 28/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:31:08,969 ] - Epoch 29/500 - time: 0.04 - training_loss: -5.9932
[ INFO : 2022-07-26 19:31:09,006 ] - Epoch 30/500 - time: 0.04 - training_loss: -5.9933
[ INFO : 2022-07-26 19:31:09,045 ] - Epoch 31/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:31:09,085 ] - Epoch 32/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:31:09,128 ] - Epoch 33/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:31:09,166 ] - Epoch 34/500 - time: 0.04 - training_loss: -5.9934
[ INFO : 2022-07-26 19:31:09,204 ] - Epoch 35/500 - time: 0.04 - training_loss: -5.9935
[ INFO : 2022-07-26 19:31:09,244 ] - Epoch 36/500 - time: 0.04 - training_loss: -5.9936
[ INFO : 2022-07-26 19:31:09,282 ] - Epoch 37/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:31:09,320 ] - Epoch 38/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:31:09,358 ] - Epoch 39/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,396 ] - Epoch 40/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:31:09,434 ] - Epoch 41/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,473 ] - Epoch 42/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,513 ] - Epoch 43/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,554 ] - Epoch 44/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,593 ] - Epoch 45/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,636 ] - Epoch 46/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,674 ] - Epoch 47/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:09,712 ] - Epoch 48/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,750 ] - Epoch 49/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,790 ] - Epoch 50/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,827 ] - Epoch 51/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,869 ] - Epoch 52/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:09,906 ] - Epoch 53/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,946 ] - Epoch 54/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:09,991 ] - Epoch 55/500 - time: 0.05 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,029 ] - Epoch 56/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,069 ] - Epoch 57/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,108 ] - Epoch 58/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,148 ] - Epoch 59/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,189 ] - Epoch 60/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,228 ] - Epoch 61/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,269 ] - Epoch 62/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,309 ] - Epoch 63/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,346 ] - Epoch 64/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,383 ] - Epoch 65/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,421 ] - Epoch 66/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,458 ] - Epoch 67/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,502 ] - Epoch 68/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,541 ] - Epoch 69/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,578 ] - Epoch 70/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,618 ] - Epoch 71/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,657 ] - Epoch 72/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,703 ] - Epoch 73/500 - time: 0.05 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,745 ] - Epoch 74/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,784 ] - Epoch 75/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,822 ] - Epoch 76/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:10,860 ] - Epoch 77/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,904 ] - Epoch 78/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,943 ] - Epoch 79/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:10,983 ] - Epoch 80/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,022 ] - Epoch 81/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,060 ] - Epoch 82/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,100 ] - Epoch 83/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,139 ] - Epoch 84/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,179 ] - Epoch 85/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,220 ] - Epoch 86/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,259 ] - Epoch 87/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,298 ] - Epoch 88/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,337 ] - Epoch 89/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,375 ] - Epoch 90/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:31:11,414 ] - Epoch 91/500 - time: 0.04 - training_loss: -5.9937
[ INFO : 2022-07-26 19:31:11,453 ] - Epoch 92/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,492 ] - Epoch 93/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,536 ] - Epoch 94/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,574 ] - Epoch 95/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,613 ] - Epoch 96/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,652 ] - Epoch 97/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,691 ] - Epoch 98/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,730 ] - Epoch 99/500 - time: 0.04 - training_loss: -5.9938
[ INFO : 2022-07-26 19:31:11,769 ] - Epoch 100/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,807 ] - Epoch 101/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,847 ] - Epoch 102/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,885 ] - Epoch 103/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,925 ] - Epoch 104/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,963 ] - Epoch 105/500 - time: 0.04 - training_loss: -5.9939
[ INFO : 2022-07-26 19:31:11,999 ] - Epoch 106/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:12,038 ] - Epoch 107/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:12,076 ] - Epoch 108/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:12,115 ] - Epoch 109/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,154 ] - Epoch 110/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:12,194 ] - Epoch 111/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,234 ] - Epoch 112/500 - time: 0.04 - training_loss: -5.9940
[ INFO : 2022-07-26 19:31:12,271 ] - Epoch 113/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,310 ] - Epoch 114/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,354 ] - Epoch 115/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,394 ] - Epoch 116/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,433 ] - Epoch 117/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,475 ] - Epoch 118/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,514 ] - Epoch 119/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,552 ] - Epoch 120/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,592 ] - Epoch 121/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,632 ] - Epoch 122/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,670 ] - Epoch 123/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,709 ] - Epoch 124/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,749 ] - Epoch 125/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,789 ] - Epoch 126/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,828 ] - Epoch 127/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:12,867 ] - Epoch 128/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,906 ] - Epoch 129/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,943 ] - Epoch 130/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:12,982 ] - Epoch 131/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,019 ] - Epoch 132/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,060 ] - Epoch 133/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,099 ] - Epoch 134/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,138 ] - Epoch 135/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,176 ] - Epoch 136/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,214 ] - Epoch 137/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,252 ] - Epoch 138/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,290 ] - Epoch 139/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,330 ] - Epoch 140/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,367 ] - Epoch 141/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,406 ] - Epoch 142/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,443 ] - Epoch 143/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,480 ] - Epoch 144/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,519 ] - Epoch 145/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,561 ] - Epoch 146/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,600 ] - Epoch 147/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,639 ] - Epoch 148/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,677 ] - Epoch 149/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,715 ] - Epoch 150/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,753 ] - Epoch 151/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:13,792 ] - Epoch 152/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,831 ] - Epoch 153/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,869 ] - Epoch 154/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,911 ] - Epoch 155/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,949 ] - Epoch 156/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:13,990 ] - Epoch 157/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,030 ] - Epoch 158/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,071 ] - Epoch 159/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,114 ] - Epoch 160/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,155 ] - Epoch 161/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,195 ] - Epoch 162/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,234 ] - Epoch 163/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,273 ] - Epoch 164/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,311 ] - Epoch 165/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,350 ] - Epoch 166/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,387 ] - Epoch 167/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:14,430 ] - Epoch 168/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,469 ] - Epoch 169/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,514 ] - Epoch 170/500 - time: 0.05 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,554 ] - Epoch 171/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,594 ] - Epoch 172/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,632 ] - Epoch 173/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,670 ] - Epoch 174/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,709 ] - Epoch 175/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,748 ] - Epoch 176/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,786 ] - Epoch 177/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,823 ] - Epoch 178/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,861 ] - Epoch 179/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,900 ] - Epoch 180/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,941 ] - Epoch 181/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:14,980 ] - Epoch 182/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,019 ] - Epoch 183/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,059 ] - Epoch 184/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,098 ] - Epoch 185/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,140 ] - Epoch 186/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,181 ] - Epoch 187/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,220 ] - Epoch 188/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,263 ] - Epoch 189/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,302 ] - Epoch 190/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,341 ] - Epoch 191/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,379 ] - Epoch 192/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,418 ] - Epoch 193/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,463 ] - Epoch 194/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,500 ] - Epoch 195/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,539 ] - Epoch 196/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,577 ] - Epoch 197/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,616 ] - Epoch 198/500 - time: 0.04 - training_loss: -5.9944
[ INFO : 2022-07-26 19:31:15,656 ] - Epoch 199/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,696 ] - Epoch 200/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,736 ] - Epoch 201/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,774 ] - Epoch 202/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,812 ] - Epoch 203/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,850 ] - Epoch 204/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,891 ] - Epoch 205/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,934 ] - Epoch 206/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:15,973 ] - Epoch 207/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,017 ] - Epoch 208/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,055 ] - Epoch 209/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,095 ] - Epoch 210/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,139 ] - Epoch 211/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,177 ] - Epoch 212/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,216 ] - Epoch 213/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,254 ] - Epoch 214/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,294 ] - Epoch 215/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,333 ] - Epoch 216/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,374 ] - Epoch 217/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,414 ] - Epoch 218/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,453 ] - Epoch 219/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,493 ] - Epoch 220/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,532 ] - Epoch 221/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,570 ] - Epoch 222/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,609 ] - Epoch 223/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,649 ] - Epoch 224/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,689 ] - Epoch 225/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,728 ] - Epoch 226/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,766 ] - Epoch 227/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,804 ] - Epoch 228/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,844 ] - Epoch 229/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:16,886 ] - Epoch 230/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,925 ] - Epoch 231/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:16,963 ] - Epoch 232/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,001 ] - Epoch 233/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,039 ] - Epoch 234/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,080 ] - Epoch 235/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,122 ] - Epoch 236/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,161 ] - Epoch 237/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,200 ] - Epoch 238/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,238 ] - Epoch 239/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,275 ] - Epoch 240/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,316 ] - Epoch 241/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,355 ] - Epoch 242/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:17,395 ] - Epoch 243/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,435 ] - Epoch 244/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,473 ] - Epoch 245/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,511 ] - Epoch 246/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,553 ] - Epoch 247/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,593 ] - Epoch 248/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,631 ] - Epoch 249/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,669 ] - Epoch 250/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,708 ] - Epoch 251/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,746 ] - Epoch 252/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,787 ] - Epoch 253/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,828 ] - Epoch 254/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,866 ] - Epoch 255/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,904 ] - Epoch 256/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,943 ] - Epoch 257/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:17,982 ] - Epoch 258/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,022 ] - Epoch 259/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,065 ] - Epoch 260/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,104 ] - Epoch 261/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,143 ] - Epoch 262/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,183 ] - Epoch 263/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,221 ] - Epoch 264/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,260 ] - Epoch 265/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,301 ] - Epoch 266/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,339 ] - Epoch 267/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,376 ] - Epoch 268/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,415 ] - Epoch 269/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,454 ] - Epoch 270/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,496 ] - Epoch 271/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,536 ] - Epoch 272/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,576 ] - Epoch 273/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,612 ] - Epoch 274/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,649 ] - Epoch 275/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,690 ] - Epoch 276/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,730 ] - Epoch 277/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,768 ] - Epoch 278/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,808 ] - Epoch 279/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,848 ] - Epoch 280/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,885 ] - Epoch 281/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,923 ] - Epoch 282/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:18,969 ] - Epoch 283/500 - time: 0.05 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,009 ] - Epoch 284/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,048 ] - Epoch 285/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,088 ] - Epoch 286/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,127 ] - Epoch 287/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,165 ] - Epoch 288/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,204 ] - Epoch 289/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,244 ] - Epoch 290/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,283 ] - Epoch 291/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,320 ] - Epoch 292/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,364 ] - Epoch 293/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,402 ] - Epoch 294/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,442 ] - Epoch 295/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,486 ] - Epoch 296/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,525 ] - Epoch 297/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,565 ] - Epoch 298/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,605 ] - Epoch 299/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,644 ] - Epoch 300/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,683 ] - Epoch 301/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,723 ] - Epoch 302/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,762 ] - Epoch 303/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:19,800 ] - Epoch 304/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,838 ] - Epoch 305/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,877 ] - Epoch 306/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,916 ] - Epoch 307/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,956 ] - Epoch 308/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:19,994 ] - Epoch 309/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,039 ] - Epoch 310/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,080 ] - Epoch 311/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,119 ] - Epoch 312/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,161 ] - Epoch 313/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,200 ] - Epoch 314/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,240 ] - Epoch 315/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,279 ] - Epoch 316/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,317 ] - Epoch 317/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,356 ] - Epoch 318/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,397 ] - Epoch 319/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,437 ] - Epoch 320/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,475 ] - Epoch 321/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,518 ] - Epoch 322/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,555 ] - Epoch 323/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,597 ] - Epoch 324/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,653 ] - Epoch 325/500 - time: 0.06 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,693 ] - Epoch 326/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,732 ] - Epoch 327/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,771 ] - Epoch 328/500 - time: 0.04 - training_loss: -5.9941
[ INFO : 2022-07-26 19:31:20,809 ] - Epoch 329/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,848 ] - Epoch 330/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,891 ] - Epoch 331/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,935 ] - Epoch 332/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:20,976 ] - Epoch 333/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,015 ] - Epoch 334/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,053 ] - Epoch 335/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,093 ] - Epoch 336/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,134 ] - Epoch 337/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,177 ] - Epoch 338/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,217 ] - Epoch 339/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,256 ] - Epoch 340/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,295 ] - Epoch 341/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,337 ] - Epoch 342/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,376 ] - Epoch 343/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,417 ] - Epoch 344/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,456 ] - Epoch 345/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,499 ] - Epoch 346/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,538 ] - Epoch 347/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,578 ] - Epoch 348/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,617 ] - Epoch 349/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,656 ] - Epoch 350/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,697 ] - Epoch 351/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,735 ] - Epoch 352/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,774 ] - Epoch 353/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,813 ] - Epoch 354/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,854 ] - Epoch 355/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,893 ] - Epoch 356/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,933 ] - Epoch 357/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:21,971 ] - Epoch 358/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,011 ] - Epoch 359/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,056 ] - Epoch 360/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,096 ] - Epoch 361/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,140 ] - Epoch 362/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,179 ] - Epoch 363/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,217 ] - Epoch 364/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,256 ] - Epoch 365/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,295 ] - Epoch 366/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,339 ] - Epoch 367/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,380 ] - Epoch 368/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,419 ] - Epoch 369/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,458 ] - Epoch 370/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,496 ] - Epoch 371/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,535 ] - Epoch 372/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,576 ] - Epoch 373/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,619 ] - Epoch 374/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,660 ] - Epoch 375/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,705 ] - Epoch 376/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,744 ] - Epoch 377/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,786 ] - Epoch 378/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,829 ] - Epoch 379/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,872 ] - Epoch 380/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,916 ] - Epoch 381/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,958 ] - Epoch 382/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:22,996 ] - Epoch 383/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,038 ] - Epoch 384/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,079 ] - Epoch 385/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,120 ] - Epoch 386/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,160 ] - Epoch 387/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,201 ] - Epoch 388/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,246 ] - Epoch 389/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,287 ] - Epoch 390/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,327 ] - Epoch 391/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,370 ] - Epoch 392/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,409 ] - Epoch 393/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,451 ] - Epoch 394/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,494 ] - Epoch 395/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,535 ] - Epoch 396/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,577 ] - Epoch 397/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,616 ] - Epoch 398/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,658 ] - Epoch 399/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,699 ] - Epoch 400/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:23,742 ] - Epoch 401/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,783 ] - Epoch 402/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:23,823 ] - Epoch 403/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:23,864 ] - Epoch 404/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:23,902 ] - Epoch 405/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:23,943 ] - Epoch 406/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:23,982 ] - Epoch 407/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,021 ] - Epoch 408/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,063 ] - Epoch 409/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,102 ] - Epoch 410/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,142 ] - Epoch 411/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,184 ] - Epoch 412/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,223 ] - Epoch 413/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,263 ] - Epoch 414/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,301 ] - Epoch 415/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,341 ] - Epoch 416/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,381 ] - Epoch 417/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,420 ] - Epoch 418/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,458 ] - Epoch 419/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,497 ] - Epoch 420/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,536 ] - Epoch 421/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,576 ] - Epoch 422/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,616 ] - Epoch 423/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,658 ] - Epoch 424/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,698 ] - Epoch 425/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,740 ] - Epoch 426/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,785 ] - Epoch 427/500 - time: 0.04 - training_loss: -5.9942
[ INFO : 2022-07-26 19:31:24,830 ] - Epoch 428/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,873 ] - Epoch 429/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,915 ] - Epoch 430/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:24,958 ] - Epoch 431/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,001 ] - Epoch 432/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,042 ] - Epoch 433/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,084 ] - Epoch 434/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,128 ] - Epoch 435/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,169 ] - Epoch 436/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,214 ] - Epoch 437/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:31:25,256 ] - Epoch 438/500 - time: 0.04 - training_loss: -5.9943
[ INFO : 2022-07-26 19:32:25,499 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=2, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:32:25,499 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:32:26,880 ] - Epoch 1/500 - time: 1.38 - training_loss: -5.9869
[ INFO : 2022-07-26 19:32:26,923 ] - Epoch 2/500 - time: 0.04 - training_loss: -5.9901
[ INFO : 2022-07-26 19:32:26,964 ] - Epoch 3/500 - time: 0.04 - training_loss: -5.9847
[ INFO : 2022-07-26 19:32:27,006 ] - Epoch 4/500 - time: 0.04 - training_loss: -5.9840
[ INFO : 2022-07-26 19:32:27,047 ] - Epoch 5/500 - time: 0.04 - training_loss: -5.9851
[ INFO : 2022-07-26 19:32:27,091 ] - Epoch 6/500 - time: 0.04 - training_loss: -5.9854
[ INFO : 2022-07-26 19:32:27,135 ] - Epoch 7/500 - time: 0.04 - training_loss: -5.9860
[ INFO : 2022-07-26 19:32:27,180 ] - Epoch 8/500 - time: 0.04 - training_loss: -5.9871
[ INFO : 2022-07-26 19:32:27,226 ] - Epoch 9/500 - time: 0.05 - training_loss: -5.9876
[ INFO : 2022-07-26 19:32:27,267 ] - Epoch 10/500 - time: 0.04 - training_loss: -5.9881
[ INFO : 2022-07-26 19:32:27,309 ] - Epoch 11/500 - time: 0.04 - training_loss: -5.9888
[ INFO : 2022-07-26 19:32:27,354 ] - Epoch 12/500 - time: 0.05 - training_loss: -5.9890
[ INFO : 2022-07-26 19:44:29,232 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:44:29,233 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:44:45,693 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:44:45,694 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:45:07,498 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:45:07,499 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 19:45:31,412 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 19:45:31,412 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:42:53,541 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:42:53,542 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:43:06,627 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:43:06,628 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:43:22,364 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:43:22,365 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:43:27,753 ] - Epoch 1: val_loss improved from 0.0000 to -5.3818, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:29,845 ] - Epoch 1/500 - time: 7.48 - training_loss: -5.9371 - val_loss: -5.3818
[ INFO : 2022-07-26 20:43:33,535 ] - Epoch 2: val_loss improved from 0.0000 to -5.6612, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:33,811 ] - Epoch 2/500 - time: 3.97 - training_loss: -5.9533 - val_loss: -5.6612
[ INFO : 2022-07-26 20:43:37,456 ] - Epoch 3: val_loss improved from 0.0000 to -5.9516, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:37,711 ] - Epoch 3/500 - time: 3.90 - training_loss: -5.9542 - val_loss: -5.9516
[ INFO : 2022-07-26 20:43:41,362 ] - Epoch 4: val_loss improved from 0.0000 to -3.4521, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:41,603 ] - Epoch 4/500 - time: 3.89 - training_loss: -5.9547 - val_loss: -3.4521
[ INFO : 2022-07-26 20:43:45,235 ] - Epoch 5: val_loss improved from 0.0000 to -3.4138, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:45,474 ] - Epoch 5/500 - time: 3.87 - training_loss: -5.9563 - val_loss: -3.4138
[ INFO : 2022-07-26 20:43:49,176 ] - Epoch 6: val_loss improved from 0.0000 to -2.9443, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:49,425 ] - Epoch 6/500 - time: 3.95 - training_loss: -5.9548 - val_loss: -2.9443
[ INFO : 2022-07-26 20:43:53,084 ] - Epoch 7: val_loss improved from 0.0000 to -2.2074, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:53,326 ] - Epoch 7/500 - time: 3.90 - training_loss: -5.9544 - val_loss: -2.2074
[ INFO : 2022-07-26 20:43:57,027 ] - Epoch 8: val_loss improved from 0.0000 to -1.7322, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:43:57,253 ] - Epoch 8/500 - time: 3.93 - training_loss: -5.9516 - val_loss: -1.7322
[ INFO : 2022-07-26 20:44:00,950 ] - Epoch 9: val_loss improved from 0.0000 to -2.2996, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:01,189 ] - Epoch 9/500 - time: 3.94 - training_loss: -5.9502 - val_loss: -2.2996
[ INFO : 2022-07-26 20:44:04,878 ] - Epoch 10: val_loss improved from 0.0000 to -3.3568, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:05,123 ] - Epoch 10/500 - time: 3.93 - training_loss: -5.9464 - val_loss: -3.3568
[ INFO : 2022-07-26 20:44:08,883 ] - Epoch 11: val_loss improved from 0.0000 to -1.7316, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:09,153 ] - Epoch 11/500 - time: 4.03 - training_loss: -5.9429 - val_loss: -1.7316
[ INFO : 2022-07-26 20:44:13,087 ] - Epoch 12: val_loss improved from 0.0000 to -1.7322, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:13,325 ] - Epoch 12/500 - time: 4.17 - training_loss: -5.9412 - val_loss: -1.7322
[ INFO : 2022-07-26 20:44:20,308 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=2, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:44:20,309 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:44:25,307 ] - Epoch 1: val_loss improved from 0.0000 to -5.4741, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:25,568 ] - Epoch 1/500 - time: 5.26 - training_loss: -5.9305 - val_loss: -5.4741
[ INFO : 2022-07-26 20:44:29,157 ] - Epoch 2: val_loss improved from -5.4741 to -5.7015, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:29,410 ] - Epoch 2/500 - time: 3.84 - training_loss: -5.9452 - val_loss: -5.7015
[ INFO : 2022-07-26 20:44:33,130 ] - Epoch 3: val_loss improved from -5.7015 to -5.8679, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:33,393 ] - Epoch 3/500 - time: 3.98 - training_loss: -5.9523 - val_loss: -5.8679
[ INFO : 2022-07-26 20:44:36,990 ] - Epoch 4: val_loss improved from -5.8679 to -5.9050, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:44:37,221 ] - Epoch 4/500 - time: 3.83 - training_loss: -5.9552 - val_loss: -5.9050
[ INFO : 2022-07-26 20:44:40,953 ] - Epoch 5: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:40,953 ] - Epoch 5/500 - time: 3.73 - training_loss: -5.9483 - val_loss: -3.5506
[ INFO : 2022-07-26 20:44:44,539 ] - Epoch 6: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:44,539 ] - Epoch 6/500 - time: 3.59 - training_loss: -5.9413 - val_loss: -1.9304
[ INFO : 2022-07-26 20:44:48,173 ] - Epoch 7: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:48,173 ] - Epoch 7/500 - time: 3.63 - training_loss: -5.9401 - val_loss: -1.7323
[ INFO : 2022-07-26 20:44:51,907 ] - Epoch 8: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:51,907 ] - Epoch 8/500 - time: 3.73 - training_loss: -5.9374 - val_loss: -2.6746
[ INFO : 2022-07-26 20:44:55,668 ] - Epoch 9: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:55,669 ] - Epoch 9/500 - time: 3.76 - training_loss: -5.9386 - val_loss: -1.7322
[ INFO : 2022-07-26 20:44:59,357 ] - Epoch 10: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:44:59,357 ] - Epoch 10/500 - time: 3.69 - training_loss: -5.9397 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:03,003 ] - Epoch 11: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:03,003 ] - Epoch 11/500 - time: 3.65 - training_loss: -5.9391 - val_loss: -3.3963
[ INFO : 2022-07-26 20:45:06,600 ] - Epoch 12: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:06,600 ] - Epoch 12/500 - time: 3.60 - training_loss: -5.9388 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:10,278 ] - Epoch 13: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:10,278 ] - Epoch 13/500 - time: 3.68 - training_loss: -5.9392 - val_loss: -3.2135
[ INFO : 2022-07-26 20:45:14,015 ] - Epoch 14: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:14,015 ] - Epoch 14/500 - time: 3.74 - training_loss: -5.9394 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:17,708 ] - Epoch 15: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:17,708 ] - Epoch 15/500 - time: 3.69 - training_loss: -5.9398 - val_loss: -1.7323
[ INFO : 2022-07-26 20:45:21,460 ] - Epoch 16: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:21,460 ] - Epoch 16/500 - time: 3.75 - training_loss: -5.9409 - val_loss: -2.6987
[ INFO : 2022-07-26 20:45:25,168 ] - Epoch 17: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:25,168 ] - Epoch 17/500 - time: 3.71 - training_loss: -5.9416 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:28,916 ] - Epoch 18: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:28,916 ] - Epoch 18/500 - time: 3.75 - training_loss: -5.9425 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:32,869 ] - Epoch 19: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:32,869 ] - Epoch 19/500 - time: 3.95 - training_loss: -5.9433 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:36,593 ] - Epoch 20: val_loss did not improve from -5.9050
[ INFO : 2022-07-26 20:45:36,593 ] - Epoch 20/500 - time: 3.72 - training_loss: -5.9430 - val_loss: -1.7322
[ INFO : 2022-07-26 20:45:41,406 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=512, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=512, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:45:41,407 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:45:50,396 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:45:50,397 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:45:57,642 ] - Epoch 1: val_loss improved from 0.0000 to -15.2852, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:45:57,906 ] - Epoch 1/500 - time: 7.51 - training_loss: -14.3454 - val_loss: -15.2852
[ INFO : 2022-07-26 20:46:03,678 ] - Epoch 2: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:03,679 ] - Epoch 2/500 - time: 5.77 - training_loss: -14.7609 - val_loss: -15.1246
[ INFO : 2022-07-26 20:46:09,431 ] - Epoch 3: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:09,431 ] - Epoch 3/500 - time: 5.75 - training_loss: -15.0197 - val_loss: -15.1683
[ INFO : 2022-07-26 20:46:15,287 ] - Epoch 4: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:15,287 ] - Epoch 4/500 - time: 5.86 - training_loss: -15.1521 - val_loss: -14.5761
[ INFO : 2022-07-26 20:46:21,167 ] - Epoch 5: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:21,168 ] - Epoch 5/500 - time: 5.88 - training_loss: -15.0743 - val_loss: -14.1202
[ INFO : 2022-07-26 20:46:27,216 ] - Epoch 6: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:27,216 ] - Epoch 6/500 - time: 6.05 - training_loss: -14.9890 - val_loss: -14.0156
[ INFO : 2022-07-26 20:46:33,206 ] - Epoch 7: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:33,206 ] - Epoch 7/500 - time: 5.99 - training_loss: -14.9318 - val_loss: -14.1645
[ INFO : 2022-07-26 20:46:39,105 ] - Epoch 8: val_loss did not improve from -15.2852
[ INFO : 2022-07-26 20:46:39,105 ] - Epoch 8/500 - time: 5.90 - training_loss: -14.8908 - val_loss: -13.7583
[ INFO : 2022-07-26 20:46:45,015 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:46:45,017 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:46:52,411 ] - Epoch 1: val_loss improved from 0.0000 to -15.5285, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:46:52,671 ] - Epoch 1/500 - time: 7.65 - training_loss: -15.0715 - val_loss: -15.5285
[ INFO : 2022-07-26 20:46:58,710 ] - Epoch 2: val_loss improved from -15.5285 to -15.7969, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:46:59,066 ] - Epoch 2/500 - time: 6.39 - training_loss: -15.4086 - val_loss: -15.7969
[ INFO : 2022-07-26 20:47:05,431 ] - Epoch 3: val_loss improved from -15.7969 to -16.0095, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:05,707 ] - Epoch 3/500 - time: 6.64 - training_loss: -15.6333 - val_loss: -16.0095
[ INFO : 2022-07-26 20:47:09,547 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:47:09,547 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:47:17,326 ] - Epoch 1: val_loss improved from 0.0000 to -15.3435, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:17,584 ] - Epoch 1/500 - time: 8.04 - training_loss: -14.8810 - val_loss: -15.3435
[ INFO : 2022-07-26 20:47:23,977 ] - Epoch 2: val_loss improved from -15.3435 to -15.8025, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:24,404 ] - Epoch 2/500 - time: 6.82 - training_loss: -15.3460 - val_loss: -15.8025
[ INFO : 2022-07-26 20:47:30,758 ] - Epoch 3: val_loss improved from -15.8025 to -16.3831, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:31,056 ] - Epoch 3/500 - time: 6.65 - training_loss: -15.6526 - val_loss: -16.3831
[ INFO : 2022-07-26 20:47:37,579 ] - Epoch 4: val_loss improved from -16.3831 to -16.6704, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:37,844 ] - Epoch 4/500 - time: 6.79 - training_loss: -15.8835 - val_loss: -16.6704
[ INFO : 2022-07-26 20:47:44,110 ] - Epoch 5: val_loss improved from -16.6704 to -16.8285, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:44,395 ] - Epoch 5/500 - time: 6.55 - training_loss: -16.0617 - val_loss: -16.8285
[ INFO : 2022-07-26 20:47:50,602 ] - Epoch 6: val_loss improved from -16.8285 to -16.8972, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:50,872 ] - Epoch 6/500 - time: 6.48 - training_loss: -16.1994 - val_loss: -16.8972
[ INFO : 2022-07-26 20:47:57,117 ] - Epoch 7: val_loss improved from -16.8972 to -16.9567, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:47:57,364 ] - Epoch 7/500 - time: 6.49 - training_loss: -16.3055 - val_loss: -16.9567
[ INFO : 2022-07-26 20:48:03,612 ] - Epoch 8: val_loss did not improve from -16.9567
[ INFO : 2022-07-26 20:48:03,613 ] - Epoch 8/500 - time: 6.25 - training_loss: -16.3899 - val_loss: -16.9315
[ INFO : 2022-07-26 20:48:09,824 ] - Epoch 9: val_loss did not improve from -16.9567
[ INFO : 2022-07-26 20:48:09,824 ] - Epoch 9/500 - time: 6.21 - training_loss: -16.4549 - val_loss: -16.9382
[ INFO : 2022-07-26 20:48:16,060 ] - Epoch 10: val_loss improved from -16.9567 to -17.0010, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:16,321 ] - Epoch 10/500 - time: 6.50 - training_loss: -16.5108 - val_loss: -17.0010
[ INFO : 2022-07-26 20:48:22,736 ] - Epoch 11: val_loss improved from -17.0010 to -17.0577, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:22,992 ] - Epoch 11/500 - time: 6.67 - training_loss: -16.5620 - val_loss: -17.0577
[ INFO : 2022-07-26 20:48:29,326 ] - Epoch 12: val_loss improved from -17.0577 to -17.0688, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:29,674 ] - Epoch 12/500 - time: 6.68 - training_loss: -16.6072 - val_loss: -17.0688
[ INFO : 2022-07-26 20:48:35,895 ] - Epoch 13: val_loss improved from -17.0688 to -17.0827, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:36,236 ] - Epoch 13/500 - time: 6.56 - training_loss: -16.6461 - val_loss: -17.0827
[ INFO : 2022-07-26 20:48:42,486 ] - Epoch 14: val_loss improved from -17.0827 to -17.1060, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:42,753 ] - Epoch 14/500 - time: 6.52 - training_loss: -16.6801 - val_loss: -17.1060
[ INFO : 2022-07-26 20:48:49,025 ] - Epoch 15: val_loss improved from -17.1060 to -17.1166, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:49,306 ] - Epoch 15/500 - time: 6.55 - training_loss: -16.7107 - val_loss: -17.1166
[ INFO : 2022-07-26 20:48:55,847 ] - Epoch 16: val_loss improved from -17.1166 to -17.1350, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:48:56,122 ] - Epoch 16/500 - time: 6.82 - training_loss: -16.7384 - val_loss: -17.1350
[ INFO : 2022-07-26 20:49:02,535 ] - Epoch 17: val_loss did not improve from -17.1350
[ INFO : 2022-07-26 20:49:02,535 ] - Epoch 17/500 - time: 6.41 - training_loss: -16.7642 - val_loss: -17.1338
[ INFO : 2022-07-26 20:49:08,940 ] - Epoch 18: val_loss did not improve from -17.1350
[ INFO : 2022-07-26 20:49:08,941 ] - Epoch 18/500 - time: 6.41 - training_loss: -16.7869 - val_loss: -17.1204
[ INFO : 2022-07-26 20:49:15,534 ] - Epoch 19: val_loss improved from -17.1350 to -17.1649, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:49:15,819 ] - Epoch 19/500 - time: 6.88 - training_loss: -16.8069 - val_loss: -17.1649
[ INFO : 2022-07-26 20:49:22,275 ] - Epoch 20: val_loss did not improve from -17.1649
[ INFO : 2022-07-26 20:49:22,275 ] - Epoch 20/500 - time: 6.46 - training_loss: -16.8257 - val_loss: -17.1137
[ INFO : 2022-07-26 20:49:28,890 ] - Epoch 21: val_loss improved from -17.1649 to -17.1953, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:49:29,160 ] - Epoch 21/500 - time: 6.88 - training_loss: -16.8434 - val_loss: -17.1953
[ INFO : 2022-07-26 20:49:35,568 ] - Epoch 22: val_loss improved from -17.1953 to -17.2204, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:49:35,843 ] - Epoch 22/500 - time: 6.68 - training_loss: -16.8609 - val_loss: -17.2204
[ INFO : 2022-07-26 20:49:42,136 ] - Epoch 23: val_loss improved from -17.2204 to -17.2383, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:49:42,391 ] - Epoch 23/500 - time: 6.55 - training_loss: -16.8775 - val_loss: -17.2383
[ INFO : 2022-07-26 20:49:48,641 ] - Epoch 24: val_loss did not improve from -17.2383
[ INFO : 2022-07-26 20:49:48,641 ] - Epoch 24/500 - time: 6.25 - training_loss: -16.8931 - val_loss: -17.2316
[ INFO : 2022-07-26 20:49:54,895 ] - Epoch 25: val_loss improved from -17.2383 to -17.2520, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:49:55,192 ] - Epoch 25/500 - time: 6.55 - training_loss: -16.9078 - val_loss: -17.2520
[ INFO : 2022-07-26 20:50:01,435 ] - Epoch 26: val_loss did not improve from -17.2520
[ INFO : 2022-07-26 20:50:01,435 ] - Epoch 26/500 - time: 6.24 - training_loss: -16.9216 - val_loss: -17.2439
[ INFO : 2022-07-26 20:50:07,650 ] - Epoch 27: val_loss improved from -17.2520 to -17.2560, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:50:07,989 ] - Epoch 27/500 - time: 6.55 - training_loss: -16.9345 - val_loss: -17.2560
[ INFO : 2022-07-26 20:50:14,265 ] - Epoch 28: val_loss did not improve from -17.2560
[ INFO : 2022-07-26 20:50:14,265 ] - Epoch 28/500 - time: 6.28 - training_loss: -16.9465 - val_loss: -17.2359
[ INFO : 2022-07-26 20:50:20,537 ] - Epoch 29: val_loss improved from -17.2560 to -17.2563, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:50:20,800 ] - Epoch 29/500 - time: 6.53 - training_loss: -16.9576 - val_loss: -17.2563
[ INFO : 2022-07-26 20:50:27,057 ] - Epoch 30: val_loss did not improve from -17.2563
[ INFO : 2022-07-26 20:50:27,057 ] - Epoch 30/500 - time: 6.26 - training_loss: -16.9680 - val_loss: -17.2365
[ INFO : 2022-07-26 20:50:33,295 ] - Epoch 31: val_loss did not improve from -17.2563
[ INFO : 2022-07-26 20:50:33,296 ] - Epoch 31/500 - time: 6.24 - training_loss: -16.9775 - val_loss: -17.2404
[ INFO : 2022-07-26 20:50:39,546 ] - Epoch 32: val_loss improved from -17.2563 to -17.2563, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:50:39,836 ] - Epoch 32/500 - time: 6.54 - training_loss: -16.9864 - val_loss: -17.2563
[ INFO : 2022-07-26 20:50:46,140 ] - Epoch 33: val_loss did not improve from -17.2563
[ INFO : 2022-07-26 20:50:46,140 ] - Epoch 33/500 - time: 6.30 - training_loss: -16.9950 - val_loss: -17.2491
[ INFO : 2022-07-26 20:50:52,380 ] - Epoch 34: val_loss did not improve from -17.2563
[ INFO : 2022-07-26 20:50:52,380 ] - Epoch 34/500 - time: 6.24 - training_loss: -17.0031 - val_loss: -17.2480
[ INFO : 2022-07-26 20:50:58,633 ] - Epoch 35: val_loss did not improve from -17.2563
[ INFO : 2022-07-26 20:50:58,634 ] - Epoch 35/500 - time: 6.25 - training_loss: -17.0109 - val_loss: -17.2490
[ INFO : 2022-07-26 20:51:04,875 ] - Epoch 36: val_loss improved from -17.2563 to -17.2634, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:51:05,143 ] - Epoch 36/500 - time: 6.51 - training_loss: -17.0181 - val_loss: -17.2634
[ INFO : 2022-07-26 20:51:11,428 ] - Epoch 37: val_loss did not improve from -17.2634
[ INFO : 2022-07-26 20:51:11,429 ] - Epoch 37/500 - time: 6.29 - training_loss: -17.0250 - val_loss: -17.2509
[ INFO : 2022-07-26 20:51:17,724 ] - Epoch 38: val_loss did not improve from -17.2634
[ INFO : 2022-07-26 20:51:17,724 ] - Epoch 38/500 - time: 6.30 - training_loss: -17.0315 - val_loss: -17.2413
[ INFO : 2022-07-26 20:51:23,995 ] - Epoch 39: val_loss did not improve from -17.2634
[ INFO : 2022-07-26 20:51:23,995 ] - Epoch 39/500 - time: 6.27 - training_loss: -17.0376 - val_loss: -17.2479
[ INFO : 2022-07-26 20:51:30,286 ] - Epoch 40: val_loss did not improve from -17.2634
[ INFO : 2022-07-26 20:51:30,286 ] - Epoch 40/500 - time: 6.29 - training_loss: -17.0434 - val_loss: -17.2400
[ INFO : 2022-07-26 20:51:36,581 ] - Epoch 41: val_loss improved from -17.2634 to -17.2739, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:51:36,832 ] - Epoch 41/500 - time: 6.55 - training_loss: -17.0491 - val_loss: -17.2739
[ INFO : 2022-07-26 20:51:43,089 ] - Epoch 42: val_loss improved from -17.2739 to -17.2892, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:51:43,363 ] - Epoch 42/500 - time: 6.53 - training_loss: -17.0548 - val_loss: -17.2892
[ INFO : 2022-07-26 20:51:49,664 ] - Epoch 43: val_loss improved from -17.2892 to -17.2911, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:51:49,919 ] - Epoch 43/500 - time: 6.56 - training_loss: -17.0606 - val_loss: -17.2911
[ INFO : 2022-07-26 20:51:56,293 ] - Epoch 44: val_loss improved from -17.2911 to -17.2966, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:51:56,577 ] - Epoch 44/500 - time: 6.66 - training_loss: -17.0662 - val_loss: -17.2966
[ INFO : 2022-07-26 20:52:02,813 ] - Epoch 45: val_loss improved from -17.2966 to -17.3015, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:03,076 ] - Epoch 45/500 - time: 6.50 - training_loss: -17.0716 - val_loss: -17.3015
[ INFO : 2022-07-26 20:52:09,399 ] - Epoch 46: val_loss improved from -17.3015 to -17.3018, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:09,671 ] - Epoch 46/500 - time: 6.59 - training_loss: -17.0768 - val_loss: -17.3018
[ INFO : 2022-07-26 20:52:16,371 ] - Epoch 47: val_loss did not improve from -17.3018
[ INFO : 2022-07-26 20:52:16,371 ] - Epoch 47/500 - time: 6.70 - training_loss: -17.0818 - val_loss: -17.2956
[ INFO : 2022-07-26 20:52:19,526 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:52:19,526 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 20:52:27,952 ] - Epoch 1: val_loss improved from 0.0000 to -15.0757, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:28,217 ] - Epoch 1/500 - time: 8.69 - training_loss: -14.6315 - val_loss: -15.0757
[ INFO : 2022-07-26 20:52:35,210 ] - Epoch 2: val_loss improved from -15.0757 to -15.3575, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:35,456 ] - Epoch 2/500 - time: 7.24 - training_loss: -15.0002 - val_loss: -15.3575
[ INFO : 2022-07-26 20:52:42,458 ] - Epoch 3: val_loss improved from -15.3575 to -15.5569, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:42,779 ] - Epoch 3/500 - time: 7.32 - training_loss: -15.2442 - val_loss: -15.5569
[ INFO : 2022-07-26 20:52:49,801 ] - Epoch 4: val_loss improved from -15.5569 to -15.7218, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:50,062 ] - Epoch 4/500 - time: 7.28 - training_loss: -15.4374 - val_loss: -15.7218
[ INFO : 2022-07-26 20:52:57,144 ] - Epoch 5: val_loss improved from -15.7218 to -15.9999, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:52:57,472 ] - Epoch 5/500 - time: 7.41 - training_loss: -15.5951 - val_loss: -15.9999
[ INFO : 2022-07-26 20:53:04,473 ] - Epoch 6: val_loss improved from -15.9999 to -16.3893, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:04,776 ] - Epoch 6/500 - time: 7.30 - training_loss: -15.7292 - val_loss: -16.3893
[ INFO : 2022-07-26 20:53:11,760 ] - Epoch 7: val_loss improved from -16.3893 to -16.5215, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:12,029 ] - Epoch 7/500 - time: 7.25 - training_loss: -15.8421 - val_loss: -16.5215
[ INFO : 2022-07-26 20:53:19,056 ] - Epoch 8: val_loss improved from -16.5215 to -16.6680, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:19,334 ] - Epoch 8/500 - time: 7.30 - training_loss: -15.9420 - val_loss: -16.6680
[ INFO : 2022-07-26 20:53:26,393 ] - Epoch 9: val_loss improved from -16.6680 to -16.7803, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:26,653 ] - Epoch 9/500 - time: 7.32 - training_loss: -16.0316 - val_loss: -16.7803
[ INFO : 2022-07-26 20:53:33,723 ] - Epoch 10: val_loss improved from -16.7803 to -16.9205, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:33,982 ] - Epoch 10/500 - time: 7.33 - training_loss: -16.1137 - val_loss: -16.9205
[ INFO : 2022-07-26 20:53:40,958 ] - Epoch 11: val_loss did not improve from -16.9205
[ INFO : 2022-07-26 20:53:40,958 ] - Epoch 11/500 - time: 6.98 - training_loss: -16.1878 - val_loss: -16.9172
[ INFO : 2022-07-26 20:53:48,050 ] - Epoch 12: val_loss improved from -16.9205 to -16.9869, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:48,397 ] - Epoch 12/500 - time: 7.44 - training_loss: -16.2528 - val_loss: -16.9869
[ INFO : 2022-07-26 20:53:55,493 ] - Epoch 13: val_loss improved from -16.9869 to -17.0146, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:53:55,765 ] - Epoch 13/500 - time: 7.37 - training_loss: -16.3126 - val_loss: -17.0146
[ INFO : 2022-07-26 20:54:02,860 ] - Epoch 14: val_loss improved from -17.0146 to -17.0411, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:03,124 ] - Epoch 14/500 - time: 7.36 - training_loss: -16.3654 - val_loss: -17.0411
[ INFO : 2022-07-26 20:54:10,383 ] - Epoch 15: val_loss did not improve from -17.0411
[ INFO : 2022-07-26 20:54:10,383 ] - Epoch 15/500 - time: 7.26 - training_loss: -16.4108 - val_loss: -17.0063
[ INFO : 2022-07-26 20:54:17,704 ] - Epoch 16: val_loss did not improve from -17.0411
[ INFO : 2022-07-26 20:54:17,704 ] - Epoch 16/500 - time: 7.32 - training_loss: -16.4512 - val_loss: -17.0389
[ INFO : 2022-07-26 20:54:24,802 ] - Epoch 17: val_loss improved from -17.0411 to -17.0769, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:25,055 ] - Epoch 17/500 - time: 7.35 - training_loss: -16.4881 - val_loss: -17.0769
[ INFO : 2022-07-26 20:54:32,496 ] - Epoch 18: val_loss improved from -17.0769 to -17.1289, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:32,766 ] - Epoch 18/500 - time: 7.71 - training_loss: -16.5232 - val_loss: -17.1289
[ INFO : 2022-07-26 20:54:39,897 ] - Epoch 19: val_loss improved from -17.1289 to -17.1379, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:40,174 ] - Epoch 19/500 - time: 7.41 - training_loss: -16.5562 - val_loss: -17.1379
[ INFO : 2022-07-26 20:54:47,376 ] - Epoch 20: val_loss improved from -17.1379 to -17.1634, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:47,651 ] - Epoch 20/500 - time: 7.48 - training_loss: -16.5874 - val_loss: -17.1634
[ INFO : 2022-07-26 20:54:55,487 ] - Epoch 21: val_loss improved from -17.1634 to -17.1826, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:54:55,765 ] - Epoch 21/500 - time: 8.11 - training_loss: -16.6162 - val_loss: -17.1826
[ INFO : 2022-07-26 20:55:02,819 ] - Epoch 22: val_loss improved from -17.1826 to -17.2081, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:55:03,103 ] - Epoch 22/500 - time: 7.34 - training_loss: -16.6430 - val_loss: -17.2081
[ INFO : 2022-07-26 20:55:10,140 ] - Epoch 23: val_loss did not improve from -17.2081
[ INFO : 2022-07-26 20:55:10,141 ] - Epoch 23/500 - time: 7.04 - training_loss: -16.6680 - val_loss: -17.1891
[ INFO : 2022-07-26 20:55:17,160 ] - Epoch 24: val_loss improved from -17.2081 to -17.2160, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:55:17,415 ] - Epoch 24/500 - time: 7.27 - training_loss: -16.6911 - val_loss: -17.2160
[ INFO : 2022-07-26 20:55:24,389 ] - Epoch 25: val_loss did not improve from -17.2160
[ INFO : 2022-07-26 20:55:24,390 ] - Epoch 25/500 - time: 6.97 - training_loss: -16.7126 - val_loss: -17.2063
[ INFO : 2022-07-26 20:55:31,368 ] - Epoch 26: val_loss improved from -17.2160 to -17.2301, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:55:31,624 ] - Epoch 26/500 - time: 7.23 - training_loss: -16.7325 - val_loss: -17.2301
[ INFO : 2022-07-26 20:55:38,617 ] - Epoch 27: val_loss improved from -17.2301 to -17.2388, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:55:38,895 ] - Epoch 27/500 - time: 7.27 - training_loss: -16.7515 - val_loss: -17.2388
[ INFO : 2022-07-26 20:55:45,864 ] - Epoch 28: val_loss improved from -17.2388 to -17.2484, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:55:46,129 ] - Epoch 28/500 - time: 7.23 - training_loss: -16.7693 - val_loss: -17.2484
[ INFO : 2022-07-26 20:55:53,106 ] - Epoch 29: val_loss did not improve from -17.2484
[ INFO : 2022-07-26 20:55:53,106 ] - Epoch 29/500 - time: 6.98 - training_loss: -16.7862 - val_loss: -17.2385
[ INFO : 2022-07-26 20:56:00,086 ] - Epoch 30: val_loss improved from -17.2484 to -17.2517, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:56:00,355 ] - Epoch 30/500 - time: 7.25 - training_loss: -16.8020 - val_loss: -17.2517
[ INFO : 2022-07-26 20:56:07,314 ] - Epoch 31: val_loss did not improve from -17.2517
[ INFO : 2022-07-26 20:56:07,314 ] - Epoch 31/500 - time: 6.96 - training_loss: -16.8168 - val_loss: -17.2475
[ INFO : 2022-07-26 20:56:14,267 ] - Epoch 32: val_loss did not improve from -17.2517
[ INFO : 2022-07-26 20:56:14,267 ] - Epoch 32/500 - time: 6.95 - training_loss: -16.8307 - val_loss: -17.2368
[ INFO : 2022-07-26 20:56:21,247 ] - Epoch 33: val_loss did not improve from -17.2517
[ INFO : 2022-07-26 20:56:21,247 ] - Epoch 33/500 - time: 6.98 - training_loss: -16.8437 - val_loss: -17.2352
[ INFO : 2022-07-26 20:56:28,228 ] - Epoch 34: val_loss improved from -17.2517 to -17.2545, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:56:28,494 ] - Epoch 34/500 - time: 7.25 - training_loss: -16.8558 - val_loss: -17.2545
[ INFO : 2022-07-26 20:56:35,496 ] - Epoch 35: val_loss did not improve from -17.2545
[ INFO : 2022-07-26 20:56:35,496 ] - Epoch 35/500 - time: 7.00 - training_loss: -16.8675 - val_loss: -17.2501
[ INFO : 2022-07-26 20:56:42,476 ] - Epoch 36: val_loss improved from -17.2545 to -17.2634, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:56:42,758 ] - Epoch 36/500 - time: 7.26 - training_loss: -16.8787 - val_loss: -17.2634
[ INFO : 2022-07-26 20:56:49,759 ] - Epoch 37: val_loss did not improve from -17.2634
[ INFO : 2022-07-26 20:56:49,759 ] - Epoch 37/500 - time: 7.00 - training_loss: -16.8893 - val_loss: -17.2585
[ INFO : 2022-07-26 20:56:56,807 ] - Epoch 38: val_loss improved from -17.2634 to -17.2705, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:56:57,074 ] - Epoch 38/500 - time: 7.31 - training_loss: -16.8995 - val_loss: -17.2705
[ INFO : 2022-07-26 20:57:04,048 ] - Epoch 39: val_loss did not improve from -17.2705
[ INFO : 2022-07-26 20:57:04,048 ] - Epoch 39/500 - time: 6.97 - training_loss: -16.9092 - val_loss: -17.2605
[ INFO : 2022-07-26 20:57:11,022 ] - Epoch 40: val_loss improved from -17.2705 to -17.2753, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:11,348 ] - Epoch 40/500 - time: 7.30 - training_loss: -16.9184 - val_loss: -17.2753
[ INFO : 2022-07-26 20:57:18,310 ] - Epoch 41: val_loss improved from -17.2753 to -17.2835, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:18,574 ] - Epoch 41/500 - time: 7.23 - training_loss: -16.9274 - val_loss: -17.2835
[ INFO : 2022-07-26 20:57:25,556 ] - Epoch 42: val_loss improved from -17.2835 to -17.2950, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:25,810 ] - Epoch 42/500 - time: 7.24 - training_loss: -16.9363 - val_loss: -17.2950
[ INFO : 2022-07-26 20:57:32,828 ] - Epoch 43: val_loss improved from -17.2950 to -17.2994, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:33,102 ] - Epoch 43/500 - time: 7.29 - training_loss: -16.9448 - val_loss: -17.2994
[ INFO : 2022-07-26 20:57:40,061 ] - Epoch 44: val_loss improved from -17.2994 to -17.3009, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:40,343 ] - Epoch 44/500 - time: 7.24 - training_loss: -16.9530 - val_loss: -17.3009
[ INFO : 2022-07-26 20:57:47,337 ] - Epoch 45: val_loss improved from -17.3009 to -17.3031, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:47,625 ] - Epoch 45/500 - time: 7.28 - training_loss: -16.9609 - val_loss: -17.3031
[ INFO : 2022-07-26 20:57:54,588 ] - Epoch 46: val_loss improved from -17.3031 to -17.3056, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:57:54,880 ] - Epoch 46/500 - time: 7.25 - training_loss: -16.9685 - val_loss: -17.3056
[ INFO : 2022-07-26 20:58:01,847 ] - Epoch 47: val_loss improved from -17.3056 to -17.3067, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:02,121 ] - Epoch 47/500 - time: 7.24 - training_loss: -16.9758 - val_loss: -17.3067
[ INFO : 2022-07-26 20:58:09,074 ] - Epoch 48: val_loss improved from -17.3067 to -17.3080, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:09,334 ] - Epoch 48/500 - time: 7.21 - training_loss: -16.9829 - val_loss: -17.3080
[ INFO : 2022-07-26 20:58:16,296 ] - Epoch 49: val_loss improved from -17.3080 to -17.3086, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:16,579 ] - Epoch 49/500 - time: 7.24 - training_loss: -16.9896 - val_loss: -17.3086
[ INFO : 2022-07-26 20:58:23,550 ] - Epoch 50: val_loss improved from -17.3086 to -17.3090, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:23,821 ] - Epoch 50/500 - time: 7.24 - training_loss: -16.9961 - val_loss: -17.3090
[ INFO : 2022-07-26 20:58:30,782 ] - Epoch 51: val_loss improved from -17.3090 to -17.3096, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:31,081 ] - Epoch 51/500 - time: 7.26 - training_loss: -17.0024 - val_loss: -17.3096
[ INFO : 2022-07-26 20:58:38,045 ] - Epoch 52: val_loss improved from -17.3096 to -17.3105, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:38,317 ] - Epoch 52/500 - time: 7.24 - training_loss: -17.0084 - val_loss: -17.3105
[ INFO : 2022-07-26 20:58:45,280 ] - Epoch 53: val_loss improved from -17.3105 to -17.3105, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:45,578 ] - Epoch 53/500 - time: 7.26 - training_loss: -17.0142 - val_loss: -17.3105
[ INFO : 2022-07-26 20:58:52,538 ] - Epoch 54: val_loss improved from -17.3105 to -17.3111, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:58:52,816 ] - Epoch 54/500 - time: 7.24 - training_loss: -17.0198 - val_loss: -17.3111
[ INFO : 2022-07-26 20:58:59,954 ] - Epoch 55: val_loss did not improve from -17.3111
[ INFO : 2022-07-26 20:58:59,954 ] - Epoch 55/500 - time: 7.14 - training_loss: -17.0251 - val_loss: -17.3109
[ INFO : 2022-07-26 20:59:07,166 ] - Epoch 56: val_loss improved from -17.3111 to -17.3114, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:59:07,463 ] - Epoch 56/500 - time: 7.51 - training_loss: -17.0303 - val_loss: -17.3114
[ INFO : 2022-07-26 20:59:14,688 ] - Epoch 57: val_loss improved from -17.3114 to -17.3115, saving model to ./DGCCA.model
[ INFO : 2022-07-26 20:59:14,956 ] - Epoch 57/500 - time: 7.49 - training_loss: -17.0354 - val_loss: -17.3115
[ INFO : 2022-07-26 20:59:22,161 ] - Epoch 58: val_loss did not improve from -17.3115
[ INFO : 2022-07-26 20:59:22,161 ] - Epoch 58/500 - time: 7.20 - training_loss: -17.0402 - val_loss: -17.3115
[ INFO : 2022-07-26 20:59:29,286 ] - Epoch 59: val_loss did not improve from -17.3115
[ INFO : 2022-07-26 20:59:29,286 ] - Epoch 59/500 - time: 7.13 - training_loss: -17.0449 - val_loss: -17.3114
[ INFO : 2022-07-26 20:59:36,270 ] - Epoch 60: val_loss did not improve from -17.3115
[ INFO : 2022-07-26 20:59:36,270 ] - Epoch 60/500 - time: 6.98 - training_loss: -17.0494 - val_loss: -17.3109
[ INFO : 2022-07-26 20:59:43,280 ] - Epoch 61: val_loss did not improve from -17.3115
[ INFO : 2022-07-26 20:59:43,280 ] - Epoch 61/500 - time: 7.01 - training_loss: -17.0538 - val_loss: -17.3111
[ INFO : 2022-07-26 20:59:50,342 ] - Epoch 62: val_loss did not improve from -17.3115
[ INFO : 2022-07-26 20:59:50,342 ] - Epoch 62/500 - time: 7.06 - training_loss: -17.0580 - val_loss: -17.3074
[ INFO : 2022-07-26 20:59:58,035 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 20:59:58,036 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.01
    lr: 0.01
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 21:00:06,741 ] - Epoch 1: val_loss improved from 0.0000 to -14.1843, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:07,101 ] - Epoch 1/500 - time: 9.07 - training_loss: -13.3859 - val_loss: -14.1843
[ INFO : 2022-07-26 21:00:14,415 ] - Epoch 2: val_loss improved from -14.1843 to -14.5318, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:14,687 ] - Epoch 2/500 - time: 7.59 - training_loss: -13.8007 - val_loss: -14.5318
[ INFO : 2022-07-26 21:00:21,752 ] - Epoch 3: val_loss improved from -14.5318 to -14.9913, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:22,018 ] - Epoch 3/500 - time: 7.33 - training_loss: -14.2116 - val_loss: -14.9913
[ INFO : 2022-07-26 21:00:29,775 ] - Epoch 4: val_loss improved from -14.9913 to -15.5165, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:30,039 ] - Epoch 4/500 - time: 8.02 - training_loss: -14.5337 - val_loss: -15.5165
[ INFO : 2022-07-26 21:00:34,951 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 21:00:34,951 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 21:00:43,386 ] - Epoch 1: val_loss improved from 0.0000 to -15.0727, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:43,665 ] - Epoch 1/500 - time: 8.71 - training_loss: -14.6889 - val_loss: -15.0727
[ INFO : 2022-07-26 21:00:50,686 ] - Epoch 2: val_loss improved from -15.0727 to -15.4496, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:50,977 ] - Epoch 2/500 - time: 7.31 - training_loss: -15.1255 - val_loss: -15.4496
[ INFO : 2022-07-26 21:00:58,014 ] - Epoch 3: val_loss improved from -15.4496 to -15.7894, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:00:58,282 ] - Epoch 3/500 - time: 7.30 - training_loss: -15.4121 - val_loss: -15.7894
[ INFO : 2022-07-26 21:01:05,317 ] - Epoch 4: val_loss improved from -15.7894 to -16.0063, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:05,599 ] - Epoch 4/500 - time: 7.32 - training_loss: -15.6294 - val_loss: -16.0063
[ INFO : 2022-07-26 21:01:12,623 ] - Epoch 5: val_loss improved from -16.0063 to -16.2739, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:12,876 ] - Epoch 5/500 - time: 7.28 - training_loss: -15.8058 - val_loss: -16.2739
[ INFO : 2022-07-26 21:01:19,860 ] - Epoch 6: val_loss improved from -16.2739 to -16.6026, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:20,135 ] - Epoch 6/500 - time: 7.26 - training_loss: -15.9534 - val_loss: -16.6026
[ INFO : 2022-07-26 21:01:27,154 ] - Epoch 7: val_loss improved from -16.6026 to -16.7550, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:27,436 ] - Epoch 7/500 - time: 7.30 - training_loss: -16.0671 - val_loss: -16.7550
[ INFO : 2022-07-26 21:01:34,414 ] - Epoch 8: val_loss improved from -16.7550 to -16.8692, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:34,675 ] - Epoch 8/500 - time: 7.24 - training_loss: -16.1663 - val_loss: -16.8692
[ INFO : 2022-07-26 21:01:41,653 ] - Epoch 9: val_loss improved from -16.8692 to -16.9635, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:41,921 ] - Epoch 9/500 - time: 7.25 - training_loss: -16.2539 - val_loss: -16.9635
[ INFO : 2022-07-26 21:01:48,919 ] - Epoch 10: val_loss improved from -16.9635 to -17.0021, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:49,173 ] - Epoch 10/500 - time: 7.25 - training_loss: -16.3307 - val_loss: -17.0021
[ INFO : 2022-07-26 21:01:56,154 ] - Epoch 11: val_loss improved from -17.0021 to -17.0693, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:01:56,459 ] - Epoch 11/500 - time: 7.29 - training_loss: -16.3980 - val_loss: -17.0693
[ INFO : 2022-07-26 21:02:03,461 ] - Epoch 12: val_loss improved from -17.0693 to -17.0897, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:03,785 ] - Epoch 12/500 - time: 7.33 - training_loss: -16.4569 - val_loss: -17.0897
[ INFO : 2022-07-26 21:02:10,782 ] - Epoch 13: val_loss improved from -17.0897 to -17.1199, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:11,049 ] - Epoch 13/500 - time: 7.26 - training_loss: -16.5065 - val_loss: -17.1199
[ INFO : 2022-07-26 21:02:18,061 ] - Epoch 14: val_loss improved from -17.1199 to -17.1353, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:18,337 ] - Epoch 14/500 - time: 7.29 - training_loss: -16.5523 - val_loss: -17.1353
[ INFO : 2022-07-26 21:02:25,392 ] - Epoch 15: val_loss improved from -17.1353 to -17.1729, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:25,892 ] - Epoch 15/500 - time: 7.55 - training_loss: -16.5935 - val_loss: -17.1729
[ INFO : 2022-07-26 21:02:33,186 ] - Epoch 16: val_loss did not improve from -17.1729
[ INFO : 2022-07-26 21:02:33,187 ] - Epoch 16/500 - time: 7.29 - training_loss: -16.6307 - val_loss: -17.1525
[ INFO : 2022-07-26 21:02:40,226 ] - Epoch 17: val_loss improved from -17.1729 to -17.1983, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:40,489 ] - Epoch 17/500 - time: 7.30 - training_loss: -16.6638 - val_loss: -17.1983
[ INFO : 2022-07-26 21:02:47,821 ] - Epoch 18: val_loss improved from -17.1983 to -17.2088, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:02:48,127 ] - Epoch 18/500 - time: 7.64 - training_loss: -16.6940 - val_loss: -17.2088
[ INFO : 2022-07-26 21:02:55,100 ] - Epoch 19: val_loss did not improve from -17.2088
[ INFO : 2022-07-26 21:02:55,100 ] - Epoch 19/500 - time: 6.97 - training_loss: -16.7216 - val_loss: -17.1961
[ INFO : 2022-07-26 21:03:02,312 ] - Epoch 20: val_loss improved from -17.2088 to -17.2197, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:03:02,578 ] - Epoch 20/500 - time: 7.48 - training_loss: -16.7466 - val_loss: -17.2197
[ INFO : 2022-07-26 21:03:09,571 ] - Epoch 21: val_loss did not improve from -17.2197
[ INFO : 2022-07-26 21:03:09,572 ] - Epoch 21/500 - time: 6.99 - training_loss: -16.7700 - val_loss: -17.2167
[ INFO : 2022-07-26 21:03:16,562 ] - Epoch 22: val_loss improved from -17.2197 to -17.2228, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:03:16,813 ] - Epoch 22/500 - time: 7.24 - training_loss: -16.7915 - val_loss: -17.2228
[ INFO : 2022-07-26 21:03:23,759 ] - Epoch 23: val_loss improved from -17.2228 to -17.2275, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:03:24,002 ] - Epoch 23/500 - time: 7.19 - training_loss: -16.8113 - val_loss: -17.2275
[ INFO : 2022-07-26 21:03:30,974 ] - Epoch 24: val_loss did not improve from -17.2275
[ INFO : 2022-07-26 21:03:30,974 ] - Epoch 24/500 - time: 6.97 - training_loss: -16.8294 - val_loss: -17.2169
[ INFO : 2022-07-26 21:03:37,944 ] - Epoch 25: val_loss improved from -17.2275 to -17.2399, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:03:38,186 ] - Epoch 25/500 - time: 7.21 - training_loss: -16.8460 - val_loss: -17.2399
[ INFO : 2022-07-26 21:03:45,288 ] - Epoch 26: val_loss did not improve from -17.2399
[ INFO : 2022-07-26 21:03:45,288 ] - Epoch 26/500 - time: 7.10 - training_loss: -16.8613 - val_loss: -17.2223
[ INFO : 2022-07-26 21:03:52,322 ] - Epoch 27: val_loss did not improve from -17.2399
[ INFO : 2022-07-26 21:03:52,323 ] - Epoch 27/500 - time: 7.04 - training_loss: -16.8754 - val_loss: -17.2372
[ INFO : 2022-07-26 21:03:59,518 ] - Epoch 28: val_loss did not improve from -17.2399
[ INFO : 2022-07-26 21:03:59,518 ] - Epoch 28/500 - time: 7.20 - training_loss: -16.8888 - val_loss: -17.2133
[ INFO : 2022-07-26 21:04:06,474 ] - Epoch 29: val_loss improved from -17.2399 to -17.2478, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:04:06,750 ] - Epoch 29/500 - time: 7.23 - training_loss: -16.9012 - val_loss: -17.2478
[ INFO : 2022-07-26 21:04:13,997 ] - Epoch 30: val_loss did not improve from -17.2478
[ INFO : 2022-07-26 21:04:13,997 ] - Epoch 30/500 - time: 7.25 - training_loss: -16.9132 - val_loss: -17.2420
[ INFO : 2022-07-26 21:04:21,523 ] - Epoch 31: val_loss improved from -17.2478 to -17.2527, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:04:21,790 ] - Epoch 31/500 - time: 7.79 - training_loss: -16.9245 - val_loss: -17.2527
[ INFO : 2022-07-26 21:04:28,982 ] - Epoch 32: val_loss improved from -17.2527 to -17.2566, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:04:29,221 ] - Epoch 32/500 - time: 7.43 - training_loss: -16.9355 - val_loss: -17.2566
[ INFO : 2022-07-26 21:04:36,213 ] - Epoch 33: val_loss improved from -17.2566 to -17.2720, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:04:36,469 ] - Epoch 33/500 - time: 7.25 - training_loss: -16.9460 - val_loss: -17.2720
[ INFO : 2022-07-26 21:04:43,433 ] - Epoch 34: val_loss did not improve from -17.2720
[ INFO : 2022-07-26 21:04:43,433 ] - Epoch 34/500 - time: 6.96 - training_loss: -16.9559 - val_loss: -17.2634
[ INFO : 2022-07-26 21:04:50,376 ] - Epoch 35: val_loss did not improve from -17.2720
[ INFO : 2022-07-26 21:04:50,377 ] - Epoch 35/500 - time: 6.94 - training_loss: -16.9652 - val_loss: -17.2693
[ INFO : 2022-07-26 21:04:57,353 ] - Epoch 36: val_loss did not improve from -17.2720
[ INFO : 2022-07-26 21:04:57,353 ] - Epoch 36/500 - time: 6.98 - training_loss: -16.9741 - val_loss: -17.2700
[ INFO : 2022-07-26 21:05:04,335 ] - Epoch 37: val_loss did not improve from -17.2720
[ INFO : 2022-07-26 21:05:04,335 ] - Epoch 37/500 - time: 6.98 - training_loss: -16.9825 - val_loss: -17.2673
[ INFO : 2022-07-26 21:05:11,304 ] - Epoch 38: val_loss improved from -17.2720 to -17.2739, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:11,609 ] - Epoch 38/500 - time: 7.27 - training_loss: -16.9905 - val_loss: -17.2739
[ INFO : 2022-07-26 21:05:18,578 ] - Epoch 39: val_loss did not improve from -17.2739
[ INFO : 2022-07-26 21:05:18,579 ] - Epoch 39/500 - time: 6.97 - training_loss: -16.9981 - val_loss: -17.2651
[ INFO : 2022-07-26 21:05:25,553 ] - Epoch 40: val_loss improved from -17.2739 to -17.2784, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:25,815 ] - Epoch 40/500 - time: 7.24 - training_loss: -17.0053 - val_loss: -17.2784
[ INFO : 2022-07-26 21:05:32,810 ] - Epoch 41: val_loss improved from -17.2784 to -17.2894, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:33,099 ] - Epoch 41/500 - time: 7.28 - training_loss: -17.0123 - val_loss: -17.2894
[ INFO : 2022-07-26 21:05:40,067 ] - Epoch 42: val_loss improved from -17.2894 to -17.2945, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:40,345 ] - Epoch 42/500 - time: 7.25 - training_loss: -17.0192 - val_loss: -17.2945
[ INFO : 2022-07-26 21:05:47,354 ] - Epoch 43: val_loss improved from -17.2945 to -17.2999, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:47,617 ] - Epoch 43/500 - time: 7.27 - training_loss: -17.0258 - val_loss: -17.2999
[ INFO : 2022-07-26 21:05:54,572 ] - Epoch 44: val_loss improved from -17.2999 to -17.3044, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:05:54,873 ] - Epoch 44/500 - time: 7.26 - training_loss: -17.0322 - val_loss: -17.3044
[ INFO : 2022-07-26 21:06:01,837 ] - Epoch 45: val_loss did not improve from -17.3044
[ INFO : 2022-07-26 21:06:01,837 ] - Epoch 45/500 - time: 6.96 - training_loss: -17.0384 - val_loss: -17.3038
[ INFO : 2022-07-26 21:06:08,834 ] - Epoch 46: val_loss improved from -17.3044 to -17.3077, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:06:09,099 ] - Epoch 46/500 - time: 7.26 - training_loss: -17.0444 - val_loss: -17.3077
[ INFO : 2022-07-26 21:06:16,035 ] - Epoch 47: val_loss improved from -17.3077 to -17.3077, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:06:16,307 ] - Epoch 47/500 - time: 7.21 - training_loss: -17.0501 - val_loss: -17.3077
[ INFO : 2022-07-26 21:06:23,290 ] - Epoch 48: val_loss improved from -17.3077 to -17.3092, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:06:23,562 ] - Epoch 48/500 - time: 7.25 - training_loss: -17.0556 - val_loss: -17.3092
[ INFO : 2022-07-26 21:06:30,525 ] - Epoch 49: val_loss improved from -17.3092 to -17.3099, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:06:30,807 ] - Epoch 49/500 - time: 7.25 - training_loss: -17.0609 - val_loss: -17.3099
[ INFO : 2022-07-26 21:06:37,772 ] - Epoch 50: val_loss did not improve from -17.3099
[ INFO : 2022-07-26 21:06:37,772 ] - Epoch 50/500 - time: 6.96 - training_loss: -17.0660 - val_loss: -17.3096
[ INFO : 2022-07-26 21:06:44,718 ] - Epoch 51: val_loss improved from -17.3099 to -17.3107, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:06:44,989 ] - Epoch 51/500 - time: 7.22 - training_loss: -17.0709 - val_loss: -17.3107
[ INFO : 2022-07-26 21:06:52,304 ] - Epoch 52: val_loss did not improve from -17.3107
[ INFO : 2022-07-26 21:06:52,304 ] - Epoch 52/500 - time: 7.32 - training_loss: -17.0756 - val_loss: -17.3101
[ INFO : 2022-07-26 21:06:59,278 ] - Epoch 53: val_loss did not improve from -17.3107
[ INFO : 2022-07-26 21:06:59,278 ] - Epoch 53/500 - time: 6.97 - training_loss: -17.0801 - val_loss: -17.3099
[ INFO : 2022-07-26 21:07:06,275 ] - Epoch 54: val_loss improved from -17.3107 to -17.3112, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:07:06,549 ] - Epoch 54/500 - time: 7.27 - training_loss: -17.0845 - val_loss: -17.3112
[ INFO : 2022-07-26 21:07:13,526 ] - Epoch 55: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:13,526 ] - Epoch 55/500 - time: 6.98 - training_loss: -17.0887 - val_loss: -17.3089
[ INFO : 2022-07-26 21:07:20,511 ] - Epoch 56: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:20,511 ] - Epoch 56/500 - time: 6.99 - training_loss: -17.0928 - val_loss: -17.3110
[ INFO : 2022-07-26 21:07:27,491 ] - Epoch 57: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:27,492 ] - Epoch 57/500 - time: 6.98 - training_loss: -17.0967 - val_loss: -17.3075
[ INFO : 2022-07-26 21:07:34,435 ] - Epoch 58: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:34,435 ] - Epoch 58/500 - time: 6.94 - training_loss: -17.1005 - val_loss: -17.3094
[ INFO : 2022-07-26 21:07:41,416 ] - Epoch 59: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:41,417 ] - Epoch 59/500 - time: 6.98 - training_loss: -17.1041 - val_loss: -17.3066
[ INFO : 2022-07-26 21:07:48,394 ] - Epoch 60: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:48,394 ] - Epoch 60/500 - time: 6.98 - training_loss: -17.1076 - val_loss: -17.3062
[ INFO : 2022-07-26 21:07:55,365 ] - Epoch 61: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:07:55,365 ] - Epoch 61/500 - time: 6.97 - training_loss: -17.1109 - val_loss: -17.3040
[ INFO : 2022-07-26 21:08:02,339 ] - Epoch 62: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:02,339 ] - Epoch 62/500 - time: 6.97 - training_loss: -17.1142 - val_loss: -17.3027
[ INFO : 2022-07-26 21:08:09,284 ] - Epoch 63: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:09,284 ] - Epoch 63/500 - time: 6.94 - training_loss: -17.1173 - val_loss: -17.3005
[ INFO : 2022-07-26 21:08:16,273 ] - Epoch 64: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:16,273 ] - Epoch 64/500 - time: 6.99 - training_loss: -17.1203 - val_loss: -17.3043
[ INFO : 2022-07-26 21:08:23,299 ] - Epoch 65: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:23,299 ] - Epoch 65/500 - time: 7.03 - training_loss: -17.1232 - val_loss: -17.2966
[ INFO : 2022-07-26 21:08:30,364 ] - Epoch 66: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:30,364 ] - Epoch 66/500 - time: 7.06 - training_loss: -17.1261 - val_loss: -17.3053
[ INFO : 2022-07-26 21:08:37,367 ] - Epoch 67: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:37,367 ] - Epoch 67/500 - time: 7.00 - training_loss: -17.1288 - val_loss: -17.3000
[ INFO : 2022-07-26 21:08:44,347 ] - Epoch 68: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:44,347 ] - Epoch 68/500 - time: 6.98 - training_loss: -17.1315 - val_loss: -17.3011
[ INFO : 2022-07-26 21:08:51,331 ] - Epoch 69: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:51,331 ] - Epoch 69/500 - time: 6.98 - training_loss: -17.1341 - val_loss: -17.3056
[ INFO : 2022-07-26 21:08:58,298 ] - Epoch 70: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:08:58,298 ] - Epoch 70/500 - time: 6.97 - training_loss: -17.1366 - val_loss: -17.2988
[ INFO : 2022-07-26 21:09:05,274 ] - Epoch 71: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:05,275 ] - Epoch 71/500 - time: 6.98 - training_loss: -17.1391 - val_loss: -17.3079
[ INFO : 2022-07-26 21:09:12,824 ] - Epoch 72: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:12,824 ] - Epoch 72/500 - time: 7.55 - training_loss: -17.1415 - val_loss: -17.3024
[ INFO : 2022-07-26 21:09:20,217 ] - Epoch 73: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:20,217 ] - Epoch 73/500 - time: 7.39 - training_loss: -17.1439 - val_loss: -17.3060
[ INFO : 2022-07-26 21:09:27,490 ] - Epoch 74: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:27,490 ] - Epoch 74/500 - time: 7.27 - training_loss: -17.1461 - val_loss: -17.3052
[ INFO : 2022-07-26 21:09:34,549 ] - Epoch 75: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:34,549 ] - Epoch 75/500 - time: 7.06 - training_loss: -17.1484 - val_loss: -17.3056
[ INFO : 2022-07-26 21:09:41,764 ] - Epoch 76: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:41,764 ] - Epoch 76/500 - time: 7.22 - training_loss: -17.1505 - val_loss: -17.3016
[ INFO : 2022-07-26 21:09:49,048 ] - Epoch 77: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:49,048 ] - Epoch 77/500 - time: 7.28 - training_loss: -17.1526 - val_loss: -17.3053
[ INFO : 2022-07-26 21:09:56,216 ] - Epoch 78: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:09:56,216 ] - Epoch 78/500 - time: 7.17 - training_loss: -17.1546 - val_loss: -17.3008
[ INFO : 2022-07-26 21:10:03,380 ] - Epoch 79: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:10:03,380 ] - Epoch 79/500 - time: 7.16 - training_loss: -17.1566 - val_loss: -17.3006
[ INFO : 2022-07-26 21:10:10,480 ] - Epoch 80: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:10:10,480 ] - Epoch 80/500 - time: 7.10 - training_loss: -17.1585 - val_loss: -17.3033
[ INFO : 2022-07-26 21:10:17,532 ] - Epoch 81: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:10:17,533 ] - Epoch 81/500 - time: 7.05 - training_loss: -17.1604 - val_loss: -17.3068
[ INFO : 2022-07-26 21:10:24,577 ] - Epoch 82: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:10:24,578 ] - Epoch 82/500 - time: 7.04 - training_loss: -17.1623 - val_loss: -17.3075
[ INFO : 2022-07-26 21:10:31,635 ] - Epoch 83: val_loss did not improve from -17.3112
[ INFO : 2022-07-26 21:10:31,635 ] - Epoch 83/500 - time: 7.06 - training_loss: -17.1642 - val_loss: -17.3101
[ INFO : 2022-07-26 21:10:38,677 ] - Epoch 84: val_loss improved from -17.3112 to -17.3122, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:10:38,972 ] - Epoch 84/500 - time: 7.34 - training_loss: -17.1660 - val_loss: -17.3122
[ INFO : 2022-07-26 21:10:45,990 ] - Epoch 85: val_loss improved from -17.3122 to -17.3129, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:10:46,268 ] - Epoch 85/500 - time: 7.30 - training_loss: -17.1678 - val_loss: -17.3129
[ INFO : 2022-07-26 21:10:53,315 ] - Epoch 86: val_loss improved from -17.3129 to -17.3134, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:10:53,602 ] - Epoch 86/500 - time: 7.33 - training_loss: -17.1695 - val_loss: -17.3134
[ INFO : 2022-07-26 21:11:00,642 ] - Epoch 87: val_loss improved from -17.3134 to -17.3140, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:00,938 ] - Epoch 87/500 - time: 7.34 - training_loss: -17.1712 - val_loss: -17.3140
[ INFO : 2022-07-26 21:11:07,981 ] - Epoch 88: val_loss improved from -17.3140 to -17.3143, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:08,242 ] - Epoch 88/500 - time: 7.30 - training_loss: -17.1729 - val_loss: -17.3143
[ INFO : 2022-07-26 21:11:15,279 ] - Epoch 89: val_loss improved from -17.3143 to -17.3143, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:15,554 ] - Epoch 89/500 - time: 7.31 - training_loss: -17.1746 - val_loss: -17.3143
[ INFO : 2022-07-26 21:11:22,585 ] - Epoch 90: val_loss improved from -17.3143 to -17.3145, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:22,859 ] - Epoch 90/500 - time: 7.30 - training_loss: -17.1762 - val_loss: -17.3145
[ INFO : 2022-07-26 21:11:29,899 ] - Epoch 91: val_loss improved from -17.3145 to -17.3147, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:30,175 ] - Epoch 91/500 - time: 7.32 - training_loss: -17.1777 - val_loss: -17.3147
[ INFO : 2022-07-26 21:11:37,217 ] - Epoch 92: val_loss improved from -17.3147 to -17.3148, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:37,490 ] - Epoch 92/500 - time: 7.32 - training_loss: -17.1793 - val_loss: -17.3148
[ INFO : 2022-07-26 21:11:44,522 ] - Epoch 93: val_loss improved from -17.3148 to -17.3149, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:44,808 ] - Epoch 93/500 - time: 7.32 - training_loss: -17.1808 - val_loss: -17.3149
[ INFO : 2022-07-26 21:11:51,844 ] - Epoch 94: val_loss improved from -17.3149 to -17.3150, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:52,124 ] - Epoch 94/500 - time: 7.32 - training_loss: -17.1823 - val_loss: -17.3150
[ INFO : 2022-07-26 21:11:59,197 ] - Epoch 95: val_loss improved from -17.3150 to -17.3150, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:11:59,546 ] - Epoch 95/500 - time: 7.42 - training_loss: -17.1837 - val_loss: -17.3150
[ INFO : 2022-07-26 21:12:06,585 ] - Epoch 96: val_loss improved from -17.3150 to -17.3151, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:06,859 ] - Epoch 96/500 - time: 7.31 - training_loss: -17.1851 - val_loss: -17.3151
[ INFO : 2022-07-26 21:12:13,870 ] - Epoch 97: val_loss did not improve from -17.3151
[ INFO : 2022-07-26 21:12:13,870 ] - Epoch 97/500 - time: 7.01 - training_loss: -17.1865 - val_loss: -17.3150
[ INFO : 2022-07-26 21:12:20,908 ] - Epoch 98: val_loss improved from -17.3151 to -17.3151, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:21,235 ] - Epoch 98/500 - time: 7.37 - training_loss: -17.1879 - val_loss: -17.3151
[ INFO : 2022-07-26 21:12:28,245 ] - Epoch 99: val_loss improved from -17.3151 to -17.3151, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:28,510 ] - Epoch 99/500 - time: 7.27 - training_loss: -17.1892 - val_loss: -17.3151
[ INFO : 2022-07-26 21:12:35,529 ] - Epoch 100: val_loss improved from -17.3151 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:35,825 ] - Epoch 100/500 - time: 7.32 - training_loss: -17.1905 - val_loss: -17.3152
[ INFO : 2022-07-26 21:12:42,823 ] - Epoch 101: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:43,107 ] - Epoch 101/500 - time: 7.28 - training_loss: -17.1918 - val_loss: -17.3152
[ INFO : 2022-07-26 21:12:50,134 ] - Epoch 102: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:50,403 ] - Epoch 102/500 - time: 7.30 - training_loss: -17.1931 - val_loss: -17.3152
[ INFO : 2022-07-26 21:12:57,425 ] - Epoch 103: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:12:57,714 ] - Epoch 103/500 - time: 7.31 - training_loss: -17.1943 - val_loss: -17.3152
[ INFO : 2022-07-26 21:13:04,738 ] - Epoch 104: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:04,981 ] - Epoch 104/500 - time: 7.27 - training_loss: -17.1955 - val_loss: -17.3152
[ INFO : 2022-07-26 21:13:12,013 ] - Epoch 105: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:12,301 ] - Epoch 105/500 - time: 7.32 - training_loss: -17.1967 - val_loss: -17.3152
[ INFO : 2022-07-26 21:13:19,316 ] - Epoch 106: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:19,573 ] - Epoch 106/500 - time: 7.27 - training_loss: -17.1979 - val_loss: -17.3152
[ INFO : 2022-07-26 21:13:26,617 ] - Epoch 107: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:26,876 ] - Epoch 107/500 - time: 7.30 - training_loss: -17.1990 - val_loss: -17.3152
[ INFO : 2022-07-26 21:13:33,907 ] - Epoch 108: val_loss improved from -17.3152 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:34,179 ] - Epoch 108/500 - time: 7.30 - training_loss: -17.2001 - val_loss: -17.3153
[ INFO : 2022-07-26 21:13:41,228 ] - Epoch 109: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:13:41,228 ] - Epoch 109/500 - time: 7.05 - training_loss: -17.2012 - val_loss: -17.3153
[ INFO : 2022-07-26 21:13:48,245 ] - Epoch 110: val_loss improved from -17.3153 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:13:48,520 ] - Epoch 110/500 - time: 7.29 - training_loss: -17.2023 - val_loss: -17.3153
[ INFO : 2022-07-26 21:13:55,564 ] - Epoch 111: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:13:55,564 ] - Epoch 111/500 - time: 7.04 - training_loss: -17.2034 - val_loss: -17.3152
[ INFO : 2022-07-26 21:14:02,585 ] - Epoch 112: val_loss improved from -17.3153 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:14:02,978 ] - Epoch 112/500 - time: 7.41 - training_loss: -17.2044 - val_loss: -17.3153
[ INFO : 2022-07-26 21:14:10,095 ] - Epoch 113: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:10,095 ] - Epoch 113/500 - time: 7.12 - training_loss: -17.2054 - val_loss: -17.3153
[ INFO : 2022-07-26 21:14:17,372 ] - Epoch 114: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:17,372 ] - Epoch 114/500 - time: 7.28 - training_loss: -17.2065 - val_loss: -17.3153
[ INFO : 2022-07-26 21:14:24,643 ] - Epoch 115: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:24,643 ] - Epoch 115/500 - time: 7.27 - training_loss: -17.2074 - val_loss: -17.3153
[ INFO : 2022-07-26 21:14:31,896 ] - Epoch 116: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:31,896 ] - Epoch 116/500 - time: 7.25 - training_loss: -17.2084 - val_loss: -17.3152
[ INFO : 2022-07-26 21:14:39,179 ] - Epoch 117: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:39,179 ] - Epoch 117/500 - time: 7.28 - training_loss: -17.2094 - val_loss: -17.3153
[ INFO : 2022-07-26 21:14:46,493 ] - Epoch 118: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:46,493 ] - Epoch 118/500 - time: 7.31 - training_loss: -17.2103 - val_loss: -17.3152
[ INFO : 2022-07-26 21:14:53,793 ] - Epoch 119: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:14:53,793 ] - Epoch 119/500 - time: 7.30 - training_loss: -17.2112 - val_loss: -17.3150
[ INFO : 2022-07-26 21:15:01,167 ] - Epoch 120: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:15:01,167 ] - Epoch 120/500 - time: 7.37 - training_loss: -17.2121 - val_loss: -17.3152
[ INFO : 2022-07-26 21:15:08,421 ] - Epoch 121: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:15:08,421 ] - Epoch 121/500 - time: 7.25 - training_loss: -17.2130 - val_loss: -17.3150
[ INFO : 2022-07-26 21:15:15,688 ] - Epoch 122: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:15:15,689 ] - Epoch 122/500 - time: 7.27 - training_loss: -17.2139 - val_loss: -17.3152
[ INFO : 2022-07-26 21:15:22,986 ] - Epoch 123: val_loss improved from -17.3153 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:15:23,310 ] - Epoch 123/500 - time: 7.62 - training_loss: -17.2148 - val_loss: -17.3153
[ INFO : 2022-07-26 21:15:30,627 ] - Epoch 124: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 21:15:30,627 ] - Epoch 124/500 - time: 7.32 - training_loss: -17.2156 - val_loss: -17.3153
[ INFO : 2022-07-26 21:15:37,996 ] - Epoch 125: val_loss improved from -17.3153 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:15:38,274 ] - Epoch 125/500 - time: 7.65 - training_loss: -17.2165 - val_loss: -17.3153
[ INFO : 2022-07-26 21:15:45,543 ] - Epoch 126: val_loss improved from -17.3153 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:15:45,828 ] - Epoch 126/500 - time: 7.55 - training_loss: -17.2173 - val_loss: -17.3154
[ INFO : 2022-07-26 21:15:53,169 ] - Epoch 127: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 21:15:53,169 ] - Epoch 127/500 - time: 7.34 - training_loss: -17.2181 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:00,442 ] - Epoch 128: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:00,793 ] - Epoch 128/500 - time: 7.62 - training_loss: -17.2189 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:08,065 ] - Epoch 129: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:08,351 ] - Epoch 129/500 - time: 7.56 - training_loss: -17.2197 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:15,582 ] - Epoch 130: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:15,873 ] - Epoch 130/500 - time: 7.52 - training_loss: -17.2205 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:23,146 ] - Epoch 131: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 21:16:23,146 ] - Epoch 131/500 - time: 7.27 - training_loss: -17.2212 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:30,395 ] - Epoch 132: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:30,687 ] - Epoch 132/500 - time: 7.54 - training_loss: -17.2220 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:37,922 ] - Epoch 133: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:38,213 ] - Epoch 133/500 - time: 7.52 - training_loss: -17.2227 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:45,491 ] - Epoch 134: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 21:16:45,492 ] - Epoch 134/500 - time: 7.28 - training_loss: -17.2234 - val_loss: -17.3154
[ INFO : 2022-07-26 21:16:52,745 ] - Epoch 135: val_loss improved from -17.3154 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:16:53,013 ] - Epoch 135/500 - time: 7.52 - training_loss: -17.2242 - val_loss: -17.3154
[ INFO : 2022-07-26 21:17:00,290 ] - Epoch 136: val_loss improved from -17.3154 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:00,592 ] - Epoch 136/500 - time: 7.58 - training_loss: -17.2249 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:07,880 ] - Epoch 137: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 21:17:07,880 ] - Epoch 137/500 - time: 7.29 - training_loss: -17.2256 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:15,136 ] - Epoch 138: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:15,404 ] - Epoch 138/500 - time: 7.52 - training_loss: -17.2262 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:22,678 ] - Epoch 139: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:22,933 ] - Epoch 139/500 - time: 7.53 - training_loss: -17.2269 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:30,203 ] - Epoch 140: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:30,481 ] - Epoch 140/500 - time: 7.55 - training_loss: -17.2276 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:37,771 ] - Epoch 141: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:38,059 ] - Epoch 141/500 - time: 7.58 - training_loss: -17.2282 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:45,361 ] - Epoch 142: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:45,620 ] - Epoch 142/500 - time: 7.56 - training_loss: -17.2289 - val_loss: -17.3155
[ INFO : 2022-07-26 21:17:52,866 ] - Epoch 143: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:17:53,164 ] - Epoch 143/500 - time: 7.54 - training_loss: -17.2295 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:00,492 ] - Epoch 144: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:00,793 ] - Epoch 144/500 - time: 7.63 - training_loss: -17.2302 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:08,154 ] - Epoch 145: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:08,413 ] - Epoch 145/500 - time: 7.62 - training_loss: -17.2308 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:15,761 ] - Epoch 146: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:16,031 ] - Epoch 146/500 - time: 7.62 - training_loss: -17.2314 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:23,391 ] - Epoch 147: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:23,683 ] - Epoch 147/500 - time: 7.65 - training_loss: -17.2320 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:31,015 ] - Epoch 148: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:31,280 ] - Epoch 148/500 - time: 7.60 - training_loss: -17.2326 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:38,572 ] - Epoch 149: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:38,874 ] - Epoch 149/500 - time: 7.59 - training_loss: -17.2332 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:46,136 ] - Epoch 150: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:46,412 ] - Epoch 150/500 - time: 7.54 - training_loss: -17.2338 - val_loss: -17.3155
[ INFO : 2022-07-26 21:18:53,690 ] - Epoch 151: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:18:53,972 ] - Epoch 151/500 - time: 7.56 - training_loss: -17.2343 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:01,264 ] - Epoch 152: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:01,543 ] - Epoch 152/500 - time: 7.57 - training_loss: -17.2349 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:08,730 ] - Epoch 153: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:09,024 ] - Epoch 153/500 - time: 7.48 - training_loss: -17.2355 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:16,068 ] - Epoch 154: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:16,349 ] - Epoch 154/500 - time: 7.32 - training_loss: -17.2360 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:23,465 ] - Epoch 155: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:23,767 ] - Epoch 155/500 - time: 7.42 - training_loss: -17.2366 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:30,825 ] - Epoch 156: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:31,079 ] - Epoch 156/500 - time: 7.31 - training_loss: -17.2371 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:38,114 ] - Epoch 157: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:38,384 ] - Epoch 157/500 - time: 7.30 - training_loss: -17.2376 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:45,400 ] - Epoch 158: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:45,699 ] - Epoch 158/500 - time: 7.31 - training_loss: -17.2382 - val_loss: -17.3155
[ INFO : 2022-07-26 21:19:52,730 ] - Epoch 159: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:19:52,995 ] - Epoch 159/500 - time: 7.30 - training_loss: -17.2387 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:00,030 ] - Epoch 160: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:00,291 ] - Epoch 160/500 - time: 7.30 - training_loss: -17.2392 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:07,335 ] - Epoch 161: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:07,620 ] - Epoch 161/500 - time: 7.33 - training_loss: -17.2397 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:14,627 ] - Epoch 162: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:14,888 ] - Epoch 162/500 - time: 7.27 - training_loss: -17.2402 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:21,908 ] - Epoch 163: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:22,178 ] - Epoch 163/500 - time: 7.29 - training_loss: -17.2407 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:29,203 ] - Epoch 164: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:29,464 ] - Epoch 164/500 - time: 7.29 - training_loss: -17.2412 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:36,498 ] - Epoch 165: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:36,834 ] - Epoch 165/500 - time: 7.37 - training_loss: -17.2416 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:43,863 ] - Epoch 166: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:44,199 ] - Epoch 166/500 - time: 7.37 - training_loss: -17.2421 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:51,240 ] - Epoch 167: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:51,497 ] - Epoch 167/500 - time: 7.30 - training_loss: -17.2426 - val_loss: -17.3155
[ INFO : 2022-07-26 21:20:58,558 ] - Epoch 168: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:20:58,817 ] - Epoch 168/500 - time: 7.32 - training_loss: -17.2431 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:05,858 ] - Epoch 169: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:06,161 ] - Epoch 169/500 - time: 7.34 - training_loss: -17.2435 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:13,265 ] - Epoch 170: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:13,532 ] - Epoch 170/500 - time: 7.37 - training_loss: -17.2440 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:20,618 ] - Epoch 171: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:20,918 ] - Epoch 171/500 - time: 7.38 - training_loss: -17.2444 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:27,954 ] - Epoch 172: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:28,228 ] - Epoch 172/500 - time: 7.31 - training_loss: -17.2449 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:35,309 ] - Epoch 173: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:35,581 ] - Epoch 173/500 - time: 7.35 - training_loss: -17.2453 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:42,618 ] - Epoch 174: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:42,920 ] - Epoch 174/500 - time: 7.34 - training_loss: -17.2457 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:49,934 ] - Epoch 175: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:50,199 ] - Epoch 175/500 - time: 7.28 - training_loss: -17.2461 - val_loss: -17.3155
[ INFO : 2022-07-26 21:21:57,219 ] - Epoch 176: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:21:57,476 ] - Epoch 176/500 - time: 7.28 - training_loss: -17.2466 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:04,520 ] - Epoch 177: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:04,783 ] - Epoch 177/500 - time: 7.31 - training_loss: -17.2470 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:11,792 ] - Epoch 178: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:12,061 ] - Epoch 178/500 - time: 7.28 - training_loss: -17.2474 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:19,089 ] - Epoch 179: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:19,359 ] - Epoch 179/500 - time: 7.30 - training_loss: -17.2478 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:26,387 ] - Epoch 180: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:26,661 ] - Epoch 180/500 - time: 7.30 - training_loss: -17.2482 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:33,664 ] - Epoch 181: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:33,924 ] - Epoch 181/500 - time: 7.26 - training_loss: -17.2486 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:40,949 ] - Epoch 182: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:41,216 ] - Epoch 182/500 - time: 7.29 - training_loss: -17.2490 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:48,279 ] - Epoch 183: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:48,521 ] - Epoch 183/500 - time: 7.30 - training_loss: -17.2494 - val_loss: -17.3155
[ INFO : 2022-07-26 21:22:55,557 ] - Epoch 184: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:22:55,817 ] - Epoch 184/500 - time: 7.30 - training_loss: -17.2498 - val_loss: -17.3155
[ INFO : 2022-07-26 21:23:02,883 ] - Epoch 185: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:03,266 ] - Epoch 185/500 - time: 7.45 - training_loss: -17.2502 - val_loss: -17.3155
[ INFO : 2022-07-26 21:23:10,322 ] - Epoch 186: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:10,595 ] - Epoch 186/500 - time: 7.33 - training_loss: -17.2505 - val_loss: -17.3155
[ INFO : 2022-07-26 21:23:17,609 ] - Epoch 187: val_loss improved from -17.3155 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:17,876 ] - Epoch 187/500 - time: 7.28 - training_loss: -17.2509 - val_loss: -17.3156
[ INFO : 2022-07-26 21:23:24,908 ] - Epoch 188: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:25,189 ] - Epoch 188/500 - time: 7.31 - training_loss: -17.2513 - val_loss: -17.3156
[ INFO : 2022-07-26 21:23:32,213 ] - Epoch 189: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:32,496 ] - Epoch 189/500 - time: 7.31 - training_loss: -17.2516 - val_loss: -17.3156
[ INFO : 2022-07-26 21:23:39,587 ] - Epoch 190: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:39,851 ] - Epoch 190/500 - time: 7.35 - training_loss: -17.2520 - val_loss: -17.3156
[ INFO : 2022-07-26 21:23:46,931 ] - Epoch 191: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:47,189 ] - Epoch 191/500 - time: 7.34 - training_loss: -17.2524 - val_loss: -17.3156
[ INFO : 2022-07-26 21:23:54,221 ] - Epoch 192: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:23:54,500 ] - Epoch 192/500 - time: 7.31 - training_loss: -17.2527 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:01,525 ] - Epoch 193: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:01,833 ] - Epoch 193/500 - time: 7.33 - training_loss: -17.2531 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:08,851 ] - Epoch 194: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:09,200 ] - Epoch 194/500 - time: 7.37 - training_loss: -17.2534 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:16,247 ] - Epoch 195: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:16,509 ] - Epoch 195/500 - time: 7.31 - training_loss: -17.2538 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:23,585 ] - Epoch 196: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:23,870 ] - Epoch 196/500 - time: 7.36 - training_loss: -17.2541 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:30,925 ] - Epoch 197: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:31,189 ] - Epoch 197/500 - time: 7.32 - training_loss: -17.2544 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:38,215 ] - Epoch 198: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:38,499 ] - Epoch 198/500 - time: 7.31 - training_loss: -17.2548 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:45,514 ] - Epoch 199: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:45,765 ] - Epoch 199/500 - time: 7.27 - training_loss: -17.2551 - val_loss: -17.3156
[ INFO : 2022-07-26 21:24:52,800 ] - Epoch 200: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:24:53,064 ] - Epoch 200/500 - time: 7.30 - training_loss: -17.2554 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:00,102 ] - Epoch 201: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:00,351 ] - Epoch 201/500 - time: 7.29 - training_loss: -17.2558 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:07,406 ] - Epoch 202: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:07,654 ] - Epoch 202/500 - time: 7.30 - training_loss: -17.2561 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:14,667 ] - Epoch 203: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:14,967 ] - Epoch 203/500 - time: 7.31 - training_loss: -17.2564 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:22,007 ] - Epoch 204: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:22,331 ] - Epoch 204/500 - time: 7.36 - training_loss: -17.2567 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:29,351 ] - Epoch 205: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:29,642 ] - Epoch 205/500 - time: 7.31 - training_loss: -17.2570 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:36,660 ] - Epoch 206: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:36,916 ] - Epoch 206/500 - time: 7.27 - training_loss: -17.2573 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:43,938 ] - Epoch 207: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:44,204 ] - Epoch 207/500 - time: 7.29 - training_loss: -17.2576 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:51,220 ] - Epoch 208: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:51,500 ] - Epoch 208/500 - time: 7.30 - training_loss: -17.2579 - val_loss: -17.3156
[ INFO : 2022-07-26 21:25:58,501 ] - Epoch 209: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:25:58,782 ] - Epoch 209/500 - time: 7.28 - training_loss: -17.2582 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:05,811 ] - Epoch 210: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:06,079 ] - Epoch 210/500 - time: 7.30 - training_loss: -17.2585 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:13,133 ] - Epoch 211: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:13,416 ] - Epoch 211/500 - time: 7.34 - training_loss: -17.2588 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:20,429 ] - Epoch 212: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:20,696 ] - Epoch 212/500 - time: 7.28 - training_loss: -17.2591 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:27,727 ] - Epoch 213: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:27,997 ] - Epoch 213/500 - time: 7.30 - training_loss: -17.2594 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:35,063 ] - Epoch 214: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:35,359 ] - Epoch 214/500 - time: 7.36 - training_loss: -17.2597 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:42,393 ] - Epoch 215: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:42,670 ] - Epoch 215/500 - time: 7.31 - training_loss: -17.2600 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:49,693 ] - Epoch 216: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:49,972 ] - Epoch 216/500 - time: 7.30 - training_loss: -17.2602 - val_loss: -17.3156
[ INFO : 2022-07-26 21:26:57,031 ] - Epoch 217: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:26:57,310 ] - Epoch 217/500 - time: 7.34 - training_loss: -17.2605 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:04,362 ] - Epoch 218: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:04,648 ] - Epoch 218/500 - time: 7.34 - training_loss: -17.2608 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:11,751 ] - Epoch 219: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:12,007 ] - Epoch 219/500 - time: 7.36 - training_loss: -17.2611 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:19,052 ] - Epoch 220: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:19,334 ] - Epoch 220/500 - time: 7.33 - training_loss: -17.2613 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:26,371 ] - Epoch 221: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:26,637 ] - Epoch 221/500 - time: 7.30 - training_loss: -17.2616 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:33,663 ] - Epoch 222: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:33,925 ] - Epoch 222/500 - time: 7.29 - training_loss: -17.2619 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:40,938 ] - Epoch 223: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:41,237 ] - Epoch 223/500 - time: 7.31 - training_loss: -17.2621 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:48,260 ] - Epoch 224: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:48,516 ] - Epoch 224/500 - time: 7.28 - training_loss: -17.2624 - val_loss: -17.3156
[ INFO : 2022-07-26 21:27:55,566 ] - Epoch 225: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:27:55,831 ] - Epoch 225/500 - time: 7.32 - training_loss: -17.2627 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:02,865 ] - Epoch 226: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:03,170 ] - Epoch 226/500 - time: 7.34 - training_loss: -17.2629 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:10,221 ] - Epoch 227: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:10,491 ] - Epoch 227/500 - time: 7.32 - training_loss: -17.2632 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:17,507 ] - Epoch 228: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:17,761 ] - Epoch 228/500 - time: 7.27 - training_loss: -17.2634 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:24,790 ] - Epoch 229: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:25,128 ] - Epoch 229/500 - time: 7.37 - training_loss: -17.2637 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:32,157 ] - Epoch 230: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:32,448 ] - Epoch 230/500 - time: 7.32 - training_loss: -17.2639 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:39,477 ] - Epoch 231: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:39,734 ] - Epoch 231/500 - time: 7.29 - training_loss: -17.2642 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:46,747 ] - Epoch 232: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:47,006 ] - Epoch 232/500 - time: 7.27 - training_loss: -17.2644 - val_loss: -17.3156
[ INFO : 2022-07-26 21:28:54,024 ] - Epoch 233: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:28:54,310 ] - Epoch 233/500 - time: 7.30 - training_loss: -17.2646 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:01,322 ] - Epoch 234: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:01,617 ] - Epoch 234/500 - time: 7.31 - training_loss: -17.2649 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:08,639 ] - Epoch 235: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:08,897 ] - Epoch 235/500 - time: 7.28 - training_loss: -17.2651 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:15,924 ] - Epoch 236: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:16,204 ] - Epoch 236/500 - time: 7.31 - training_loss: -17.2654 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:23,219 ] - Epoch 237: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:23,480 ] - Epoch 237/500 - time: 7.28 - training_loss: -17.2656 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:30,498 ] - Epoch 238: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:30,780 ] - Epoch 238/500 - time: 7.30 - training_loss: -17.2658 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:37,783 ] - Epoch 239: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:38,058 ] - Epoch 239/500 - time: 7.28 - training_loss: -17.2660 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:45,081 ] - Epoch 240: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:45,395 ] - Epoch 240/500 - time: 7.34 - training_loss: -17.2663 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:52,417 ] - Epoch 241: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:29:52,709 ] - Epoch 241/500 - time: 7.31 - training_loss: -17.2665 - val_loss: -17.3156
[ INFO : 2022-07-26 21:29:59,719 ] - Epoch 242: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:00,007 ] - Epoch 242/500 - time: 7.30 - training_loss: -17.2667 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:07,044 ] - Epoch 243: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:07,328 ] - Epoch 243/500 - time: 7.32 - training_loss: -17.2669 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:14,397 ] - Epoch 244: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:14,669 ] - Epoch 244/500 - time: 7.34 - training_loss: -17.2672 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:21,686 ] - Epoch 245: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:21,971 ] - Epoch 245/500 - time: 7.30 - training_loss: -17.2674 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:28,995 ] - Epoch 246: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:29,258 ] - Epoch 246/500 - time: 7.29 - training_loss: -17.2676 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:36,286 ] - Epoch 247: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:36,576 ] - Epoch 247/500 - time: 7.32 - training_loss: -17.2678 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:43,576 ] - Epoch 248: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:43,894 ] - Epoch 248/500 - time: 7.32 - training_loss: -17.2680 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:50,922 ] - Epoch 249: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:51,192 ] - Epoch 249/500 - time: 7.30 - training_loss: -17.2682 - val_loss: -17.3156
[ INFO : 2022-07-26 21:30:58,198 ] - Epoch 250: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:30:58,479 ] - Epoch 250/500 - time: 7.29 - training_loss: -17.2684 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:05,557 ] - Epoch 251: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:05,823 ] - Epoch 251/500 - time: 7.34 - training_loss: -17.2686 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:12,843 ] - Epoch 252: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:13,143 ] - Epoch 252/500 - time: 7.32 - training_loss: -17.2689 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:20,142 ] - Epoch 253: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:20,423 ] - Epoch 253/500 - time: 7.28 - training_loss: -17.2691 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:27,442 ] - Epoch 254: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:27,734 ] - Epoch 254/500 - time: 7.31 - training_loss: -17.2693 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:34,752 ] - Epoch 255: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:35,010 ] - Epoch 255/500 - time: 7.27 - training_loss: -17.2695 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:42,024 ] - Epoch 256: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:42,304 ] - Epoch 256/500 - time: 7.29 - training_loss: -17.2697 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:49,354 ] - Epoch 257: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:49,634 ] - Epoch 257/500 - time: 7.33 - training_loss: -17.2699 - val_loss: -17.3156
[ INFO : 2022-07-26 21:31:56,670 ] - Epoch 258: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:31:56,937 ] - Epoch 258/500 - time: 7.30 - training_loss: -17.2701 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:03,972 ] - Epoch 259: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:04,240 ] - Epoch 259/500 - time: 7.30 - training_loss: -17.2702 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:11,435 ] - Epoch 260: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:11,687 ] - Epoch 260/500 - time: 7.45 - training_loss: -17.2704 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:18,956 ] - Epoch 261: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:19,238 ] - Epoch 261/500 - time: 7.55 - training_loss: -17.2706 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:26,514 ] - Epoch 262: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:26,932 ] - Epoch 262/500 - time: 7.69 - training_loss: -17.2708 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:34,235 ] - Epoch 263: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:34,500 ] - Epoch 263/500 - time: 7.57 - training_loss: -17.2710 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:41,800 ] - Epoch 264: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:42,091 ] - Epoch 264/500 - time: 7.59 - training_loss: -17.2712 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:49,362 ] - Epoch 265: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:49,625 ] - Epoch 265/500 - time: 7.53 - training_loss: -17.2714 - val_loss: -17.3156
[ INFO : 2022-07-26 21:32:56,929 ] - Epoch 266: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:32:57,187 ] - Epoch 266/500 - time: 7.56 - training_loss: -17.2716 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:04,579 ] - Epoch 267: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:04,862 ] - Epoch 267/500 - time: 7.68 - training_loss: -17.2718 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:12,117 ] - Epoch 268: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:12,394 ] - Epoch 268/500 - time: 7.53 - training_loss: -17.2719 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:19,666 ] - Epoch 269: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:19,929 ] - Epoch 269/500 - time: 7.54 - training_loss: -17.2721 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:27,193 ] - Epoch 270: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:27,474 ] - Epoch 270/500 - time: 7.54 - training_loss: -17.2723 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:34,863 ] - Epoch 271: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:35,144 ] - Epoch 271/500 - time: 7.67 - training_loss: -17.2725 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:42,406 ] - Epoch 272: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:42,673 ] - Epoch 272/500 - time: 7.53 - training_loss: -17.2726 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:49,903 ] - Epoch 273: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:50,201 ] - Epoch 273/500 - time: 7.53 - training_loss: -17.2728 - val_loss: -17.3156
[ INFO : 2022-07-26 21:33:57,500 ] - Epoch 274: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:33:57,769 ] - Epoch 274/500 - time: 7.57 - training_loss: -17.2730 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:05,038 ] - Epoch 275: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:05,307 ] - Epoch 275/500 - time: 7.54 - training_loss: -17.2732 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:12,616 ] - Epoch 276: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:12,901 ] - Epoch 276/500 - time: 7.59 - training_loss: -17.2733 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:20,214 ] - Epoch 277: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:20,474 ] - Epoch 277/500 - time: 7.57 - training_loss: -17.2735 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:27,869 ] - Epoch 278: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:28,133 ] - Epoch 278/500 - time: 7.66 - training_loss: -17.2737 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:35,430 ] - Epoch 279: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:35,719 ] - Epoch 279/500 - time: 7.59 - training_loss: -17.2738 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:43,070 ] - Epoch 280: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:43,357 ] - Epoch 280/500 - time: 7.64 - training_loss: -17.2740 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:50,648 ] - Epoch 281: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:50,926 ] - Epoch 281/500 - time: 7.57 - training_loss: -17.2742 - val_loss: -17.3156
[ INFO : 2022-07-26 21:34:58,217 ] - Epoch 282: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:34:58,490 ] - Epoch 282/500 - time: 7.56 - training_loss: -17.2743 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:05,731 ] - Epoch 283: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:05,981 ] - Epoch 283/500 - time: 7.49 - training_loss: -17.2745 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:13,258 ] - Epoch 284: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:13,537 ] - Epoch 284/500 - time: 7.56 - training_loss: -17.2747 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:20,811 ] - Epoch 285: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:21,082 ] - Epoch 285/500 - time: 7.54 - training_loss: -17.2748 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:28,345 ] - Epoch 286: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:28,633 ] - Epoch 286/500 - time: 7.55 - training_loss: -17.2750 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:35,880 ] - Epoch 287: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:36,160 ] - Epoch 287/500 - time: 7.53 - training_loss: -17.2751 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:43,388 ] - Epoch 288: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:43,689 ] - Epoch 288/500 - time: 7.53 - training_loss: -17.2753 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:50,713 ] - Epoch 289: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 21:35:51,002 ] - Epoch 289/500 - time: 7.31 - training_loss: -17.2755 - val_loss: -17.3156
[ INFO : 2022-07-26 21:35:58,044 ] - Epoch 290: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:35:58,045 ] - Epoch 290/500 - time: 7.04 - training_loss: -17.2756 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:05,057 ] - Epoch 291: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:05,057 ] - Epoch 291/500 - time: 7.01 - training_loss: -17.2758 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:12,063 ] - Epoch 292: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:12,063 ] - Epoch 292/500 - time: 7.01 - training_loss: -17.2759 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:19,102 ] - Epoch 293: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:19,102 ] - Epoch 293/500 - time: 7.04 - training_loss: -17.2761 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:26,158 ] - Epoch 294: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:26,158 ] - Epoch 294/500 - time: 7.06 - training_loss: -17.2762 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:33,214 ] - Epoch 295: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:33,214 ] - Epoch 295/500 - time: 7.06 - training_loss: -17.2764 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:40,296 ] - Epoch 296: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:40,296 ] - Epoch 296/500 - time: 7.08 - training_loss: -17.2765 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:47,332 ] - Epoch 297: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:47,332 ] - Epoch 297/500 - time: 7.04 - training_loss: -17.2767 - val_loss: -17.3156
[ INFO : 2022-07-26 21:36:54,353 ] - Epoch 298: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:36:54,353 ] - Epoch 298/500 - time: 7.02 - training_loss: -17.2768 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:01,418 ] - Epoch 299: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:01,419 ] - Epoch 299/500 - time: 7.07 - training_loss: -17.2770 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:08,460 ] - Epoch 300: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:08,460 ] - Epoch 300/500 - time: 7.04 - training_loss: -17.2771 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:15,495 ] - Epoch 301: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:15,495 ] - Epoch 301/500 - time: 7.03 - training_loss: -17.2773 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:22,551 ] - Epoch 302: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:22,551 ] - Epoch 302/500 - time: 7.06 - training_loss: -17.2774 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:29,582 ] - Epoch 303: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:29,582 ] - Epoch 303/500 - time: 7.03 - training_loss: -17.2775 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:36,620 ] - Epoch 304: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:36,620 ] - Epoch 304/500 - time: 7.04 - training_loss: -17.2777 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:43,656 ] - Epoch 305: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:43,656 ] - Epoch 305/500 - time: 7.04 - training_loss: -17.2778 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:50,690 ] - Epoch 306: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:50,691 ] - Epoch 306/500 - time: 7.04 - training_loss: -17.2780 - val_loss: -17.3156
[ INFO : 2022-07-26 21:37:57,725 ] - Epoch 307: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:37:57,725 ] - Epoch 307/500 - time: 7.03 - training_loss: -17.2781 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:04,765 ] - Epoch 308: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:04,765 ] - Epoch 308/500 - time: 7.04 - training_loss: -17.2782 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:11,795 ] - Epoch 309: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:11,795 ] - Epoch 309/500 - time: 7.03 - training_loss: -17.2784 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:18,800 ] - Epoch 310: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:18,800 ] - Epoch 310/500 - time: 7.00 - training_loss: -17.2785 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:25,836 ] - Epoch 311: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:25,836 ] - Epoch 311/500 - time: 7.04 - training_loss: -17.2786 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:32,874 ] - Epoch 312: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:32,874 ] - Epoch 312/500 - time: 7.04 - training_loss: -17.2788 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:39,901 ] - Epoch 313: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:39,901 ] - Epoch 313/500 - time: 7.03 - training_loss: -17.2789 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:46,955 ] - Epoch 314: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:46,955 ] - Epoch 314/500 - time: 7.05 - training_loss: -17.2790 - val_loss: -17.3156
[ INFO : 2022-07-26 21:38:54,022 ] - Epoch 315: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:38:54,022 ] - Epoch 315/500 - time: 7.07 - training_loss: -17.2792 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:01,077 ] - Epoch 316: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:01,077 ] - Epoch 316/500 - time: 7.05 - training_loss: -17.2793 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:08,105 ] - Epoch 317: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:08,106 ] - Epoch 317/500 - time: 7.03 - training_loss: -17.2794 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:15,128 ] - Epoch 318: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:15,128 ] - Epoch 318/500 - time: 7.02 - training_loss: -17.2796 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:22,134 ] - Epoch 319: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:22,134 ] - Epoch 319/500 - time: 7.01 - training_loss: -17.2797 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:29,216 ] - Epoch 320: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:29,217 ] - Epoch 320/500 - time: 7.08 - training_loss: -17.2798 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:36,265 ] - Epoch 321: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:36,265 ] - Epoch 321/500 - time: 7.05 - training_loss: -17.2799 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:43,304 ] - Epoch 322: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:43,305 ] - Epoch 322/500 - time: 7.04 - training_loss: -17.2801 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:50,350 ] - Epoch 323: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:50,350 ] - Epoch 323/500 - time: 7.04 - training_loss: -17.2802 - val_loss: -17.3156
[ INFO : 2022-07-26 21:39:57,395 ] - Epoch 324: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:39:57,395 ] - Epoch 324/500 - time: 7.05 - training_loss: -17.2803 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:04,423 ] - Epoch 325: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:04,423 ] - Epoch 325/500 - time: 7.03 - training_loss: -17.2804 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:11,438 ] - Epoch 326: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:11,438 ] - Epoch 326/500 - time: 7.02 - training_loss: -17.2806 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:18,478 ] - Epoch 327: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:18,478 ] - Epoch 327/500 - time: 7.04 - training_loss: -17.2807 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:25,495 ] - Epoch 328: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:25,495 ] - Epoch 328/500 - time: 7.02 - training_loss: -17.2808 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:32,546 ] - Epoch 329: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:32,546 ] - Epoch 329/500 - time: 7.05 - training_loss: -17.2809 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:39,601 ] - Epoch 330: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:39,601 ] - Epoch 330/500 - time: 7.05 - training_loss: -17.2811 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:46,631 ] - Epoch 331: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:46,631 ] - Epoch 331/500 - time: 7.03 - training_loss: -17.2812 - val_loss: -17.3156
[ INFO : 2022-07-26 21:40:53,664 ] - Epoch 332: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:40:53,665 ] - Epoch 332/500 - time: 7.03 - training_loss: -17.2813 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:00,722 ] - Epoch 333: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:00,722 ] - Epoch 333/500 - time: 7.06 - training_loss: -17.2814 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:07,753 ] - Epoch 334: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:07,753 ] - Epoch 334/500 - time: 7.03 - training_loss: -17.2815 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:14,793 ] - Epoch 335: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:14,793 ] - Epoch 335/500 - time: 7.04 - training_loss: -17.2816 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:21,860 ] - Epoch 336: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:21,860 ] - Epoch 336/500 - time: 7.07 - training_loss: -17.2818 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:28,914 ] - Epoch 337: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:28,914 ] - Epoch 337/500 - time: 7.05 - training_loss: -17.2819 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:35,936 ] - Epoch 338: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:35,936 ] - Epoch 338/500 - time: 7.02 - training_loss: -17.2820 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:42,975 ] - Epoch 339: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:42,976 ] - Epoch 339/500 - time: 7.04 - training_loss: -17.2821 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:50,068 ] - Epoch 340: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:50,068 ] - Epoch 340/500 - time: 7.09 - training_loss: -17.2822 - val_loss: -17.3156
[ INFO : 2022-07-26 21:41:57,127 ] - Epoch 341: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:41:57,127 ] - Epoch 341/500 - time: 7.06 - training_loss: -17.2823 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:04,188 ] - Epoch 342: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:04,189 ] - Epoch 342/500 - time: 7.06 - training_loss: -17.2824 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:11,236 ] - Epoch 343: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:11,236 ] - Epoch 343/500 - time: 7.05 - training_loss: -17.2825 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:18,292 ] - Epoch 344: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:18,292 ] - Epoch 344/500 - time: 7.06 - training_loss: -17.2827 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:25,379 ] - Epoch 345: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:25,379 ] - Epoch 345/500 - time: 7.09 - training_loss: -17.2828 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:32,399 ] - Epoch 346: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:32,399 ] - Epoch 346/500 - time: 7.02 - training_loss: -17.2829 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:39,424 ] - Epoch 347: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:39,424 ] - Epoch 347/500 - time: 7.03 - training_loss: -17.2830 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:46,477 ] - Epoch 348: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:46,477 ] - Epoch 348/500 - time: 7.05 - training_loss: -17.2831 - val_loss: -17.3156
[ INFO : 2022-07-26 21:42:53,519 ] - Epoch 349: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:42:53,520 ] - Epoch 349/500 - time: 7.04 - training_loss: -17.2832 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:00,595 ] - Epoch 350: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:00,596 ] - Epoch 350/500 - time: 7.08 - training_loss: -17.2833 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:07,651 ] - Epoch 351: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:07,651 ] - Epoch 351/500 - time: 7.06 - training_loss: -17.2834 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:14,698 ] - Epoch 352: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:14,698 ] - Epoch 352/500 - time: 7.05 - training_loss: -17.2835 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:21,749 ] - Epoch 353: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:21,749 ] - Epoch 353/500 - time: 7.05 - training_loss: -17.2836 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:29,066 ] - Epoch 354: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:29,066 ] - Epoch 354/500 - time: 7.32 - training_loss: -17.2837 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:40,507 ] - Epoch 355: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:40,507 ] - Epoch 355/500 - time: 11.44 - training_loss: -17.2838 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:47,543 ] - Epoch 356: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:47,543 ] - Epoch 356/500 - time: 7.04 - training_loss: -17.2839 - val_loss: -17.3156
[ INFO : 2022-07-26 21:43:54,599 ] - Epoch 357: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:43:54,599 ] - Epoch 357/500 - time: 7.06 - training_loss: -17.2840 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:01,638 ] - Epoch 358: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:01,638 ] - Epoch 358/500 - time: 7.04 - training_loss: -17.2841 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:08,668 ] - Epoch 359: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:08,668 ] - Epoch 359/500 - time: 7.03 - training_loss: -17.2842 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:15,695 ] - Epoch 360: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:15,695 ] - Epoch 360/500 - time: 7.03 - training_loss: -17.2843 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:22,739 ] - Epoch 361: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:22,739 ] - Epoch 361/500 - time: 7.04 - training_loss: -17.2844 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:29,768 ] - Epoch 362: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:29,769 ] - Epoch 362/500 - time: 7.03 - training_loss: -17.2845 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:36,785 ] - Epoch 363: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:36,785 ] - Epoch 363/500 - time: 7.02 - training_loss: -17.2846 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:43,824 ] - Epoch 364: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:43,824 ] - Epoch 364/500 - time: 7.04 - training_loss: -17.2847 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:50,862 ] - Epoch 365: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:50,862 ] - Epoch 365/500 - time: 7.04 - training_loss: -17.2848 - val_loss: -17.3156
[ INFO : 2022-07-26 21:44:57,882 ] - Epoch 366: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:44:57,882 ] - Epoch 366/500 - time: 7.02 - training_loss: -17.2849 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:04,933 ] - Epoch 367: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:04,933 ] - Epoch 367/500 - time: 7.05 - training_loss: -17.2850 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:11,977 ] - Epoch 368: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:11,977 ] - Epoch 368/500 - time: 7.04 - training_loss: -17.2851 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:18,997 ] - Epoch 369: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:18,997 ] - Epoch 369/500 - time: 7.02 - training_loss: -17.2852 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:26,028 ] - Epoch 370: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:26,028 ] - Epoch 370/500 - time: 7.03 - training_loss: -17.2853 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:33,064 ] - Epoch 371: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:33,064 ] - Epoch 371/500 - time: 7.04 - training_loss: -17.2854 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:40,080 ] - Epoch 372: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:40,080 ] - Epoch 372/500 - time: 7.02 - training_loss: -17.2855 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:47,111 ] - Epoch 373: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:47,111 ] - Epoch 373/500 - time: 7.03 - training_loss: -17.2856 - val_loss: -17.3156
[ INFO : 2022-07-26 21:45:54,141 ] - Epoch 374: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:45:54,141 ] - Epoch 374/500 - time: 7.03 - training_loss: -17.2857 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:01,166 ] - Epoch 375: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:01,166 ] - Epoch 375/500 - time: 7.03 - training_loss: -17.2858 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:08,205 ] - Epoch 376: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:08,205 ] - Epoch 376/500 - time: 7.04 - training_loss: -17.2859 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:15,242 ] - Epoch 377: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:15,242 ] - Epoch 377/500 - time: 7.04 - training_loss: -17.2860 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:22,252 ] - Epoch 378: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:22,252 ] - Epoch 378/500 - time: 7.01 - training_loss: -17.2861 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:29,287 ] - Epoch 379: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:29,287 ] - Epoch 379/500 - time: 7.04 - training_loss: -17.2862 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:36,338 ] - Epoch 380: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:36,339 ] - Epoch 380/500 - time: 7.05 - training_loss: -17.2862 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:43,376 ] - Epoch 381: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:43,376 ] - Epoch 381/500 - time: 7.04 - training_loss: -17.2863 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:50,412 ] - Epoch 382: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:50,412 ] - Epoch 382/500 - time: 7.04 - training_loss: -17.2864 - val_loss: -17.3156
[ INFO : 2022-07-26 21:46:57,450 ] - Epoch 383: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:46:57,451 ] - Epoch 383/500 - time: 7.04 - training_loss: -17.2865 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:04,485 ] - Epoch 384: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:04,485 ] - Epoch 384/500 - time: 7.03 - training_loss: -17.2866 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:11,717 ] - Epoch 385: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:11,717 ] - Epoch 385/500 - time: 7.23 - training_loss: -17.2867 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:19,023 ] - Epoch 386: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:19,023 ] - Epoch 386/500 - time: 7.31 - training_loss: -17.2868 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:26,317 ] - Epoch 387: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:26,317 ] - Epoch 387/500 - time: 7.29 - training_loss: -17.2869 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:33,573 ] - Epoch 388: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:33,573 ] - Epoch 388/500 - time: 7.26 - training_loss: -17.2869 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:40,839 ] - Epoch 389: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:40,839 ] - Epoch 389/500 - time: 7.27 - training_loss: -17.2870 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:48,088 ] - Epoch 390: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:48,089 ] - Epoch 390/500 - time: 7.25 - training_loss: -17.2871 - val_loss: -17.3156
[ INFO : 2022-07-26 21:47:55,349 ] - Epoch 391: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:47:55,350 ] - Epoch 391/500 - time: 7.26 - training_loss: -17.2872 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:02,671 ] - Epoch 392: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:02,671 ] - Epoch 392/500 - time: 7.32 - training_loss: -17.2873 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:09,952 ] - Epoch 393: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:09,952 ] - Epoch 393/500 - time: 7.28 - training_loss: -17.2874 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:17,236 ] - Epoch 394: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:17,237 ] - Epoch 394/500 - time: 7.29 - training_loss: -17.2875 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:24,528 ] - Epoch 395: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:24,528 ] - Epoch 395/500 - time: 7.29 - training_loss: -17.2875 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:31,801 ] - Epoch 396: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:31,801 ] - Epoch 396/500 - time: 7.27 - training_loss: -17.2876 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:39,218 ] - Epoch 397: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:39,218 ] - Epoch 397/500 - time: 7.42 - training_loss: -17.2877 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:46,513 ] - Epoch 398: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:46,513 ] - Epoch 398/500 - time: 7.29 - training_loss: -17.2878 - val_loss: -17.3156
[ INFO : 2022-07-26 21:48:53,786 ] - Epoch 399: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:48:53,786 ] - Epoch 399/500 - time: 7.27 - training_loss: -17.2879 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:01,066 ] - Epoch 400: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:01,066 ] - Epoch 400/500 - time: 7.28 - training_loss: -17.2880 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:08,347 ] - Epoch 401: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:08,347 ] - Epoch 401/500 - time: 7.28 - training_loss: -17.2880 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:15,605 ] - Epoch 402: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:15,605 ] - Epoch 402/500 - time: 7.26 - training_loss: -17.2881 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:22,900 ] - Epoch 403: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:22,900 ] - Epoch 403/500 - time: 7.30 - training_loss: -17.2882 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:30,172 ] - Epoch 404: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:30,172 ] - Epoch 404/500 - time: 7.27 - training_loss: -17.2883 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:37,508 ] - Epoch 405: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:37,508 ] - Epoch 405/500 - time: 7.34 - training_loss: -17.2884 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:44,774 ] - Epoch 406: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:44,775 ] - Epoch 406/500 - time: 7.27 - training_loss: -17.2884 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:52,083 ] - Epoch 407: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:52,084 ] - Epoch 407/500 - time: 7.31 - training_loss: -17.2885 - val_loss: -17.3156
[ INFO : 2022-07-26 21:49:59,354 ] - Epoch 408: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:49:59,355 ] - Epoch 408/500 - time: 7.27 - training_loss: -17.2886 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:06,574 ] - Epoch 409: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:06,574 ] - Epoch 409/500 - time: 7.22 - training_loss: -17.2887 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:13,829 ] - Epoch 410: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:13,830 ] - Epoch 410/500 - time: 7.26 - training_loss: -17.2887 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:21,091 ] - Epoch 411: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:21,091 ] - Epoch 411/500 - time: 7.26 - training_loss: -17.2888 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:28,450 ] - Epoch 412: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:28,450 ] - Epoch 412/500 - time: 7.36 - training_loss: -17.2889 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:35,733 ] - Epoch 413: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:35,734 ] - Epoch 413/500 - time: 7.28 - training_loss: -17.2890 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:43,038 ] - Epoch 414: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:43,038 ] - Epoch 414/500 - time: 7.30 - training_loss: -17.2891 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:50,353 ] - Epoch 415: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:50,353 ] - Epoch 415/500 - time: 7.32 - training_loss: -17.2891 - val_loss: -17.3156
[ INFO : 2022-07-26 21:50:57,617 ] - Epoch 416: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:50:57,617 ] - Epoch 416/500 - time: 7.26 - training_loss: -17.2892 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:04,908 ] - Epoch 417: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:04,909 ] - Epoch 417/500 - time: 7.29 - training_loss: -17.2893 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:12,200 ] - Epoch 418: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:12,202 ] - Epoch 418/500 - time: 7.29 - training_loss: -17.2894 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:19,510 ] - Epoch 419: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:19,510 ] - Epoch 419/500 - time: 7.31 - training_loss: -17.2894 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:26,780 ] - Epoch 420: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:26,780 ] - Epoch 420/500 - time: 7.27 - training_loss: -17.2895 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:34,054 ] - Epoch 421: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:34,054 ] - Epoch 421/500 - time: 7.27 - training_loss: -17.2896 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:41,340 ] - Epoch 422: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:41,340 ] - Epoch 422/500 - time: 7.29 - training_loss: -17.2896 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:48,580 ] - Epoch 423: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:48,580 ] - Epoch 423/500 - time: 7.24 - training_loss: -17.2897 - val_loss: -17.3156
[ INFO : 2022-07-26 21:51:55,883 ] - Epoch 424: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:51:55,883 ] - Epoch 424/500 - time: 7.30 - training_loss: -17.2898 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:03,142 ] - Epoch 425: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:03,142 ] - Epoch 425/500 - time: 7.26 - training_loss: -17.2899 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:10,422 ] - Epoch 426: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:10,422 ] - Epoch 426/500 - time: 7.28 - training_loss: -17.2899 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:17,670 ] - Epoch 427: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:17,670 ] - Epoch 427/500 - time: 7.25 - training_loss: -17.2900 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:24,966 ] - Epoch 428: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:24,966 ] - Epoch 428/500 - time: 7.30 - training_loss: -17.2901 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:32,227 ] - Epoch 429: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:32,227 ] - Epoch 429/500 - time: 7.26 - training_loss: -17.2902 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:39,477 ] - Epoch 430: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:39,477 ] - Epoch 430/500 - time: 7.25 - training_loss: -17.2902 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:46,620 ] - Epoch 431: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:46,620 ] - Epoch 431/500 - time: 7.14 - training_loss: -17.2903 - val_loss: -17.3156
[ INFO : 2022-07-26 21:52:53,737 ] - Epoch 432: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:52:53,737 ] - Epoch 432/500 - time: 7.12 - training_loss: -17.2904 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:00,823 ] - Epoch 433: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:00,823 ] - Epoch 433/500 - time: 7.09 - training_loss: -17.2904 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:07,879 ] - Epoch 434: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:07,880 ] - Epoch 434/500 - time: 7.06 - training_loss: -17.2905 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:14,937 ] - Epoch 435: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:14,937 ] - Epoch 435/500 - time: 7.06 - training_loss: -17.2906 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:22,026 ] - Epoch 436: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:22,026 ] - Epoch 436/500 - time: 7.09 - training_loss: -17.2906 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:29,064 ] - Epoch 437: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:29,064 ] - Epoch 437/500 - time: 7.04 - training_loss: -17.2907 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:36,110 ] - Epoch 438: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:36,110 ] - Epoch 438/500 - time: 7.05 - training_loss: -17.2908 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:43,154 ] - Epoch 439: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:43,155 ] - Epoch 439/500 - time: 7.04 - training_loss: -17.2908 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:50,179 ] - Epoch 440: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:50,179 ] - Epoch 440/500 - time: 7.02 - training_loss: -17.2909 - val_loss: -17.3156
[ INFO : 2022-07-26 21:53:57,253 ] - Epoch 441: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:53:57,253 ] - Epoch 441/500 - time: 7.07 - training_loss: -17.2910 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:04,316 ] - Epoch 442: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:04,316 ] - Epoch 442/500 - time: 7.06 - training_loss: -17.2910 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:11,373 ] - Epoch 443: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:11,373 ] - Epoch 443/500 - time: 7.06 - training_loss: -17.2911 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:18,418 ] - Epoch 444: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:18,418 ] - Epoch 444/500 - time: 7.05 - training_loss: -17.2912 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:25,465 ] - Epoch 445: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:25,465 ] - Epoch 445/500 - time: 7.05 - training_loss: -17.2912 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:32,535 ] - Epoch 446: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:32,535 ] - Epoch 446/500 - time: 7.07 - training_loss: -17.2913 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:39,576 ] - Epoch 447: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:39,576 ] - Epoch 447/500 - time: 7.04 - training_loss: -17.2914 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:46,591 ] - Epoch 448: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:46,591 ] - Epoch 448/500 - time: 7.01 - training_loss: -17.2914 - val_loss: -17.3156
[ INFO : 2022-07-26 21:54:53,607 ] - Epoch 449: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:54:53,607 ] - Epoch 449/500 - time: 7.02 - training_loss: -17.2915 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:00,664 ] - Epoch 450: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:00,664 ] - Epoch 450/500 - time: 7.06 - training_loss: -17.2916 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:07,706 ] - Epoch 451: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:07,706 ] - Epoch 451/500 - time: 7.04 - training_loss: -17.2916 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:14,789 ] - Epoch 452: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:14,789 ] - Epoch 452/500 - time: 7.08 - training_loss: -17.2917 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:21,825 ] - Epoch 453: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:21,825 ] - Epoch 453/500 - time: 7.04 - training_loss: -17.2918 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:28,851 ] - Epoch 454: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:28,852 ] - Epoch 454/500 - time: 7.03 - training_loss: -17.2918 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:35,852 ] - Epoch 455: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:35,852 ] - Epoch 455/500 - time: 7.00 - training_loss: -17.2919 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:42,876 ] - Epoch 456: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:42,876 ] - Epoch 456/500 - time: 7.02 - training_loss: -17.2919 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:49,910 ] - Epoch 457: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:49,910 ] - Epoch 457/500 - time: 7.03 - training_loss: -17.2920 - val_loss: -17.3156
[ INFO : 2022-07-26 21:55:56,952 ] - Epoch 458: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:55:56,952 ] - Epoch 458/500 - time: 7.04 - training_loss: -17.2921 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:03,995 ] - Epoch 459: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:03,995 ] - Epoch 459/500 - time: 7.04 - training_loss: -17.2921 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:11,015 ] - Epoch 460: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:11,016 ] - Epoch 460/500 - time: 7.02 - training_loss: -17.2922 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:18,031 ] - Epoch 461: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:18,031 ] - Epoch 461/500 - time: 7.01 - training_loss: -17.2923 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:25,073 ] - Epoch 462: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:25,073 ] - Epoch 462/500 - time: 7.04 - training_loss: -17.2923 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:32,082 ] - Epoch 463: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:32,083 ] - Epoch 463/500 - time: 7.01 - training_loss: -17.2924 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:39,151 ] - Epoch 464: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:39,151 ] - Epoch 464/500 - time: 7.07 - training_loss: -17.2924 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:46,185 ] - Epoch 465: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:46,186 ] - Epoch 465/500 - time: 7.04 - training_loss: -17.2925 - val_loss: -17.3156
[ INFO : 2022-07-26 21:56:53,223 ] - Epoch 466: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:56:53,223 ] - Epoch 466/500 - time: 7.04 - training_loss: -17.2926 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:00,319 ] - Epoch 467: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:00,319 ] - Epoch 467/500 - time: 7.10 - training_loss: -17.2926 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:07,372 ] - Epoch 468: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:07,372 ] - Epoch 468/500 - time: 7.05 - training_loss: -17.2927 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:14,428 ] - Epoch 469: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:14,428 ] - Epoch 469/500 - time: 7.06 - training_loss: -17.2927 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:21,463 ] - Epoch 470: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:21,463 ] - Epoch 470/500 - time: 7.03 - training_loss: -17.2928 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:28,513 ] - Epoch 471: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:28,513 ] - Epoch 471/500 - time: 7.05 - training_loss: -17.2929 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:35,559 ] - Epoch 472: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:35,560 ] - Epoch 472/500 - time: 7.05 - training_loss: -17.2929 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:42,602 ] - Epoch 473: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:42,603 ] - Epoch 473/500 - time: 7.04 - training_loss: -17.2930 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:49,638 ] - Epoch 474: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:49,638 ] - Epoch 474/500 - time: 7.04 - training_loss: -17.2930 - val_loss: -17.3156
[ INFO : 2022-07-26 21:57:56,697 ] - Epoch 475: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:57:56,697 ] - Epoch 475/500 - time: 7.06 - training_loss: -17.2931 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:03,747 ] - Epoch 476: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:03,747 ] - Epoch 476/500 - time: 7.05 - training_loss: -17.2931 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:10,795 ] - Epoch 477: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:10,795 ] - Epoch 477/500 - time: 7.05 - training_loss: -17.2932 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:17,836 ] - Epoch 478: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:17,836 ] - Epoch 478/500 - time: 7.04 - training_loss: -17.2933 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:24,900 ] - Epoch 479: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:24,900 ] - Epoch 479/500 - time: 7.06 - training_loss: -17.2933 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:31,955 ] - Epoch 480: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:31,955 ] - Epoch 480/500 - time: 7.06 - training_loss: -17.2934 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:39,158 ] - Epoch 481: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:39,158 ] - Epoch 481/500 - time: 7.20 - training_loss: -17.2934 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:46,336 ] - Epoch 482: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:46,336 ] - Epoch 482/500 - time: 7.18 - training_loss: -17.2935 - val_loss: -17.3156
[ INFO : 2022-07-26 21:58:53,598 ] - Epoch 483: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:58:53,598 ] - Epoch 483/500 - time: 7.26 - training_loss: -17.2935 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:00,900 ] - Epoch 484: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:00,900 ] - Epoch 484/500 - time: 7.30 - training_loss: -17.2936 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:08,195 ] - Epoch 485: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:08,195 ] - Epoch 485/500 - time: 7.29 - training_loss: -17.2937 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:15,501 ] - Epoch 486: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:15,501 ] - Epoch 486/500 - time: 7.31 - training_loss: -17.2937 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:22,740 ] - Epoch 487: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:22,740 ] - Epoch 487/500 - time: 7.24 - training_loss: -17.2938 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:30,006 ] - Epoch 488: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:30,007 ] - Epoch 488/500 - time: 7.27 - training_loss: -17.2938 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:37,283 ] - Epoch 489: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:37,284 ] - Epoch 489/500 - time: 7.28 - training_loss: -17.2939 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:44,525 ] - Epoch 490: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:44,525 ] - Epoch 490/500 - time: 7.24 - training_loss: -17.2939 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:51,790 ] - Epoch 491: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:51,791 ] - Epoch 491/500 - time: 7.27 - training_loss: -17.2940 - val_loss: -17.3156
[ INFO : 2022-07-26 21:59:59,053 ] - Epoch 492: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 21:59:59,053 ] - Epoch 492/500 - time: 7.26 - training_loss: -17.2940 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:06,360 ] - Epoch 493: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:06,360 ] - Epoch 493/500 - time: 7.31 - training_loss: -17.2941 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:13,620 ] - Epoch 494: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:13,621 ] - Epoch 494/500 - time: 7.26 - training_loss: -17.2941 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:20,902 ] - Epoch 495: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:20,902 ] - Epoch 495/500 - time: 7.28 - training_loss: -17.2942 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:28,184 ] - Epoch 496: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:28,184 ] - Epoch 496/500 - time: 7.28 - training_loss: -17.2943 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:35,843 ] - Epoch 497: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:35,844 ] - Epoch 497/500 - time: 7.66 - training_loss: -17.2943 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:43,256 ] - Epoch 498: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:43,257 ] - Epoch 498/500 - time: 7.41 - training_loss: -17.2944 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:50,513 ] - Epoch 499: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:50,513 ] - Epoch 499/500 - time: 7.26 - training_loss: -17.2944 - val_loss: -17.3156
[ INFO : 2022-07-26 22:00:57,791 ] - Epoch 500: val_loss did not improve from -17.3156
[ INFO : 2022-07-26 22:00:57,791 ] - Epoch 500/500 - time: 7.28 - training_loss: -17.2945 - val_loss: -17.3156
[ INFO : 2022-07-26 22:01:01,472 ] - loss on validation data: -17.3156
[ INFO : 2022-07-26 22:06:48,496 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-26 22:06:48,496 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-26 22:06:57,091 ] - Epoch 1: val_loss improved from 0.0000 to -15.0132, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:06:57,366 ] - Epoch 1/200 - time: 8.87 - training_loss: -14.6868 - val_loss: -15.0132
[ INFO : 2022-07-26 22:07:04,494 ] - Epoch 2: val_loss improved from -15.0132 to -15.4357, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:04,801 ] - Epoch 2/200 - time: 7.44 - training_loss: -15.1154 - val_loss: -15.4357
[ INFO : 2022-07-26 22:07:11,940 ] - Epoch 3: val_loss improved from -15.4357 to -15.7386, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:12,226 ] - Epoch 3/200 - time: 7.42 - training_loss: -15.3932 - val_loss: -15.7386
[ INFO : 2022-07-26 22:07:19,371 ] - Epoch 4: val_loss improved from -15.7386 to -15.9481, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:19,669 ] - Epoch 4/200 - time: 7.44 - training_loss: -15.6032 - val_loss: -15.9481
[ INFO : 2022-07-26 22:07:26,774 ] - Epoch 5: val_loss improved from -15.9481 to -16.3548, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:27,036 ] - Epoch 5/200 - time: 7.37 - training_loss: -15.7735 - val_loss: -16.3548
[ INFO : 2022-07-26 22:07:34,126 ] - Epoch 6: val_loss improved from -16.3548 to -16.6227, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:34,373 ] - Epoch 6/200 - time: 7.34 - training_loss: -15.9181 - val_loss: -16.6227
[ INFO : 2022-07-26 22:07:41,501 ] - Epoch 7: val_loss improved from -16.6227 to -16.7325, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:41,772 ] - Epoch 7/200 - time: 7.40 - training_loss: -16.0371 - val_loss: -16.7325
[ INFO : 2022-07-26 22:07:48,859 ] - Epoch 8: val_loss improved from -16.7325 to -16.8615, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:49,122 ] - Epoch 8/200 - time: 7.35 - training_loss: -16.1386 - val_loss: -16.8615
[ INFO : 2022-07-26 22:07:56,249 ] - Epoch 9: val_loss improved from -16.8615 to -16.9132, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:07:56,527 ] - Epoch 9/200 - time: 7.40 - training_loss: -16.2257 - val_loss: -16.9132
[ INFO : 2022-07-26 22:08:03,624 ] - Epoch 10: val_loss improved from -16.9132 to -16.9768, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:03,892 ] - Epoch 10/200 - time: 7.36 - training_loss: -16.3009 - val_loss: -16.9768
[ INFO : 2022-07-26 22:08:10,969 ] - Epoch 11: val_loss improved from -16.9768 to -17.0206, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:11,270 ] - Epoch 11/200 - time: 7.38 - training_loss: -16.3662 - val_loss: -17.0206
[ INFO : 2022-07-26 22:08:18,348 ] - Epoch 12: val_loss improved from -17.0206 to -17.0752, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:18,610 ] - Epoch 12/200 - time: 7.34 - training_loss: -16.4243 - val_loss: -17.0752
[ INFO : 2022-07-26 22:08:25,699 ] - Epoch 13: val_loss improved from -17.0752 to -17.0867, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:25,987 ] - Epoch 13/200 - time: 7.38 - training_loss: -16.4763 - val_loss: -17.0867
[ INFO : 2022-07-26 22:08:33,108 ] - Epoch 14: val_loss improved from -17.0867 to -17.1137, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:33,365 ] - Epoch 14/200 - time: 7.38 - training_loss: -16.5223 - val_loss: -17.1137
[ INFO : 2022-07-26 22:08:40,497 ] - Epoch 15: val_loss did not improve from -17.1137
[ INFO : 2022-07-26 22:08:40,497 ] - Epoch 15/200 - time: 7.13 - training_loss: -16.5639 - val_loss: -17.1072
[ INFO : 2022-07-26 22:08:47,644 ] - Epoch 16: val_loss improved from -17.1137 to -17.1308, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:47,905 ] - Epoch 16/200 - time: 7.41 - training_loss: -16.6015 - val_loss: -17.1308
[ INFO : 2022-07-26 22:08:55,171 ] - Epoch 17: val_loss improved from -17.1308 to -17.1449, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:08:55,469 ] - Epoch 17/200 - time: 7.56 - training_loss: -16.6341 - val_loss: -17.1449
[ INFO : 2022-07-26 22:09:02,778 ] - Epoch 18: val_loss improved from -17.1449 to -17.1730, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:09:03,048 ] - Epoch 18/200 - time: 7.58 - training_loss: -16.6643 - val_loss: -17.1730
[ INFO : 2022-07-26 22:09:10,364 ] - Epoch 19: val_loss improved from -17.1730 to -17.1816, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:09:10,633 ] - Epoch 19/200 - time: 7.58 - training_loss: -16.6923 - val_loss: -17.1816
[ INFO : 2022-07-26 22:09:18,152 ] - Epoch 20: val_loss improved from -17.1816 to -17.2061, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:09:18,435 ] - Epoch 20/200 - time: 7.80 - training_loss: -16.7180 - val_loss: -17.2061
[ INFO : 2022-07-26 22:09:25,782 ] - Epoch 21: val_loss did not improve from -17.2061
[ INFO : 2022-07-26 22:09:25,782 ] - Epoch 21/200 - time: 7.35 - training_loss: -16.7419 - val_loss: -17.1883
[ INFO : 2022-07-26 22:09:32,920 ] - Epoch 22: val_loss did not improve from -17.2061
[ INFO : 2022-07-26 22:09:32,920 ] - Epoch 22/200 - time: 7.14 - training_loss: -16.7637 - val_loss: -17.2040
[ INFO : 2022-07-26 22:09:40,062 ] - Epoch 23: val_loss improved from -17.2061 to -17.2286, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:09:40,329 ] - Epoch 23/200 - time: 7.41 - training_loss: -16.7839 - val_loss: -17.2286
[ INFO : 2022-07-26 22:09:47,488 ] - Epoch 24: val_loss did not improve from -17.2286
[ INFO : 2022-07-26 22:09:47,488 ] - Epoch 24/200 - time: 7.16 - training_loss: -16.8029 - val_loss: -17.2219
[ INFO : 2022-07-26 22:09:54,636 ] - Epoch 25: val_loss improved from -17.2286 to -17.2299, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:09:54,907 ] - Epoch 25/200 - time: 7.42 - training_loss: -16.8206 - val_loss: -17.2299
[ INFO : 2022-07-26 22:10:02,046 ] - Epoch 26: val_loss improved from -17.2299 to -17.2313, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:10:02,321 ] - Epoch 26/200 - time: 7.41 - training_loss: -16.8370 - val_loss: -17.2313
[ INFO : 2022-07-26 22:10:09,421 ] - Epoch 27: val_loss improved from -17.2313 to -17.2473, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:10:09,719 ] - Epoch 27/200 - time: 7.40 - training_loss: -16.8522 - val_loss: -17.2473
[ INFO : 2022-07-26 22:10:16,837 ] - Epoch 28: val_loss improved from -17.2473 to -17.2477, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:10:17,107 ] - Epoch 28/200 - time: 7.39 - training_loss: -16.8666 - val_loss: -17.2477
[ INFO : 2022-07-26 22:10:24,186 ] - Epoch 29: val_loss improved from -17.2477 to -17.2569, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:10:24,457 ] - Epoch 29/200 - time: 7.35 - training_loss: -16.8803 - val_loss: -17.2569
[ INFO : 2022-07-26 22:10:31,615 ] - Epoch 30: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:10:31,615 ] - Epoch 30/200 - time: 7.16 - training_loss: -16.8933 - val_loss: -17.2282
[ INFO : 2022-07-26 22:10:38,752 ] - Epoch 31: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:10:38,752 ] - Epoch 31/200 - time: 7.14 - training_loss: -16.9053 - val_loss: -17.2374
[ INFO : 2022-07-26 22:10:45,922 ] - Epoch 32: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:10:45,923 ] - Epoch 32/200 - time: 7.17 - training_loss: -16.9166 - val_loss: -17.2498
[ INFO : 2022-07-26 22:10:53,063 ] - Epoch 33: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:10:53,063 ] - Epoch 33/200 - time: 7.14 - training_loss: -16.9270 - val_loss: -17.2489
[ INFO : 2022-07-26 22:11:00,180 ] - Epoch 34: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:11:00,181 ] - Epoch 34/200 - time: 7.12 - training_loss: -16.9368 - val_loss: -17.2294
[ INFO : 2022-07-26 22:11:07,337 ] - Epoch 35: val_loss did not improve from -17.2569
[ INFO : 2022-07-26 22:11:07,337 ] - Epoch 35/200 - time: 7.16 - training_loss: -16.9462 - val_loss: -17.2545
[ INFO : 2022-07-26 22:11:14,632 ] - Epoch 36: val_loss improved from -17.2569 to -17.2620, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:11:14,897 ] - Epoch 36/200 - time: 7.56 - training_loss: -16.9554 - val_loss: -17.2620
[ INFO : 2022-07-26 22:11:22,140 ] - Epoch 37: val_loss improved from -17.2620 to -17.2644, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:11:22,411 ] - Epoch 37/200 - time: 7.51 - training_loss: -16.9642 - val_loss: -17.2644
[ INFO : 2022-07-26 22:11:29,557 ] - Epoch 38: val_loss improved from -17.2644 to -17.2683, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:11:29,848 ] - Epoch 38/200 - time: 7.44 - training_loss: -16.9727 - val_loss: -17.2683
[ INFO : 2022-07-26 22:11:36,987 ] - Epoch 39: val_loss improved from -17.2683 to -17.2821, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:11:37,277 ] - Epoch 39/200 - time: 7.43 - training_loss: -16.9808 - val_loss: -17.2821
[ INFO : 2022-07-26 22:11:44,413 ] - Epoch 40: val_loss did not improve from -17.2821
[ INFO : 2022-07-26 22:11:44,414 ] - Epoch 40/200 - time: 7.14 - training_loss: -16.9887 - val_loss: -17.2785
[ INFO : 2022-07-26 22:11:51,575 ] - Epoch 41: val_loss improved from -17.2821 to -17.2935, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:11:51,850 ] - Epoch 41/200 - time: 7.44 - training_loss: -16.9962 - val_loss: -17.2935
[ INFO : 2022-07-26 22:11:59,039 ] - Epoch 42: val_loss did not improve from -17.2935
[ INFO : 2022-07-26 22:11:59,039 ] - Epoch 42/200 - time: 7.19 - training_loss: -17.0035 - val_loss: -17.2893
[ INFO : 2022-07-26 22:12:06,162 ] - Epoch 43: val_loss improved from -17.2935 to -17.3004, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:06,424 ] - Epoch 43/200 - time: 7.38 - training_loss: -17.0104 - val_loss: -17.3004
[ INFO : 2022-07-26 22:12:13,520 ] - Epoch 44: val_loss improved from -17.3004 to -17.3013, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:13,788 ] - Epoch 44/200 - time: 7.36 - training_loss: -17.0172 - val_loss: -17.3013
[ INFO : 2022-07-26 22:12:20,911 ] - Epoch 45: val_loss improved from -17.3013 to -17.3049, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:21,186 ] - Epoch 45/200 - time: 7.40 - training_loss: -17.0237 - val_loss: -17.3049
[ INFO : 2022-07-26 22:12:28,309 ] - Epoch 46: val_loss improved from -17.3049 to -17.3051, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:28,584 ] - Epoch 46/200 - time: 7.40 - training_loss: -17.0300 - val_loss: -17.3051
[ INFO : 2022-07-26 22:12:35,683 ] - Epoch 47: val_loss improved from -17.3051 to -17.3064, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:35,960 ] - Epoch 47/200 - time: 7.38 - training_loss: -17.0360 - val_loss: -17.3064
[ INFO : 2022-07-26 22:12:43,060 ] - Epoch 48: val_loss improved from -17.3064 to -17.3078, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:43,391 ] - Epoch 48/200 - time: 7.43 - training_loss: -17.0418 - val_loss: -17.3078
[ INFO : 2022-07-26 22:12:50,523 ] - Epoch 49: val_loss improved from -17.3078 to -17.3080, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:50,799 ] - Epoch 49/200 - time: 7.41 - training_loss: -17.0473 - val_loss: -17.3080
[ INFO : 2022-07-26 22:12:57,886 ] - Epoch 50: val_loss improved from -17.3080 to -17.3095, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:12:58,180 ] - Epoch 50/200 - time: 7.38 - training_loss: -17.0527 - val_loss: -17.3095
[ INFO : 2022-07-26 22:13:05,280 ] - Epoch 51: val_loss did not improve from -17.3095
[ INFO : 2022-07-26 22:13:05,280 ] - Epoch 51/200 - time: 7.10 - training_loss: -17.0578 - val_loss: -17.3083
[ INFO : 2022-07-26 22:13:12,382 ] - Epoch 52: val_loss improved from -17.3095 to -17.3100, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:13:12,651 ] - Epoch 52/200 - time: 7.37 - training_loss: -17.0628 - val_loss: -17.3100
[ INFO : 2022-07-26 22:13:19,717 ] - Epoch 53: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:19,717 ] - Epoch 53/200 - time: 7.07 - training_loss: -17.0675 - val_loss: -17.3077
[ INFO : 2022-07-26 22:13:26,775 ] - Epoch 54: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:26,775 ] - Epoch 54/200 - time: 7.06 - training_loss: -17.0721 - val_loss: -17.3089
[ INFO : 2022-07-26 22:13:33,873 ] - Epoch 55: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:33,873 ] - Epoch 55/200 - time: 7.10 - training_loss: -17.0765 - val_loss: -17.3088
[ INFO : 2022-07-26 22:13:40,977 ] - Epoch 56: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:40,977 ] - Epoch 56/200 - time: 7.10 - training_loss: -17.0808 - val_loss: -17.3054
[ INFO : 2022-07-26 22:13:48,094 ] - Epoch 57: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:48,094 ] - Epoch 57/200 - time: 7.12 - training_loss: -17.0849 - val_loss: -17.3088
[ INFO : 2022-07-26 22:13:55,167 ] - Epoch 58: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:13:55,167 ] - Epoch 58/200 - time: 7.07 - training_loss: -17.0888 - val_loss: -17.3022
[ INFO : 2022-07-26 22:14:02,242 ] - Epoch 59: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:02,242 ] - Epoch 59/200 - time: 7.07 - training_loss: -17.0926 - val_loss: -17.3077
[ INFO : 2022-07-26 22:14:09,387 ] - Epoch 60: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:09,387 ] - Epoch 60/200 - time: 7.15 - training_loss: -17.0963 - val_loss: -17.2984
[ INFO : 2022-07-26 22:14:16,489 ] - Epoch 61: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:16,490 ] - Epoch 61/200 - time: 7.10 - training_loss: -17.0998 - val_loss: -17.3068
[ INFO : 2022-07-26 22:14:23,631 ] - Epoch 62: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:23,631 ] - Epoch 62/200 - time: 7.14 - training_loss: -17.1032 - val_loss: -17.2954
[ INFO : 2022-07-26 22:14:30,739 ] - Epoch 63: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:30,739 ] - Epoch 63/200 - time: 7.11 - training_loss: -17.1065 - val_loss: -17.3046
[ INFO : 2022-07-26 22:14:37,851 ] - Epoch 64: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:37,851 ] - Epoch 64/200 - time: 7.11 - training_loss: -17.1096 - val_loss: -17.2949
[ INFO : 2022-07-26 22:14:44,964 ] - Epoch 65: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:44,965 ] - Epoch 65/200 - time: 7.11 - training_loss: -17.1127 - val_loss: -17.2983
[ INFO : 2022-07-26 22:14:52,075 ] - Epoch 66: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:52,075 ] - Epoch 66/200 - time: 7.11 - training_loss: -17.1157 - val_loss: -17.2964
[ INFO : 2022-07-26 22:14:59,192 ] - Epoch 67: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:14:59,192 ] - Epoch 67/200 - time: 7.12 - training_loss: -17.1185 - val_loss: -17.2976
[ INFO : 2022-07-26 22:15:06,303 ] - Epoch 68: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:06,303 ] - Epoch 68/200 - time: 7.11 - training_loss: -17.1213 - val_loss: -17.3013
[ INFO : 2022-07-26 22:15:13,409 ] - Epoch 69: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:13,409 ] - Epoch 69/200 - time: 7.11 - training_loss: -17.1240 - val_loss: -17.2931
[ INFO : 2022-07-26 22:15:20,534 ] - Epoch 70: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:20,535 ] - Epoch 70/200 - time: 7.13 - training_loss: -17.1266 - val_loss: -17.3035
[ INFO : 2022-07-26 22:15:27,882 ] - Epoch 71: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:27,883 ] - Epoch 71/200 - time: 7.35 - training_loss: -17.1292 - val_loss: -17.3020
[ INFO : 2022-07-26 22:15:35,210 ] - Epoch 72: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:35,210 ] - Epoch 72/200 - time: 7.33 - training_loss: -17.1317 - val_loss: -17.3010
[ INFO : 2022-07-26 22:15:42,514 ] - Epoch 73: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:42,514 ] - Epoch 73/200 - time: 7.30 - training_loss: -17.1342 - val_loss: -17.3049
[ INFO : 2022-07-26 22:15:49,816 ] - Epoch 74: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:49,816 ] - Epoch 74/200 - time: 7.30 - training_loss: -17.1366 - val_loss: -17.2997
[ INFO : 2022-07-26 22:15:57,127 ] - Epoch 75: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:15:57,128 ] - Epoch 75/200 - time: 7.31 - training_loss: -17.1389 - val_loss: -17.3051
[ INFO : 2022-07-26 22:16:04,431 ] - Epoch 76: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:04,431 ] - Epoch 76/200 - time: 7.30 - training_loss: -17.1412 - val_loss: -17.2991
[ INFO : 2022-07-26 22:16:11,858 ] - Epoch 77: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:11,858 ] - Epoch 77/200 - time: 7.43 - training_loss: -17.1434 - val_loss: -17.3032
[ INFO : 2022-07-26 22:16:19,127 ] - Epoch 78: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:19,127 ] - Epoch 78/200 - time: 7.27 - training_loss: -17.1456 - val_loss: -17.2980
[ INFO : 2022-07-26 22:16:26,418 ] - Epoch 79: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:26,418 ] - Epoch 79/200 - time: 7.29 - training_loss: -17.1477 - val_loss: -17.3015
[ INFO : 2022-07-26 22:16:33,739 ] - Epoch 80: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:33,739 ] - Epoch 80/200 - time: 7.32 - training_loss: -17.1497 - val_loss: -17.2922
[ INFO : 2022-07-26 22:16:41,018 ] - Epoch 81: val_loss did not improve from -17.3100
[ INFO : 2022-07-26 22:16:41,018 ] - Epoch 81/200 - time: 7.28 - training_loss: -17.1517 - val_loss: -17.3042
[ INFO : 2022-07-26 22:16:48,273 ] - Epoch 82: val_loss improved from -17.3100 to -17.3101, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:16:48,534 ] - Epoch 82/200 - time: 7.52 - training_loss: -17.1536 - val_loss: -17.3101
[ INFO : 2022-07-26 22:16:55,821 ] - Epoch 83: val_loss improved from -17.3101 to -17.3114, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:16:56,108 ] - Epoch 83/200 - time: 7.57 - training_loss: -17.1556 - val_loss: -17.3114
[ INFO : 2022-07-26 22:17:03,384 ] - Epoch 84: val_loss did not improve from -17.3114
[ INFO : 2022-07-26 22:17:03,384 ] - Epoch 84/200 - time: 7.28 - training_loss: -17.1575 - val_loss: -17.3111
[ INFO : 2022-07-26 22:17:10,677 ] - Epoch 85: val_loss improved from -17.3114 to -17.3128, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:17:10,952 ] - Epoch 85/200 - time: 7.57 - training_loss: -17.1594 - val_loss: -17.3128
[ INFO : 2022-07-26 22:17:18,267 ] - Epoch 86: val_loss improved from -17.3128 to -17.3141, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:17:18,533 ] - Epoch 86/200 - time: 7.58 - training_loss: -17.1613 - val_loss: -17.3141
[ INFO : 2022-07-26 22:17:25,878 ] - Epoch 87: val_loss did not improve from -17.3141
[ INFO : 2022-07-26 22:17:25,878 ] - Epoch 87/200 - time: 7.34 - training_loss: -17.1631 - val_loss: -17.3140
[ INFO : 2022-07-26 22:17:33,200 ] - Epoch 88: val_loss improved from -17.3141 to -17.3141, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:17:33,470 ] - Epoch 88/200 - time: 7.59 - training_loss: -17.1648 - val_loss: -17.3141
[ INFO : 2022-07-26 22:17:40,834 ] - Epoch 89: val_loss improved from -17.3141 to -17.3147, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:17:41,126 ] - Epoch 89/200 - time: 7.66 - training_loss: -17.1666 - val_loss: -17.3147
[ INFO : 2022-07-26 22:17:48,453 ] - Epoch 90: val_loss did not improve from -17.3147
[ INFO : 2022-07-26 22:17:48,453 ] - Epoch 90/200 - time: 7.33 - training_loss: -17.1683 - val_loss: -17.3146
[ INFO : 2022-07-26 22:17:55,790 ] - Epoch 91: val_loss improved from -17.3147 to -17.3148, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:17:56,043 ] - Epoch 91/200 - time: 7.59 - training_loss: -17.1699 - val_loss: -17.3148
[ INFO : 2022-07-26 22:18:03,415 ] - Epoch 92: val_loss improved from -17.3148 to -17.3149, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:03,664 ] - Epoch 92/200 - time: 7.62 - training_loss: -17.1716 - val_loss: -17.3149
[ INFO : 2022-07-26 22:18:10,984 ] - Epoch 93: val_loss improved from -17.3149 to -17.3150, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:11,261 ] - Epoch 93/200 - time: 7.60 - training_loss: -17.1732 - val_loss: -17.3150
[ INFO : 2022-07-26 22:18:18,496 ] - Epoch 94: val_loss did not improve from -17.3150
[ INFO : 2022-07-26 22:18:18,496 ] - Epoch 94/200 - time: 7.23 - training_loss: -17.1747 - val_loss: -17.3150
[ INFO : 2022-07-26 22:18:25,835 ] - Epoch 95: val_loss improved from -17.3150 to -17.3151, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:26,116 ] - Epoch 95/200 - time: 7.62 - training_loss: -17.1763 - val_loss: -17.3151
[ INFO : 2022-07-26 22:18:33,467 ] - Epoch 96: val_loss did not improve from -17.3151
[ INFO : 2022-07-26 22:18:33,467 ] - Epoch 96/200 - time: 7.35 - training_loss: -17.1778 - val_loss: -17.3151
[ INFO : 2022-07-26 22:18:40,808 ] - Epoch 97: val_loss improved from -17.3151 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:41,081 ] - Epoch 97/200 - time: 7.61 - training_loss: -17.1792 - val_loss: -17.3152
[ INFO : 2022-07-26 22:18:48,410 ] - Epoch 98: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:48,739 ] - Epoch 98/200 - time: 7.66 - training_loss: -17.1807 - val_loss: -17.3152
[ INFO : 2022-07-26 22:18:56,051 ] - Epoch 99: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:18:56,320 ] - Epoch 99/200 - time: 7.58 - training_loss: -17.1821 - val_loss: -17.3152
[ INFO : 2022-07-26 22:19:03,713 ] - Epoch 100: val_loss improved from -17.3152 to -17.3152, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:19:03,981 ] - Epoch 100/200 - time: 7.66 - training_loss: -17.1834 - val_loss: -17.3152
[ INFO : 2022-07-26 22:19:11,322 ] - Epoch 101: val_loss did not improve from -17.3152
[ INFO : 2022-07-26 22:19:11,322 ] - Epoch 101/200 - time: 7.34 - training_loss: -17.1848 - val_loss: -17.3152
[ INFO : 2022-07-26 22:19:18,639 ] - Epoch 102: val_loss improved from -17.3152 to -17.3153, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:19:18,914 ] - Epoch 102/200 - time: 7.59 - training_loss: -17.1861 - val_loss: -17.3153
[ INFO : 2022-07-26 22:19:26,304 ] - Epoch 103: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 22:19:26,304 ] - Epoch 103/200 - time: 7.39 - training_loss: -17.1874 - val_loss: -17.3152
[ INFO : 2022-07-26 22:19:33,585 ] - Epoch 104: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 22:19:33,585 ] - Epoch 104/200 - time: 7.28 - training_loss: -17.1887 - val_loss: -17.3153
[ INFO : 2022-07-26 22:19:40,902 ] - Epoch 105: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 22:19:40,902 ] - Epoch 105/200 - time: 7.32 - training_loss: -17.1899 - val_loss: -17.3153
[ INFO : 2022-07-26 22:19:48,232 ] - Epoch 106: val_loss did not improve from -17.3153
[ INFO : 2022-07-26 22:19:48,232 ] - Epoch 106/200 - time: 7.33 - training_loss: -17.1912 - val_loss: -17.3152
[ INFO : 2022-07-26 22:19:55,523 ] - Epoch 107: val_loss improved from -17.3153 to -17.3154, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:19:55,797 ] - Epoch 107/200 - time: 7.57 - training_loss: -17.1924 - val_loss: -17.3154
[ INFO : 2022-07-26 22:20:03,097 ] - Epoch 108: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:03,097 ] - Epoch 108/200 - time: 7.30 - training_loss: -17.1936 - val_loss: -17.3152
[ INFO : 2022-07-26 22:20:10,403 ] - Epoch 109: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:10,403 ] - Epoch 109/200 - time: 7.31 - training_loss: -17.1947 - val_loss: -17.3153
[ INFO : 2022-07-26 22:20:17,681 ] - Epoch 110: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:17,681 ] - Epoch 110/200 - time: 7.28 - training_loss: -17.1959 - val_loss: -17.3153
[ INFO : 2022-07-26 22:20:24,971 ] - Epoch 111: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:24,971 ] - Epoch 111/200 - time: 7.29 - training_loss: -17.1970 - val_loss: -17.3149
[ INFO : 2022-07-26 22:20:32,265 ] - Epoch 112: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:32,265 ] - Epoch 112/200 - time: 7.29 - training_loss: -17.1981 - val_loss: -17.3153
[ INFO : 2022-07-26 22:20:39,597 ] - Epoch 113: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:39,597 ] - Epoch 113/200 - time: 7.33 - training_loss: -17.1992 - val_loss: -17.3151
[ INFO : 2022-07-26 22:20:46,930 ] - Epoch 114: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:46,930 ] - Epoch 114/200 - time: 7.33 - training_loss: -17.2002 - val_loss: -17.3145
[ INFO : 2022-07-26 22:20:54,269 ] - Epoch 115: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:20:54,269 ] - Epoch 115/200 - time: 7.34 - training_loss: -17.2013 - val_loss: -17.3153
[ INFO : 2022-07-26 22:21:01,564 ] - Epoch 116: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:01,564 ] - Epoch 116/200 - time: 7.29 - training_loss: -17.2023 - val_loss: -17.3141
[ INFO : 2022-07-26 22:21:08,854 ] - Epoch 117: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:08,854 ] - Epoch 117/200 - time: 7.29 - training_loss: -17.2033 - val_loss: -17.3138
[ INFO : 2022-07-26 22:21:16,118 ] - Epoch 118: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:16,118 ] - Epoch 118/200 - time: 7.26 - training_loss: -17.2043 - val_loss: -17.3151
[ INFO : 2022-07-26 22:21:23,413 ] - Epoch 119: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:23,414 ] - Epoch 119/200 - time: 7.30 - training_loss: -17.2052 - val_loss: -17.3114
[ INFO : 2022-07-26 22:21:30,707 ] - Epoch 120: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:30,707 ] - Epoch 120/200 - time: 7.29 - training_loss: -17.2062 - val_loss: -17.3145
[ INFO : 2022-07-26 22:21:37,960 ] - Epoch 121: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:37,960 ] - Epoch 121/200 - time: 7.25 - training_loss: -17.2071 - val_loss: -17.3125
[ INFO : 2022-07-26 22:21:45,278 ] - Epoch 122: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:45,278 ] - Epoch 122/200 - time: 7.32 - training_loss: -17.2080 - val_loss: -17.3149
[ INFO : 2022-07-26 22:21:52,586 ] - Epoch 123: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:52,586 ] - Epoch 123/200 - time: 7.31 - training_loss: -17.2090 - val_loss: -17.3153
[ INFO : 2022-07-26 22:21:59,897 ] - Epoch 124: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:21:59,897 ] - Epoch 124/200 - time: 7.31 - training_loss: -17.2099 - val_loss: -17.3148
[ INFO : 2022-07-26 22:22:07,184 ] - Epoch 125: val_loss did not improve from -17.3154
[ INFO : 2022-07-26 22:22:07,184 ] - Epoch 125/200 - time: 7.29 - training_loss: -17.2107 - val_loss: -17.3152
[ INFO : 2022-07-26 22:22:14,489 ] - Epoch 126: val_loss improved from -17.3154 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:22:14,746 ] - Epoch 126/200 - time: 7.56 - training_loss: -17.2116 - val_loss: -17.3155
[ INFO : 2022-07-26 22:22:22,035 ] - Epoch 127: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 22:22:22,035 ] - Epoch 127/200 - time: 7.29 - training_loss: -17.2125 - val_loss: -17.3153
[ INFO : 2022-07-26 22:22:29,301 ] - Epoch 128: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 22:22:29,302 ] - Epoch 128/200 - time: 7.27 - training_loss: -17.2133 - val_loss: -17.3154
[ INFO : 2022-07-26 22:22:36,603 ] - Epoch 129: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:22:36,864 ] - Epoch 129/200 - time: 7.56 - training_loss: -17.2141 - val_loss: -17.3155
[ INFO : 2022-07-26 22:22:44,136 ] - Epoch 130: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 22:22:44,136 ] - Epoch 130/200 - time: 7.27 - training_loss: -17.2149 - val_loss: -17.3155
[ INFO : 2022-07-26 22:22:51,397 ] - Epoch 131: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 22:22:51,397 ] - Epoch 131/200 - time: 7.26 - training_loss: -17.2158 - val_loss: -17.3155
[ INFO : 2022-07-26 22:22:58,702 ] - Epoch 132: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:22:58,984 ] - Epoch 132/200 - time: 7.59 - training_loss: -17.2165 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:06,287 ] - Epoch 133: val_loss did not improve from -17.3155
[ INFO : 2022-07-26 22:23:06,287 ] - Epoch 133/200 - time: 7.30 - training_loss: -17.2173 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:13,389 ] - Epoch 134: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:13,653 ] - Epoch 134/200 - time: 7.37 - training_loss: -17.2181 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:20,767 ] - Epoch 135: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:21,033 ] - Epoch 135/200 - time: 7.38 - training_loss: -17.2189 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:28,153 ] - Epoch 136: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:28,416 ] - Epoch 136/200 - time: 7.38 - training_loss: -17.2196 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:35,545 ] - Epoch 137: val_loss improved from -17.3155 to -17.3155, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:35,804 ] - Epoch 137/200 - time: 7.39 - training_loss: -17.2203 - val_loss: -17.3155
[ INFO : 2022-07-26 22:23:42,903 ] - Epoch 138: val_loss improved from -17.3155 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:43,255 ] - Epoch 138/200 - time: 7.45 - training_loss: -17.2211 - val_loss: -17.3156
[ INFO : 2022-07-26 22:23:50,358 ] - Epoch 139: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:50,642 ] - Epoch 139/200 - time: 7.39 - training_loss: -17.2218 - val_loss: -17.3156
[ INFO : 2022-07-26 22:23:57,724 ] - Epoch 140: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:23:58,047 ] - Epoch 140/200 - time: 7.41 - training_loss: -17.2225 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:05,168 ] - Epoch 141: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:05,453 ] - Epoch 141/200 - time: 7.41 - training_loss: -17.2232 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:12,542 ] - Epoch 142: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:12,811 ] - Epoch 142/200 - time: 7.36 - training_loss: -17.2239 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:19,949 ] - Epoch 143: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:20,213 ] - Epoch 143/200 - time: 7.40 - training_loss: -17.2245 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:27,370 ] - Epoch 144: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:27,666 ] - Epoch 144/200 - time: 7.45 - training_loss: -17.2252 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:34,716 ] - Epoch 145: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:34,988 ] - Epoch 145/200 - time: 7.32 - training_loss: -17.2259 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:42,073 ] - Epoch 146: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:42,331 ] - Epoch 146/200 - time: 7.34 - training_loss: -17.2265 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:49,416 ] - Epoch 147: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:49,696 ] - Epoch 147/200 - time: 7.37 - training_loss: -17.2271 - val_loss: -17.3156
[ INFO : 2022-07-26 22:24:56,825 ] - Epoch 148: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:24:57,087 ] - Epoch 148/200 - time: 7.39 - training_loss: -17.2278 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:04,199 ] - Epoch 149: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:04,477 ] - Epoch 149/200 - time: 7.39 - training_loss: -17.2284 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:11,560 ] - Epoch 150: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:11,888 ] - Epoch 150/200 - time: 7.41 - training_loss: -17.2290 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:19,020 ] - Epoch 151: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:19,308 ] - Epoch 151/200 - time: 7.42 - training_loss: -17.2296 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:26,419 ] - Epoch 152: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:26,741 ] - Epoch 152/200 - time: 7.43 - training_loss: -17.2302 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:33,852 ] - Epoch 153: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:34,112 ] - Epoch 153/200 - time: 7.37 - training_loss: -17.2308 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:41,182 ] - Epoch 154: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:41,443 ] - Epoch 154/200 - time: 7.33 - training_loss: -17.2314 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:48,565 ] - Epoch 155: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:48,877 ] - Epoch 155/200 - time: 7.43 - training_loss: -17.2320 - val_loss: -17.3156
[ INFO : 2022-07-26 22:25:55,965 ] - Epoch 156: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:25:56,222 ] - Epoch 156/200 - time: 7.35 - training_loss: -17.2325 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:03,298 ] - Epoch 157: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:03,595 ] - Epoch 157/200 - time: 7.37 - training_loss: -17.2331 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:10,685 ] - Epoch 158: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:10,963 ] - Epoch 158/200 - time: 7.37 - training_loss: -17.2336 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:18,043 ] - Epoch 159: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:18,298 ] - Epoch 159/200 - time: 7.33 - training_loss: -17.2342 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:25,401 ] - Epoch 160: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:25,642 ] - Epoch 160/200 - time: 7.34 - training_loss: -17.2347 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:32,711 ] - Epoch 161: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:32,986 ] - Epoch 161/200 - time: 7.34 - training_loss: -17.2352 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:40,092 ] - Epoch 162: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:40,417 ] - Epoch 162/200 - time: 7.43 - training_loss: -17.2358 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:47,531 ] - Epoch 163: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:47,805 ] - Epoch 163/200 - time: 7.39 - training_loss: -17.2363 - val_loss: -17.3156
[ INFO : 2022-07-26 22:26:54,910 ] - Epoch 164: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:26:55,278 ] - Epoch 164/200 - time: 7.47 - training_loss: -17.2368 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:02,383 ] - Epoch 165: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:02,647 ] - Epoch 165/200 - time: 7.37 - training_loss: -17.2373 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:09,788 ] - Epoch 166: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:10,063 ] - Epoch 166/200 - time: 7.42 - training_loss: -17.2378 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:17,175 ] - Epoch 167: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:17,449 ] - Epoch 167/200 - time: 7.39 - training_loss: -17.2383 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:24,584 ] - Epoch 168: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:24,851 ] - Epoch 168/200 - time: 7.40 - training_loss: -17.2388 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:31,983 ] - Epoch 169: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:32,238 ] - Epoch 169/200 - time: 7.39 - training_loss: -17.2393 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:39,348 ] - Epoch 170: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:39,608 ] - Epoch 170/200 - time: 7.37 - training_loss: -17.2398 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:46,733 ] - Epoch 171: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:47,002 ] - Epoch 171/200 - time: 7.39 - training_loss: -17.2402 - val_loss: -17.3156
[ INFO : 2022-07-26 22:27:54,097 ] - Epoch 172: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:27:54,360 ] - Epoch 172/200 - time: 7.36 - training_loss: -17.2407 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:01,507 ] - Epoch 173: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:01,794 ] - Epoch 173/200 - time: 7.43 - training_loss: -17.2412 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:08,904 ] - Epoch 174: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:09,180 ] - Epoch 174/200 - time: 7.39 - training_loss: -17.2416 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:16,298 ] - Epoch 175: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:16,555 ] - Epoch 175/200 - time: 7.38 - training_loss: -17.2421 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:23,648 ] - Epoch 176: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:23,917 ] - Epoch 176/200 - time: 7.36 - training_loss: -17.2425 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:31,011 ] - Epoch 177: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:31,351 ] - Epoch 177/200 - time: 7.43 - training_loss: -17.2429 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:38,457 ] - Epoch 178: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:38,741 ] - Epoch 178/200 - time: 7.39 - training_loss: -17.2434 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:45,986 ] - Epoch 179: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:46,248 ] - Epoch 179/200 - time: 7.51 - training_loss: -17.2438 - val_loss: -17.3156
[ INFO : 2022-07-26 22:28:53,552 ] - Epoch 180: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:28:53,822 ] - Epoch 180/200 - time: 7.57 - training_loss: -17.2442 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:01,154 ] - Epoch 181: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:01,506 ] - Epoch 181/200 - time: 7.68 - training_loss: -17.2447 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:08,809 ] - Epoch 182: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:09,119 ] - Epoch 182/200 - time: 7.61 - training_loss: -17.2451 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:16,356 ] - Epoch 183: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:16,624 ] - Epoch 183/200 - time: 7.50 - training_loss: -17.2455 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:23,878 ] - Epoch 184: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:24,153 ] - Epoch 184/200 - time: 7.53 - training_loss: -17.2459 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:31,479 ] - Epoch 185: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:31,750 ] - Epoch 185/200 - time: 7.60 - training_loss: -17.2463 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:39,109 ] - Epoch 186: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:39,383 ] - Epoch 186/200 - time: 7.63 - training_loss: -17.2467 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:46,697 ] - Epoch 187: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:46,981 ] - Epoch 187/200 - time: 7.60 - training_loss: -17.2471 - val_loss: -17.3156
[ INFO : 2022-07-26 22:29:54,356 ] - Epoch 188: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:29:54,641 ] - Epoch 188/200 - time: 7.66 - training_loss: -17.2475 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:01,988 ] - Epoch 189: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:02,274 ] - Epoch 189/200 - time: 7.63 - training_loss: -17.2479 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:09,630 ] - Epoch 190: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:09,921 ] - Epoch 190/200 - time: 7.65 - training_loss: -17.2482 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:17,225 ] - Epoch 191: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:17,484 ] - Epoch 191/200 - time: 7.56 - training_loss: -17.2486 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:24,807 ] - Epoch 192: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:25,070 ] - Epoch 192/200 - time: 7.59 - training_loss: -17.2490 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:32,370 ] - Epoch 193: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:32,656 ] - Epoch 193/200 - time: 7.59 - training_loss: -17.2494 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:39,973 ] - Epoch 194: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:40,234 ] - Epoch 194/200 - time: 7.58 - training_loss: -17.2497 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:47,523 ] - Epoch 195: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:47,787 ] - Epoch 195/200 - time: 7.55 - training_loss: -17.2501 - val_loss: -17.3156
[ INFO : 2022-07-26 22:30:55,132 ] - Epoch 196: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:30:55,415 ] - Epoch 196/200 - time: 7.63 - training_loss: -17.2505 - val_loss: -17.3156
[ INFO : 2022-07-26 22:31:02,807 ] - Epoch 197: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:31:03,092 ] - Epoch 197/200 - time: 7.68 - training_loss: -17.2508 - val_loss: -17.3156
[ INFO : 2022-07-26 22:31:10,441 ] - Epoch 198: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:31:10,714 ] - Epoch 198/200 - time: 7.62 - training_loss: -17.2512 - val_loss: -17.3156
[ INFO : 2022-07-26 22:31:18,013 ] - Epoch 199: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:31:18,284 ] - Epoch 199/200 - time: 7.57 - training_loss: -17.2515 - val_loss: -17.3156
[ INFO : 2022-07-26 22:31:25,600 ] - Epoch 200: val_loss improved from -17.3156 to -17.3156, saving model to ./DGCCA.model
[ INFO : 2022-07-26 22:31:25,869 ] - Epoch 200/200 - time: 7.59 - training_loss: -17.2519 - val_loss: -17.3156
[ INFO : 2022-07-26 22:31:29,566 ] - loss on validation data: -17.3156
[ INFO : 2022-07-27 15:05:54,191 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 15:05:54,197 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 15:37:00,923 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 15:37:00,924 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 15:37:15,243 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 15:37:15,243 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 16:30:41,129 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 16:30:41,129 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 17:02:46,773 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 17:02:46,773 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 17:03:05,778 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 17:03:05,778 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 17:03:54,388 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2022-07-27 17:03:54,389 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2022-07-27 17:04:27,363 ] - Epoch 1: val_loss improved from 0.0000 to -15.7151, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:04:27,618 ] - Epoch 1/200 - time: 33.23 - training_loss: -15.4469 - val_loss: -15.7151
[ INFO : 2022-07-27 17:04:45,207 ] - Epoch 2: val_loss improved from -15.7151 to -16.4396, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:04:45,582 ] - Epoch 2/200 - time: 17.96 - training_loss: -15.8956 - val_loss: -16.4396
[ INFO : 2022-07-27 17:05:03,184 ] - Epoch 3: val_loss improved from -16.4396 to -16.5652, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:05:03,441 ] - Epoch 3/200 - time: 17.86 - training_loss: -16.1644 - val_loss: -16.5652
[ INFO : 2022-07-27 17:05:21,078 ] - Epoch 4: val_loss improved from -16.5652 to -16.7461, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:05:21,322 ] - Epoch 4/200 - time: 17.88 - training_loss: -16.3360 - val_loss: -16.7461
[ INFO : 2022-07-27 17:05:38,785 ] - Epoch 5: val_loss improved from -16.7461 to -16.8794, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:05:39,075 ] - Epoch 5/200 - time: 17.75 - training_loss: -16.4670 - val_loss: -16.8794
[ INFO : 2022-07-27 17:05:56,521 ] - Epoch 6: val_loss improved from -16.8794 to -16.9125, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:05:56,797 ] - Epoch 6/200 - time: 17.72 - training_loss: -16.5686 - val_loss: -16.9125
[ INFO : 2022-07-27 17:06:14,249 ] - Epoch 7: val_loss improved from -16.9125 to -16.9612, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:06:14,529 ] - Epoch 7/200 - time: 17.73 - training_loss: -16.6464 - val_loss: -16.9612
[ INFO : 2022-07-27 17:06:32,323 ] - Epoch 8: val_loss improved from -16.9612 to -16.9718, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:06:32,587 ] - Epoch 8/200 - time: 18.06 - training_loss: -16.7081 - val_loss: -16.9718
[ INFO : 2022-07-27 17:06:50,120 ] - Epoch 9: val_loss improved from -16.9718 to -16.9883, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:06:50,429 ] - Epoch 9/200 - time: 17.84 - training_loss: -16.7582 - val_loss: -16.9883
[ INFO : 2022-07-27 17:07:07,976 ] - Epoch 10: val_loss improved from -16.9883 to -17.0029, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:07:08,259 ] - Epoch 10/200 - time: 17.83 - training_loss: -16.8003 - val_loss: -17.0029
[ INFO : 2022-07-27 17:07:25,783 ] - Epoch 11: val_loss improved from -17.0029 to -17.0323, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:07:26,035 ] - Epoch 11/200 - time: 17.78 - training_loss: -16.8363 - val_loss: -17.0323
[ INFO : 2022-07-27 17:07:43,525 ] - Epoch 12: val_loss improved from -17.0323 to -17.0457, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:07:43,799 ] - Epoch 12/200 - time: 17.76 - training_loss: -16.8666 - val_loss: -17.0457
[ INFO : 2022-07-27 17:08:01,303 ] - Epoch 13: val_loss improved from -17.0457 to -17.0557, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:08:01,600 ] - Epoch 13/200 - time: 17.80 - training_loss: -16.8932 - val_loss: -17.0557
[ INFO : 2022-07-27 17:08:19,097 ] - Epoch 14: val_loss did not improve from -17.0557
[ INFO : 2022-07-27 17:08:19,097 ] - Epoch 14/200 - time: 17.50 - training_loss: -16.9172 - val_loss: -17.0527
[ INFO : 2022-07-27 17:08:36,574 ] - Epoch 15: val_loss improved from -17.0557 to -17.0922, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:08:36,823 ] - Epoch 15/200 - time: 17.73 - training_loss: -16.9387 - val_loss: -17.0922
[ INFO : 2022-07-27 17:08:54,334 ] - Epoch 16: val_loss did not improve from -17.0922
[ INFO : 2022-07-27 17:08:54,334 ] - Epoch 16/200 - time: 17.51 - training_loss: -16.9578 - val_loss: -17.0671
[ INFO : 2022-07-27 17:09:11,927 ] - Epoch 17: val_loss did not improve from -17.0922
[ INFO : 2022-07-27 17:09:11,928 ] - Epoch 17/200 - time: 17.59 - training_loss: -16.9749 - val_loss: -17.0604
[ INFO : 2022-07-27 17:09:29,993 ] - Epoch 18: val_loss did not improve from -17.0922
[ INFO : 2022-07-27 17:09:29,993 ] - Epoch 18/200 - time: 18.07 - training_loss: -16.9899 - val_loss: -17.0493
[ INFO : 2022-07-27 17:09:48,149 ] - Epoch 19: val_loss did not improve from -17.0922
[ INFO : 2022-07-27 17:09:48,149 ] - Epoch 19/200 - time: 18.16 - training_loss: -17.0029 - val_loss: -17.0514
[ INFO : 2022-07-27 17:10:06,096 ] - Epoch 20: val_loss did not improve from -17.0922
[ INFO : 2022-07-27 17:10:06,096 ] - Epoch 20/200 - time: 17.95 - training_loss: -17.0141 - val_loss: -17.0683
[ INFO : 2022-07-27 17:10:24,080 ] - Epoch 21: val_loss improved from -17.0922 to -17.1038, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:10:24,360 ] - Epoch 21/200 - time: 18.26 - training_loss: -17.0253 - val_loss: -17.1038
[ INFO : 2022-07-27 17:10:42,326 ] - Epoch 22: val_loss improved from -17.1038 to -17.1105, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:10:42,670 ] - Epoch 22/200 - time: 18.31 - training_loss: -17.0359 - val_loss: -17.1105
[ INFO : 2022-07-27 17:11:00,229 ] - Epoch 23: val_loss improved from -17.1105 to -17.1133, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:11:00,533 ] - Epoch 23/200 - time: 17.86 - training_loss: -17.0457 - val_loss: -17.1133
[ INFO : 2022-07-27 17:11:18,028 ] - Epoch 24: val_loss did not improve from -17.1133
[ INFO : 2022-07-27 17:11:18,028 ] - Epoch 24/200 - time: 17.50 - training_loss: -17.0549 - val_loss: -17.0805
[ INFO : 2022-07-27 17:11:35,526 ] - Epoch 25: val_loss improved from -17.1133 to -17.1150, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:11:35,802 ] - Epoch 25/200 - time: 17.77 - training_loss: -17.0635 - val_loss: -17.1150
[ INFO : 2022-07-27 17:11:53,322 ] - Epoch 26: val_loss improved from -17.1150 to -17.1159, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:11:53,623 ] - Epoch 26/200 - time: 17.82 - training_loss: -17.0716 - val_loss: -17.1159
[ INFO : 2022-07-27 17:12:11,146 ] - Epoch 27: val_loss improved from -17.1159 to -17.1164, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:12:11,403 ] - Epoch 27/200 - time: 17.78 - training_loss: -17.0791 - val_loss: -17.1164
[ INFO : 2022-07-27 17:12:28,921 ] - Epoch 28: val_loss did not improve from -17.1164
[ INFO : 2022-07-27 17:12:28,922 ] - Epoch 28/200 - time: 17.52 - training_loss: -17.0860 - val_loss: -17.0939
[ INFO : 2022-07-27 17:12:46,402 ] - Epoch 29: val_loss did not improve from -17.1164
[ INFO : 2022-07-27 17:12:46,403 ] - Epoch 29/200 - time: 17.48 - training_loss: -17.0924 - val_loss: -17.0719
[ INFO : 2022-07-27 17:13:03,910 ] - Epoch 30: val_loss did not improve from -17.1164
[ INFO : 2022-07-27 17:13:03,910 ] - Epoch 30/200 - time: 17.51 - training_loss: -17.0983 - val_loss: -17.1039
[ INFO : 2022-07-27 17:13:21,514 ] - Epoch 31: val_loss improved from -17.1164 to -17.1183, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:13:21,787 ] - Epoch 31/200 - time: 17.88 - training_loss: -17.1037 - val_loss: -17.1183
[ INFO : 2022-07-27 17:13:39,408 ] - Epoch 32: val_loss did not improve from -17.1183
[ INFO : 2022-07-27 17:13:39,408 ] - Epoch 32/200 - time: 17.62 - training_loss: -17.1090 - val_loss: -17.1072
[ INFO : 2022-07-27 17:13:56,901 ] - Epoch 33: val_loss did not improve from -17.1183
[ INFO : 2022-07-27 17:13:56,901 ] - Epoch 33/200 - time: 17.49 - training_loss: -17.1139 - val_loss: -17.1179
[ INFO : 2022-07-27 17:14:14,462 ] - Epoch 34: val_loss did not improve from -17.1183
[ INFO : 2022-07-27 17:14:14,462 ] - Epoch 34/200 - time: 17.56 - training_loss: -17.1187 - val_loss: -17.1115
[ INFO : 2022-07-27 17:14:31,968 ] - Epoch 35: val_loss improved from -17.1183 to -17.1323, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:14:32,222 ] - Epoch 35/200 - time: 17.76 - training_loss: -17.1234 - val_loss: -17.1323
[ INFO : 2022-07-27 17:14:49,813 ] - Epoch 36: val_loss improved from -17.1323 to -17.1426, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:14:50,135 ] - Epoch 36/200 - time: 17.91 - training_loss: -17.1279 - val_loss: -17.1426
[ INFO : 2022-07-27 17:15:07,655 ] - Epoch 37: val_loss did not improve from -17.1426
[ INFO : 2022-07-27 17:15:07,655 ] - Epoch 37/200 - time: 17.52 - training_loss: -17.1323 - val_loss: -17.1383
[ INFO : 2022-07-27 17:15:25,127 ] - Epoch 38: val_loss did not improve from -17.1426
[ INFO : 2022-07-27 17:15:25,127 ] - Epoch 38/200 - time: 17.47 - training_loss: -17.1366 - val_loss: -17.1383
[ INFO : 2022-07-27 17:15:42,624 ] - Epoch 39: val_loss did not improve from -17.1426
[ INFO : 2022-07-27 17:15:42,625 ] - Epoch 39/200 - time: 17.50 - training_loss: -17.1407 - val_loss: -17.1356
[ INFO : 2022-07-27 17:16:00,108 ] - Epoch 40: val_loss did not improve from -17.1426
[ INFO : 2022-07-27 17:16:00,109 ] - Epoch 40/200 - time: 17.48 - training_loss: -17.1446 - val_loss: -17.1297
[ INFO : 2022-07-27 17:16:17,599 ] - Epoch 41: val_loss improved from -17.1426 to -17.1465, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:16:17,851 ] - Epoch 41/200 - time: 17.74 - training_loss: -17.1482 - val_loss: -17.1465
[ INFO : 2022-07-27 17:16:35,342 ] - Epoch 42: val_loss did not improve from -17.1465
[ INFO : 2022-07-27 17:16:35,342 ] - Epoch 42/200 - time: 17.49 - training_loss: -17.1516 - val_loss: -17.1452
[ INFO : 2022-07-27 17:16:52,830 ] - Epoch 43: val_loss improved from -17.1465 to -17.1490, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:16:53,116 ] - Epoch 43/200 - time: 17.77 - training_loss: -17.1549 - val_loss: -17.1490
[ INFO : 2022-07-27 17:17:10,592 ] - Epoch 44: val_loss improved from -17.1490 to -17.1558, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:17:10,863 ] - Epoch 44/200 - time: 17.75 - training_loss: -17.1581 - val_loss: -17.1558
[ INFO : 2022-07-27 17:17:28,336 ] - Epoch 45: val_loss did not improve from -17.1558
[ INFO : 2022-07-27 17:17:28,337 ] - Epoch 45/200 - time: 17.47 - training_loss: -17.1612 - val_loss: -17.1523
[ INFO : 2022-07-27 17:17:45,947 ] - Epoch 46: val_loss did not improve from -17.1558
[ INFO : 2022-07-27 17:17:45,947 ] - Epoch 46/200 - time: 17.61 - training_loss: -17.1641 - val_loss: -17.1485
[ INFO : 2022-07-27 17:18:03,915 ] - Epoch 47: val_loss did not improve from -17.1558
[ INFO : 2022-07-27 17:18:03,915 ] - Epoch 47/200 - time: 17.97 - training_loss: -17.1670 - val_loss: -17.1546
[ INFO : 2022-07-27 17:18:21,977 ] - Epoch 48: val_loss improved from -17.1558 to -17.1620, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:18:22,270 ] - Epoch 48/200 - time: 18.36 - training_loss: -17.1697 - val_loss: -17.1620
[ INFO : 2022-07-27 17:18:40,264 ] - Epoch 49: val_loss did not improve from -17.1620
[ INFO : 2022-07-27 17:18:40,264 ] - Epoch 49/200 - time: 17.99 - training_loss: -17.1724 - val_loss: -17.1591
[ INFO : 2022-07-27 17:18:58,320 ] - Epoch 50: val_loss did not improve from -17.1620
[ INFO : 2022-07-27 17:18:58,321 ] - Epoch 50/200 - time: 18.06 - training_loss: -17.1749 - val_loss: -17.1526
[ INFO : 2022-07-27 17:19:16,264 ] - Epoch 51: val_loss did not improve from -17.1620
[ INFO : 2022-07-27 17:19:16,264 ] - Epoch 51/200 - time: 17.94 - training_loss: -17.1774 - val_loss: -17.1584
[ INFO : 2022-07-27 17:19:33,852 ] - Epoch 52: val_loss improved from -17.1620 to -17.1640, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:19:34,109 ] - Epoch 52/200 - time: 17.85 - training_loss: -17.1798 - val_loss: -17.1640
[ INFO : 2022-07-27 17:19:51,595 ] - Epoch 53: val_loss improved from -17.1640 to -17.1651, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:19:51,876 ] - Epoch 53/200 - time: 17.77 - training_loss: -17.1821 - val_loss: -17.1651
[ INFO : 2022-07-27 17:20:09,351 ] - Epoch 54: val_loss improved from -17.1651 to -17.1665, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:20:09,700 ] - Epoch 54/200 - time: 17.82 - training_loss: -17.1843 - val_loss: -17.1665
[ INFO : 2022-07-27 17:20:27,192 ] - Epoch 55: val_loss improved from -17.1665 to -17.1680, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:20:27,522 ] - Epoch 55/200 - time: 17.82 - training_loss: -17.1865 - val_loss: -17.1680
[ INFO : 2022-07-27 17:20:45,075 ] - Epoch 56: val_loss improved from -17.1680 to -17.1686, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:20:45,339 ] - Epoch 56/200 - time: 17.82 - training_loss: -17.1886 - val_loss: -17.1686
[ INFO : 2022-07-27 17:21:02,886 ] - Epoch 57: val_loss improved from -17.1686 to -17.1689, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:21:03,147 ] - Epoch 57/200 - time: 17.81 - training_loss: -17.1907 - val_loss: -17.1689
[ INFO : 2022-07-27 17:21:20,661 ] - Epoch 58: val_loss improved from -17.1689 to -17.1694, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:21:20,919 ] - Epoch 58/200 - time: 17.77 - training_loss: -17.1927 - val_loss: -17.1694
[ INFO : 2022-07-27 17:21:38,490 ] - Epoch 59: val_loss improved from -17.1694 to -17.1701, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:21:38,779 ] - Epoch 59/200 - time: 17.86 - training_loss: -17.1946 - val_loss: -17.1701
[ INFO : 2022-07-27 17:21:56,301 ] - Epoch 60: val_loss improved from -17.1701 to -17.1708, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:21:56,568 ] - Epoch 60/200 - time: 17.79 - training_loss: -17.1965 - val_loss: -17.1708
[ INFO : 2022-07-27 17:22:14,140 ] - Epoch 61: val_loss improved from -17.1708 to -17.1732, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:22:14,427 ] - Epoch 61/200 - time: 17.86 - training_loss: -17.1983 - val_loss: -17.1732
[ INFO : 2022-07-27 17:22:31,928 ] - Epoch 62: val_loss did not improve from -17.1732
[ INFO : 2022-07-27 17:22:31,928 ] - Epoch 62/200 - time: 17.50 - training_loss: -17.2001 - val_loss: -17.1728
[ INFO : 2022-07-27 17:22:49,517 ] - Epoch 63: val_loss improved from -17.1732 to -17.1737, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:22:49,790 ] - Epoch 63/200 - time: 17.86 - training_loss: -17.2018 - val_loss: -17.1737
[ INFO : 2022-07-27 17:23:07,299 ] - Epoch 64: val_loss improved from -17.1737 to -17.1743, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:23:07,572 ] - Epoch 64/200 - time: 17.78 - training_loss: -17.2034 - val_loss: -17.1743
[ INFO : 2022-07-27 17:23:25,135 ] - Epoch 65: val_loss improved from -17.1743 to -17.1748, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:23:25,405 ] - Epoch 65/200 - time: 17.83 - training_loss: -17.2050 - val_loss: -17.1748
[ INFO : 2022-07-27 17:23:42,926 ] - Epoch 66: val_loss improved from -17.1748 to -17.1754, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:23:43,181 ] - Epoch 66/200 - time: 17.78 - training_loss: -17.2066 - val_loss: -17.1754
[ INFO : 2022-07-27 17:24:00,753 ] - Epoch 67: val_loss improved from -17.1754 to -17.1761, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:24:01,011 ] - Epoch 67/200 - time: 17.83 - training_loss: -17.2081 - val_loss: -17.1761
[ INFO : 2022-07-27 17:24:18,541 ] - Epoch 68: val_loss improved from -17.1761 to -17.1770, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:24:18,846 ] - Epoch 68/200 - time: 17.83 - training_loss: -17.2096 - val_loss: -17.1770
[ INFO : 2022-07-27 17:24:36,349 ] - Epoch 69: val_loss improved from -17.1770 to -17.1779, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:24:36,629 ] - Epoch 69/200 - time: 17.78 - training_loss: -17.2110 - val_loss: -17.1779
[ INFO : 2022-07-27 17:24:54,178 ] - Epoch 70: val_loss improved from -17.1779 to -17.1787, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:24:54,434 ] - Epoch 70/200 - time: 17.81 - training_loss: -17.2125 - val_loss: -17.1787
[ INFO : 2022-07-27 17:25:11,900 ] - Epoch 71: val_loss improved from -17.1787 to -17.1810, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:25:12,351 ] - Epoch 71/200 - time: 17.92 - training_loss: -17.2138 - val_loss: -17.1810
[ INFO : 2022-07-27 17:25:29,956 ] - Epoch 72: val_loss did not improve from -17.1810
[ INFO : 2022-07-27 17:25:29,956 ] - Epoch 72/200 - time: 17.61 - training_loss: -17.2152 - val_loss: -17.1810
[ INFO : 2022-07-27 17:25:47,577 ] - Epoch 73: val_loss improved from -17.1810 to -17.1812, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:25:47,832 ] - Epoch 73/200 - time: 17.88 - training_loss: -17.2165 - val_loss: -17.1812
[ INFO : 2022-07-27 17:26:05,333 ] - Epoch 74: val_loss improved from -17.1812 to -17.1813, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:26:05,649 ] - Epoch 74/200 - time: 17.82 - training_loss: -17.2178 - val_loss: -17.1813
[ INFO : 2022-07-27 17:26:23,190 ] - Epoch 75: val_loss improved from -17.1813 to -17.1814, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:26:23,493 ] - Epoch 75/200 - time: 17.84 - training_loss: -17.2190 - val_loss: -17.1814
[ INFO : 2022-07-27 17:26:40,974 ] - Epoch 76: val_loss improved from -17.1814 to -17.1815, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:26:41,246 ] - Epoch 76/200 - time: 17.75 - training_loss: -17.2202 - val_loss: -17.1815
[ INFO : 2022-07-27 17:26:58,761 ] - Epoch 77: val_loss improved from -17.1815 to -17.1815, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:26:59,016 ] - Epoch 77/200 - time: 17.77 - training_loss: -17.2214 - val_loss: -17.1815
[ INFO : 2022-07-27 17:27:16,485 ] - Epoch 78: val_loss improved from -17.1815 to -17.1815, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:27:16,769 ] - Epoch 78/200 - time: 17.75 - training_loss: -17.2226 - val_loss: -17.1815
[ INFO : 2022-07-27 17:27:34,266 ] - Epoch 79: val_loss improved from -17.1815 to -17.1816, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:27:34,535 ] - Epoch 79/200 - time: 17.77 - training_loss: -17.2237 - val_loss: -17.1816
[ INFO : 2022-07-27 17:27:52,034 ] - Epoch 80: val_loss improved from -17.1816 to -17.1816, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:27:52,292 ] - Epoch 80/200 - time: 17.76 - training_loss: -17.2248 - val_loss: -17.1816
[ INFO : 2022-07-27 17:28:09,798 ] - Epoch 81: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:28:09,798 ] - Epoch 81/200 - time: 17.51 - training_loss: -17.2259 - val_loss: -17.1803
[ INFO : 2022-07-27 17:28:27,313 ] - Epoch 82: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:28:27,313 ] - Epoch 82/200 - time: 17.51 - training_loss: -17.2269 - val_loss: -17.1795
[ INFO : 2022-07-27 17:28:44,890 ] - Epoch 83: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:28:44,891 ] - Epoch 83/200 - time: 17.58 - training_loss: -17.2279 - val_loss: -17.1796
[ INFO : 2022-07-27 17:29:02,870 ] - Epoch 84: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:29:02,870 ] - Epoch 84/200 - time: 17.98 - training_loss: -17.2289 - val_loss: -17.1797
[ INFO : 2022-07-27 17:29:20,896 ] - Epoch 85: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:29:20,896 ] - Epoch 85/200 - time: 18.03 - training_loss: -17.2299 - val_loss: -17.1797
[ INFO : 2022-07-27 17:29:38,913 ] - Epoch 86: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:29:38,913 ] - Epoch 86/200 - time: 18.02 - training_loss: -17.2309 - val_loss: -17.1797
[ INFO : 2022-07-27 17:29:56,908 ] - Epoch 87: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:29:56,908 ] - Epoch 87/200 - time: 17.99 - training_loss: -17.2318 - val_loss: -17.1796
[ INFO : 2022-07-27 17:30:14,831 ] - Epoch 88: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:30:14,831 ] - Epoch 88/200 - time: 17.92 - training_loss: -17.2327 - val_loss: -17.1796
[ INFO : 2022-07-27 17:30:32,773 ] - Epoch 89: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:30:32,773 ] - Epoch 89/200 - time: 17.94 - training_loss: -17.2336 - val_loss: -17.1795
[ INFO : 2022-07-27 17:30:50,823 ] - Epoch 90: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:30:50,823 ] - Epoch 90/200 - time: 18.05 - training_loss: -17.2345 - val_loss: -17.1794
[ INFO : 2022-07-27 17:31:08,784 ] - Epoch 91: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:31:08,784 ] - Epoch 91/200 - time: 17.96 - training_loss: -17.2353 - val_loss: -17.1783
[ INFO : 2022-07-27 17:31:26,775 ] - Epoch 92: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:31:26,775 ] - Epoch 92/200 - time: 17.99 - training_loss: -17.2362 - val_loss: -17.1785
[ INFO : 2022-07-27 17:31:44,808 ] - Epoch 93: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:31:44,808 ] - Epoch 93/200 - time: 18.03 - training_loss: -17.2370 - val_loss: -17.1788
[ INFO : 2022-07-27 17:32:02,810 ] - Epoch 94: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:32:02,811 ] - Epoch 94/200 - time: 18.00 - training_loss: -17.2378 - val_loss: -17.1790
[ INFO : 2022-07-27 17:32:20,828 ] - Epoch 95: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:32:20,828 ] - Epoch 95/200 - time: 18.02 - training_loss: -17.2386 - val_loss: -17.1790
[ INFO : 2022-07-27 17:32:38,824 ] - Epoch 96: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:32:38,824 ] - Epoch 96/200 - time: 18.00 - training_loss: -17.2393 - val_loss: -17.1789
[ INFO : 2022-07-27 17:32:56,805 ] - Epoch 97: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:32:56,805 ] - Epoch 97/200 - time: 17.98 - training_loss: -17.2401 - val_loss: -17.1788
[ INFO : 2022-07-27 17:33:14,773 ] - Epoch 98: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:33:14,773 ] - Epoch 98/200 - time: 17.97 - training_loss: -17.2408 - val_loss: -17.1786
[ INFO : 2022-07-27 17:33:32,755 ] - Epoch 99: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:33:32,755 ] - Epoch 99/200 - time: 17.98 - training_loss: -17.2416 - val_loss: -17.1785
[ INFO : 2022-07-27 17:33:50,791 ] - Epoch 100: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:33:50,791 ] - Epoch 100/200 - time: 18.04 - training_loss: -17.2423 - val_loss: -17.1785
[ INFO : 2022-07-27 17:34:08,855 ] - Epoch 101: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:34:08,855 ] - Epoch 101/200 - time: 18.06 - training_loss: -17.2430 - val_loss: -17.1793
[ INFO : 2022-07-27 17:34:27,039 ] - Epoch 102: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:34:27,040 ] - Epoch 102/200 - time: 18.18 - training_loss: -17.2437 - val_loss: -17.1806
[ INFO : 2022-07-27 17:34:45,008 ] - Epoch 103: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:34:45,008 ] - Epoch 103/200 - time: 17.97 - training_loss: -17.2443 - val_loss: -17.1813
[ INFO : 2022-07-27 17:35:03,057 ] - Epoch 104: val_loss did not improve from -17.1816
[ INFO : 2022-07-27 17:35:03,057 ] - Epoch 104/200 - time: 18.05 - training_loss: -17.2450 - val_loss: -17.1816
[ INFO : 2022-07-27 17:35:21,102 ] - Epoch 105: val_loss improved from -17.1816 to -17.1818, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:35:21,383 ] - Epoch 105/200 - time: 18.33 - training_loss: -17.2457 - val_loss: -17.1818
[ INFO : 2022-07-27 17:35:39,536 ] - Epoch 106: val_loss improved from -17.1818 to -17.1819, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:35:39,808 ] - Epoch 106/200 - time: 18.42 - training_loss: -17.2463 - val_loss: -17.1819
[ INFO : 2022-07-27 17:35:57,822 ] - Epoch 107: val_loss improved from -17.1819 to -17.1820, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:35:58,158 ] - Epoch 107/200 - time: 18.35 - training_loss: -17.2469 - val_loss: -17.1820
[ INFO : 2022-07-27 17:36:16,155 ] - Epoch 108: val_loss improved from -17.1820 to -17.1821, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:36:16,431 ] - Epoch 108/200 - time: 18.27 - training_loss: -17.2476 - val_loss: -17.1821
[ INFO : 2022-07-27 17:36:34,437 ] - Epoch 109: val_loss improved from -17.1821 to -17.1823, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:36:34,698 ] - Epoch 109/200 - time: 18.27 - training_loss: -17.2482 - val_loss: -17.1823
[ INFO : 2022-07-27 17:36:52,768 ] - Epoch 110: val_loss improved from -17.1823 to -17.1824, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:36:53,079 ] - Epoch 110/200 - time: 18.38 - training_loss: -17.2488 - val_loss: -17.1824
[ INFO : 2022-07-27 17:37:11,096 ] - Epoch 111: val_loss improved from -17.1824 to -17.1841, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:37:11,363 ] - Epoch 111/200 - time: 18.28 - training_loss: -17.2494 - val_loss: -17.1841
[ INFO : 2022-07-27 17:37:29,409 ] - Epoch 112: val_loss improved from -17.1841 to -17.1843, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:37:29,722 ] - Epoch 112/200 - time: 18.36 - training_loss: -17.2499 - val_loss: -17.1843
[ INFO : 2022-07-27 17:37:47,820 ] - Epoch 113: val_loss improved from -17.1843 to -17.1843, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:37:48,087 ] - Epoch 113/200 - time: 18.36 - training_loss: -17.2505 - val_loss: -17.1843
[ INFO : 2022-07-27 17:38:06,243 ] - Epoch 114: val_loss improved from -17.1843 to -17.1844, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:38:06,558 ] - Epoch 114/200 - time: 18.47 - training_loss: -17.2511 - val_loss: -17.1844
[ INFO : 2022-07-27 17:38:24,617 ] - Epoch 115: val_loss improved from -17.1844 to -17.1845, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:38:24,869 ] - Epoch 115/200 - time: 18.31 - training_loss: -17.2516 - val_loss: -17.1845
[ INFO : 2022-07-27 17:38:42,838 ] - Epoch 116: val_loss improved from -17.1845 to -17.1845, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:38:43,084 ] - Epoch 116/200 - time: 18.21 - training_loss: -17.2522 - val_loss: -17.1845
[ INFO : 2022-07-27 17:39:00,581 ] - Epoch 117: val_loss improved from -17.1845 to -17.1845, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:39:00,850 ] - Epoch 117/200 - time: 17.76 - training_loss: -17.2527 - val_loss: -17.1845
[ INFO : 2022-07-27 17:39:18,338 ] - Epoch 118: val_loss improved from -17.1845 to -17.1845, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:39:18,776 ] - Epoch 118/200 - time: 17.93 - training_loss: -17.2532 - val_loss: -17.1845
[ INFO : 2022-07-27 17:39:36,279 ] - Epoch 119: val_loss improved from -17.1845 to -17.1846, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:39:36,625 ] - Epoch 119/200 - time: 17.85 - training_loss: -17.2537 - val_loss: -17.1846
[ INFO : 2022-07-27 17:39:54,172 ] - Epoch 120: val_loss improved from -17.1846 to -17.1846, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:39:54,446 ] - Epoch 120/200 - time: 17.82 - training_loss: -17.2542 - val_loss: -17.1846
[ INFO : 2022-07-27 17:40:11,984 ] - Epoch 121: val_loss improved from -17.1846 to -17.1846, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:40:12,238 ] - Epoch 121/200 - time: 17.79 - training_loss: -17.2547 - val_loss: -17.1846
[ INFO : 2022-07-27 17:40:29,734 ] - Epoch 122: val_loss improved from -17.1846 to -17.1848, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:40:30,021 ] - Epoch 122/200 - time: 17.78 - training_loss: -17.2552 - val_loss: -17.1848
[ INFO : 2022-07-27 17:40:47,536 ] - Epoch 123: val_loss improved from -17.1848 to -17.1848, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:40:47,824 ] - Epoch 123/200 - time: 17.80 - training_loss: -17.2557 - val_loss: -17.1848
[ INFO : 2022-07-27 17:41:05,389 ] - Epoch 124: val_loss improved from -17.1848 to -17.1849, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:41:05,678 ] - Epoch 124/200 - time: 17.85 - training_loss: -17.2562 - val_loss: -17.1849
[ INFO : 2022-07-27 17:41:23,173 ] - Epoch 125: val_loss improved from -17.1849 to -17.1849, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:41:23,453 ] - Epoch 125/200 - time: 17.77 - training_loss: -17.2567 - val_loss: -17.1849
[ INFO : 2022-07-27 17:41:40,935 ] - Epoch 126: val_loss improved from -17.1849 to -17.1850, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:41:41,236 ] - Epoch 126/200 - time: 17.78 - training_loss: -17.2571 - val_loss: -17.1850
[ INFO : 2022-07-27 17:41:58,763 ] - Epoch 127: val_loss improved from -17.1850 to -17.1850, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:41:59,024 ] - Epoch 127/200 - time: 17.79 - training_loss: -17.2576 - val_loss: -17.1850
[ INFO : 2022-07-27 17:42:16,536 ] - Epoch 128: val_loss improved from -17.1850 to -17.1850, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:42:16,790 ] - Epoch 128/200 - time: 17.77 - training_loss: -17.2580 - val_loss: -17.1850
[ INFO : 2022-07-27 17:42:34,264 ] - Epoch 129: val_loss improved from -17.1850 to -17.1850, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:42:34,533 ] - Epoch 129/200 - time: 17.74 - training_loss: -17.2585 - val_loss: -17.1850
[ INFO : 2022-07-27 17:42:52,054 ] - Epoch 130: val_loss improved from -17.1850 to -17.1850, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:42:52,324 ] - Epoch 130/200 - time: 17.79 - training_loss: -17.2589 - val_loss: -17.1850
[ INFO : 2022-07-27 17:43:09,812 ] - Epoch 131: val_loss improved from -17.1850 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:43:10,079 ] - Epoch 131/200 - time: 17.76 - training_loss: -17.2593 - val_loss: -17.1853
[ INFO : 2022-07-27 17:43:27,923 ] - Epoch 132: val_loss improved from -17.1853 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:43:28,248 ] - Epoch 132/200 - time: 18.17 - training_loss: -17.2598 - val_loss: -17.1853
[ INFO : 2022-07-27 17:43:46,358 ] - Epoch 133: val_loss improved from -17.1853 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:43:46,612 ] - Epoch 133/200 - time: 18.36 - training_loss: -17.2602 - val_loss: -17.1853
[ INFO : 2022-07-27 17:44:04,702 ] - Epoch 134: val_loss improved from -17.1853 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:44:04,979 ] - Epoch 134/200 - time: 18.37 - training_loss: -17.2606 - val_loss: -17.1853
[ INFO : 2022-07-27 17:44:23,091 ] - Epoch 135: val_loss improved from -17.1853 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:44:23,361 ] - Epoch 135/200 - time: 18.38 - training_loss: -17.2610 - val_loss: -17.1853
[ INFO : 2022-07-27 17:44:41,457 ] - Epoch 136: val_loss improved from -17.1853 to -17.1853, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:44:41,735 ] - Epoch 136/200 - time: 18.37 - training_loss: -17.2614 - val_loss: -17.1853
[ INFO : 2022-07-27 17:44:59,839 ] - Epoch 137: val_loss improved from -17.1853 to -17.1854, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:45:00,102 ] - Epoch 137/200 - time: 18.37 - training_loss: -17.2618 - val_loss: -17.1854
[ INFO : 2022-07-27 17:45:18,186 ] - Epoch 138: val_loss improved from -17.1854 to -17.1854, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:45:18,457 ] - Epoch 138/200 - time: 18.36 - training_loss: -17.2622 - val_loss: -17.1854
[ INFO : 2022-07-27 17:45:36,505 ] - Epoch 139: val_loss improved from -17.1854 to -17.1854, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:45:36,777 ] - Epoch 139/200 - time: 18.32 - training_loss: -17.2626 - val_loss: -17.1854
[ INFO : 2022-07-27 17:45:54,859 ] - Epoch 140: val_loss improved from -17.1854 to -17.1854, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:45:55,141 ] - Epoch 140/200 - time: 18.36 - training_loss: -17.2629 - val_loss: -17.1854
[ INFO : 2022-07-27 17:46:13,279 ] - Epoch 141: val_loss improved from -17.1854 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:46:13,540 ] - Epoch 141/200 - time: 18.40 - training_loss: -17.2633 - val_loss: -17.1855
[ INFO : 2022-07-27 17:46:31,563 ] - Epoch 142: val_loss did not improve from -17.1855
[ INFO : 2022-07-27 17:46:31,564 ] - Epoch 142/200 - time: 18.02 - training_loss: -17.2637 - val_loss: -17.1855
[ INFO : 2022-07-27 17:46:49,522 ] - Epoch 143: val_loss did not improve from -17.1855
[ INFO : 2022-07-27 17:46:49,522 ] - Epoch 143/200 - time: 17.96 - training_loss: -17.2640 - val_loss: -17.1855
[ INFO : 2022-07-27 17:47:07,509 ] - Epoch 144: val_loss did not improve from -17.1855
[ INFO : 2022-07-27 17:47:07,509 ] - Epoch 144/200 - time: 17.99 - training_loss: -17.2644 - val_loss: -17.1855
[ INFO : 2022-07-27 17:47:25,474 ] - Epoch 145: val_loss improved from -17.1855 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:47:25,754 ] - Epoch 145/200 - time: 18.24 - training_loss: -17.2648 - val_loss: -17.1855
[ INFO : 2022-07-27 17:47:43,800 ] - Epoch 146: val_loss improved from -17.1855 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:47:44,063 ] - Epoch 146/200 - time: 18.31 - training_loss: -17.2651 - val_loss: -17.1855
[ INFO : 2022-07-27 17:48:02,180 ] - Epoch 147: val_loss improved from -17.1855 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:48:02,444 ] - Epoch 147/200 - time: 18.38 - training_loss: -17.2654 - val_loss: -17.1855
[ INFO : 2022-07-27 17:48:20,589 ] - Epoch 148: val_loss improved from -17.1855 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:48:20,845 ] - Epoch 148/200 - time: 18.40 - training_loss: -17.2658 - val_loss: -17.1855
[ INFO : 2022-07-27 17:48:38,936 ] - Epoch 149: val_loss improved from -17.1855 to -17.1855, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:48:39,252 ] - Epoch 149/200 - time: 18.41 - training_loss: -17.2661 - val_loss: -17.1855
[ INFO : 2022-07-27 17:48:57,387 ] - Epoch 150: val_loss improved from -17.1855 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:48:57,668 ] - Epoch 150/200 - time: 18.42 - training_loss: -17.2664 - val_loss: -17.1856
[ INFO : 2022-07-27 17:49:15,750 ] - Epoch 151: val_loss improved from -17.1856 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:49:16,067 ] - Epoch 151/200 - time: 18.40 - training_loss: -17.2668 - val_loss: -17.1856
[ INFO : 2022-07-27 17:49:34,269 ] - Epoch 152: val_loss did not improve from -17.1856
[ INFO : 2022-07-27 17:49:34,269 ] - Epoch 152/200 - time: 18.20 - training_loss: -17.2671 - val_loss: -17.1856
[ INFO : 2022-07-27 17:49:52,326 ] - Epoch 153: val_loss did not improve from -17.1856
[ INFO : 2022-07-27 17:49:52,326 ] - Epoch 153/200 - time: 18.06 - training_loss: -17.2674 - val_loss: -17.1856
[ INFO : 2022-07-27 17:50:10,448 ] - Epoch 154: val_loss did not improve from -17.1856
[ INFO : 2022-07-27 17:50:10,448 ] - Epoch 154/200 - time: 18.12 - training_loss: -17.2677 - val_loss: -17.1856
[ INFO : 2022-07-27 17:50:28,545 ] - Epoch 155: val_loss improved from -17.1856 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:50:28,856 ] - Epoch 155/200 - time: 18.41 - training_loss: -17.2680 - val_loss: -17.1856
[ INFO : 2022-07-27 17:50:46,970 ] - Epoch 156: val_loss improved from -17.1856 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:50:47,299 ] - Epoch 156/200 - time: 18.44 - training_loss: -17.2683 - val_loss: -17.1856
[ INFO : 2022-07-27 17:51:05,403 ] - Epoch 157: val_loss improved from -17.1856 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:51:05,690 ] - Epoch 157/200 - time: 18.39 - training_loss: -17.2686 - val_loss: -17.1856
[ INFO : 2022-07-27 17:51:23,724 ] - Epoch 158: val_loss improved from -17.1856 to -17.1856, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:51:24,223 ] - Epoch 158/200 - time: 18.53 - training_loss: -17.2689 - val_loss: -17.1856
[ INFO : 2022-07-27 17:51:42,178 ] - Epoch 159: val_loss improved from -17.1856 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:51:42,431 ] - Epoch 159/200 - time: 18.21 - training_loss: -17.2692 - val_loss: -17.1857
[ INFO : 2022-07-27 17:52:00,475 ] - Epoch 160: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:52:00,754 ] - Epoch 160/200 - time: 18.32 - training_loss: -17.2695 - val_loss: -17.1857
[ INFO : 2022-07-27 17:52:18,743 ] - Epoch 161: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:52:19,012 ] - Epoch 161/200 - time: 18.26 - training_loss: -17.2698 - val_loss: -17.1857
[ INFO : 2022-07-27 17:52:36,998 ] - Epoch 162: val_loss did not improve from -17.1857
[ INFO : 2022-07-27 17:52:36,998 ] - Epoch 162/200 - time: 17.99 - training_loss: -17.2701 - val_loss: -17.1857
[ INFO : 2022-07-27 17:52:54,981 ] - Epoch 163: val_loss did not improve from -17.1857
[ INFO : 2022-07-27 17:52:54,981 ] - Epoch 163/200 - time: 17.98 - training_loss: -17.2704 - val_loss: -17.1857
[ INFO : 2022-07-27 17:53:13,002 ] - Epoch 164: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:53:13,268 ] - Epoch 164/200 - time: 18.29 - training_loss: -17.2706 - val_loss: -17.1857
[ INFO : 2022-07-27 17:53:30,916 ] - Epoch 165: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:53:31,200 ] - Epoch 165/200 - time: 17.93 - training_loss: -17.2709 - val_loss: -17.1857
[ INFO : 2022-07-27 17:53:48,680 ] - Epoch 166: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:53:48,941 ] - Epoch 166/200 - time: 17.74 - training_loss: -17.2712 - val_loss: -17.1857
[ INFO : 2022-07-27 17:54:06,409 ] - Epoch 167: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:54:06,665 ] - Epoch 167/200 - time: 17.72 - training_loss: -17.2714 - val_loss: -17.1857
[ INFO : 2022-07-27 17:54:24,172 ] - Epoch 168: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:54:24,430 ] - Epoch 168/200 - time: 17.77 - training_loss: -17.2717 - val_loss: -17.1857
[ INFO : 2022-07-27 17:54:41,900 ] - Epoch 169: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:54:42,155 ] - Epoch 169/200 - time: 17.72 - training_loss: -17.2720 - val_loss: -17.1857
[ INFO : 2022-07-27 17:54:59,705 ] - Epoch 170: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:54:59,962 ] - Epoch 170/200 - time: 17.81 - training_loss: -17.2722 - val_loss: -17.1857
[ INFO : 2022-07-27 17:55:17,441 ] - Epoch 171: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:55:17,713 ] - Epoch 171/200 - time: 17.75 - training_loss: -17.2725 - val_loss: -17.1857
[ INFO : 2022-07-27 17:55:35,194 ] - Epoch 172: val_loss did not improve from -17.1857
[ INFO : 2022-07-27 17:55:35,194 ] - Epoch 172/200 - time: 17.48 - training_loss: -17.2727 - val_loss: -17.1857
[ INFO : 2022-07-27 17:55:52,693 ] - Epoch 173: val_loss improved from -17.1857 to -17.1857, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:55:52,967 ] - Epoch 173/200 - time: 17.77 - training_loss: -17.2730 - val_loss: -17.1857
[ INFO : 2022-07-27 17:56:10,448 ] - Epoch 174: val_loss improved from -17.1857 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:56:10,717 ] - Epoch 174/200 - time: 17.75 - training_loss: -17.2732 - val_loss: -17.1858
[ INFO : 2022-07-27 17:56:28,265 ] - Epoch 175: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:56:28,524 ] - Epoch 175/200 - time: 17.81 - training_loss: -17.2735 - val_loss: -17.1858
[ INFO : 2022-07-27 17:56:46,012 ] - Epoch 176: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:56:46,300 ] - Epoch 176/200 - time: 17.78 - training_loss: -17.2737 - val_loss: -17.1858
[ INFO : 2022-07-27 17:57:03,785 ] - Epoch 177: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:57:04,065 ] - Epoch 177/200 - time: 17.76 - training_loss: -17.2739 - val_loss: -17.1858
[ INFO : 2022-07-27 17:57:21,567 ] - Epoch 178: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:57:21,831 ] - Epoch 178/200 - time: 17.77 - training_loss: -17.2742 - val_loss: -17.1858
[ INFO : 2022-07-27 17:57:39,343 ] - Epoch 179: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:57:39,612 ] - Epoch 179/200 - time: 17.78 - training_loss: -17.2744 - val_loss: -17.1858
[ INFO : 2022-07-27 17:57:57,114 ] - Epoch 180: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:57:57,387 ] - Epoch 180/200 - time: 17.78 - training_loss: -17.2746 - val_loss: -17.1858
[ INFO : 2022-07-27 17:58:14,962 ] - Epoch 181: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:58:15,236 ] - Epoch 181/200 - time: 17.85 - training_loss: -17.2749 - val_loss: -17.1858
[ INFO : 2022-07-27 17:58:32,755 ] - Epoch 182: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:58:33,028 ] - Epoch 182/200 - time: 17.79 - training_loss: -17.2751 - val_loss: -17.1858
[ INFO : 2022-07-27 17:58:50,523 ] - Epoch 183: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:58:50,759 ] - Epoch 183/200 - time: 17.73 - training_loss: -17.2753 - val_loss: -17.1858
[ INFO : 2022-07-27 17:59:08,243 ] - Epoch 184: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:59:08,517 ] - Epoch 184/200 - time: 17.76 - training_loss: -17.2755 - val_loss: -17.1858
[ INFO : 2022-07-27 17:59:26,054 ] - Epoch 185: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:59:26,342 ] - Epoch 185/200 - time: 17.82 - training_loss: -17.2757 - val_loss: -17.1858
[ INFO : 2022-07-27 17:59:43,854 ] - Epoch 186: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 17:59:44,142 ] - Epoch 186/200 - time: 17.80 - training_loss: -17.2760 - val_loss: -17.1858
[ INFO : 2022-07-27 18:00:01,645 ] - Epoch 187: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:00:01,931 ] - Epoch 187/200 - time: 17.79 - training_loss: -17.2762 - val_loss: -17.1858
[ INFO : 2022-07-27 18:00:19,522 ] - Epoch 188: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:00:19,813 ] - Epoch 188/200 - time: 17.88 - training_loss: -17.2764 - val_loss: -17.1858
[ INFO : 2022-07-27 18:00:37,355 ] - Epoch 189: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:00:37,629 ] - Epoch 189/200 - time: 17.82 - training_loss: -17.2766 - val_loss: -17.1858
[ INFO : 2022-07-27 18:00:55,229 ] - Epoch 190: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:00:55,512 ] - Epoch 190/200 - time: 17.88 - training_loss: -17.2768 - val_loss: -17.1858
[ INFO : 2022-07-27 18:01:13,041 ] - Epoch 191: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:01:13,518 ] - Epoch 191/200 - time: 18.01 - training_loss: -17.2770 - val_loss: -17.1858
[ INFO : 2022-07-27 18:01:31,471 ] - Epoch 192: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:01:31,728 ] - Epoch 192/200 - time: 18.21 - training_loss: -17.2772 - val_loss: -17.1858
[ INFO : 2022-07-27 18:01:49,396 ] - Epoch 193: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:01:50,187 ] - Epoch 193/200 - time: 18.46 - training_loss: -17.2774 - val_loss: -17.1858
[ INFO : 2022-07-27 18:02:07,982 ] - Epoch 194: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:02:08,735 ] - Epoch 194/200 - time: 18.55 - training_loss: -17.2776 - val_loss: -17.1858
[ INFO : 2022-07-27 18:02:26,381 ] - Epoch 195: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:02:26,633 ] - Epoch 195/200 - time: 17.90 - training_loss: -17.2778 - val_loss: -17.1858
[ INFO : 2022-07-27 18:02:44,243 ] - Epoch 196: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:02:44,528 ] - Epoch 196/200 - time: 17.90 - training_loss: -17.2780 - val_loss: -17.1858
[ INFO : 2022-07-27 18:03:02,280 ] - Epoch 197: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:03:02,584 ] - Epoch 197/200 - time: 18.06 - training_loss: -17.2782 - val_loss: -17.1858
[ INFO : 2022-07-27 18:03:20,456 ] - Epoch 198: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:03:20,715 ] - Epoch 198/200 - time: 18.13 - training_loss: -17.2784 - val_loss: -17.1858
[ INFO : 2022-07-27 18:03:38,213 ] - Epoch 199: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:03:38,523 ] - Epoch 199/200 - time: 17.81 - training_loss: -17.2786 - val_loss: -17.1858
[ INFO : 2022-07-27 18:03:56,046 ] - Epoch 200: val_loss improved from -17.1858 to -17.1858, saving model to ./DGCCA.model
[ INFO : 2022-07-27 18:03:56,298 ] - Epoch 200/200 - time: 17.77 - training_loss: -17.2787 - val_loss: -17.1858
[ INFO : 2022-07-27 18:03:59,878 ] - loss on validation data: -17.1858
[ INFO : 2023-02-21 15:10:11,558 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 15:10:11,559 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2023-02-21 15:12:04,362 ] - Epoch 1: val_loss improved from 0.0000 to -15.6226, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:12:04,786 ] - Epoch 1/200 - time: 113.23 - training_loss: -15.3992 - val_loss: -15.6226
[ INFO : 2023-02-21 15:12:31,482 ] - Epoch 2: val_loss improved from -15.6226 to -16.2837, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:12:31,843 ] - Epoch 2/200 - time: 27.06 - training_loss: -15.8375 - val_loss: -16.2837
[ INFO : 2023-02-21 15:12:59,122 ] - Epoch 3: val_loss improved from -16.2837 to -16.6597, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:12:59,417 ] - Epoch 3/200 - time: 27.57 - training_loss: -16.1126 - val_loss: -16.6597
[ INFO : 2023-02-21 15:13:25,769 ] - Epoch 4: val_loss improved from -16.6597 to -16.8550, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:13:26,067 ] - Epoch 4/200 - time: 26.65 - training_loss: -16.3120 - val_loss: -16.8550
[ INFO : 2023-02-21 15:13:52,942 ] - Epoch 5: val_loss improved from -16.8550 to -16.9002, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:13:53,266 ] - Epoch 5/200 - time: 27.20 - training_loss: -16.4569 - val_loss: -16.9002
[ INFO : 2023-02-21 15:14:19,411 ] - Epoch 6: val_loss improved from -16.9002 to -16.9216, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:14:19,770 ] - Epoch 6/200 - time: 26.50 - training_loss: -16.5627 - val_loss: -16.9216
[ INFO : 2023-02-21 15:14:45,557 ] - Epoch 7: val_loss did not improve from -16.9216
[ INFO : 2023-02-21 15:14:45,558 ] - Epoch 7/200 - time: 25.79 - training_loss: -16.6390 - val_loss: -16.8993
[ INFO : 2023-02-21 15:15:11,002 ] - Epoch 8: val_loss improved from -16.9216 to -16.9480, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:15:11,404 ] - Epoch 8/200 - time: 25.85 - training_loss: -16.6987 - val_loss: -16.9480
[ INFO : 2023-02-21 15:15:36,445 ] - Epoch 9: val_loss improved from -16.9480 to -16.9864, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:15:36,791 ] - Epoch 9/200 - time: 25.39 - training_loss: -16.7493 - val_loss: -16.9864
[ INFO : 2023-02-21 15:16:01,784 ] - Epoch 10: val_loss improved from -16.9864 to -16.9952, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:16:02,104 ] - Epoch 10/200 - time: 25.31 - training_loss: -16.7909 - val_loss: -16.9952
[ INFO : 2023-02-21 15:16:27,227 ] - Epoch 11: val_loss improved from -16.9952 to -17.0433, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:16:27,581 ] - Epoch 11/200 - time: 25.48 - training_loss: -16.8270 - val_loss: -17.0433
[ INFO : 2023-02-21 15:16:52,640 ] - Epoch 12: val_loss improved from -17.0433 to -17.0478, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:16:52,989 ] - Epoch 12/200 - time: 25.41 - training_loss: -16.8588 - val_loss: -17.0478
[ INFO : 2023-02-21 15:17:17,998 ] - Epoch 13: val_loss did not improve from -17.0478
[ INFO : 2023-02-21 15:17:17,998 ] - Epoch 13/200 - time: 25.01 - training_loss: -16.8866 - val_loss: -17.0400
[ INFO : 2023-02-21 15:17:43,012 ] - Epoch 14: val_loss improved from -17.0478 to -17.0627, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:17:43,353 ] - Epoch 14/200 - time: 25.36 - training_loss: -16.9108 - val_loss: -17.0627
[ INFO : 2023-02-21 15:18:08,520 ] - Epoch 15: val_loss did not improve from -17.0627
[ INFO : 2023-02-21 15:18:08,520 ] - Epoch 15/200 - time: 25.17 - training_loss: -16.9321 - val_loss: -17.0410
[ INFO : 2023-02-21 15:18:34,824 ] - Epoch 16: val_loss improved from -17.0627 to -17.0705, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:18:35,195 ] - Epoch 16/200 - time: 26.67 - training_loss: -16.9510 - val_loss: -17.0705
[ INFO : 2023-02-21 15:19:00,953 ] - Epoch 17: val_loss did not improve from -17.0705
[ INFO : 2023-02-21 15:19:00,954 ] - Epoch 17/200 - time: 25.76 - training_loss: -16.9680 - val_loss: -17.0689
[ INFO : 2023-02-21 15:19:26,014 ] - Epoch 18: val_loss improved from -17.0705 to -17.0796, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:19:26,313 ] - Epoch 18/200 - time: 25.36 - training_loss: -16.9831 - val_loss: -17.0796
[ INFO : 2023-02-21 15:19:51,343 ] - Epoch 19: val_loss improved from -17.0796 to -17.0903, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:19:51,638 ] - Epoch 19/200 - time: 25.32 - training_loss: -16.9969 - val_loss: -17.0903
[ INFO : 2023-02-21 15:20:16,954 ] - Epoch 20: val_loss did not improve from -17.0903
[ INFO : 2023-02-21 15:20:16,955 ] - Epoch 20/200 - time: 25.32 - training_loss: -17.0094 - val_loss: -17.0746
[ INFO : 2023-02-21 15:21:57,817 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 15:21:57,817 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2023-02-21 15:22:25,784 ] - Epoch 1: val_loss improved from 0.0000 to -15.5885, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:22:26,134 ] - Epoch 1/200 - time: 28.32 - training_loss: -15.3553 - val_loss: -15.5885
[ INFO : 2023-02-21 15:34:37,111 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 15:34:37,112 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2023-02-21 15:35:14,379 ] - Epoch 1: val_loss improved from 0.0000 to -15.5970, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:35:15,993 ] - Epoch 1/20 - time: 38.88 - training_loss: -15.4416 - val_loss: -15.5970
[ INFO : 2023-02-21 15:35:41,230 ] - Epoch 2: val_loss improved from -15.5970 to -16.4098, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:35:41,540 ] - Epoch 2/20 - time: 25.55 - training_loss: -15.8939 - val_loss: -16.4098
[ INFO : 2023-02-21 15:36:06,586 ] - Epoch 3: val_loss improved from -16.4098 to -16.6651, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:36:06,915 ] - Epoch 3/20 - time: 25.37 - training_loss: -16.1691 - val_loss: -16.6651
[ INFO : 2023-02-21 15:36:31,869 ] - Epoch 4: val_loss improved from -16.6651 to -16.8187, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:36:32,178 ] - Epoch 4/20 - time: 25.26 - training_loss: -16.3592 - val_loss: -16.8187
[ INFO : 2023-02-21 15:36:57,197 ] - Epoch 5: val_loss improved from -16.8187 to -16.8770, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:36:57,559 ] - Epoch 5/20 - time: 25.38 - training_loss: -16.4929 - val_loss: -16.8770
[ INFO : 2023-02-21 15:37:22,690 ] - Epoch 6: val_loss improved from -16.8770 to -16.8960, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:37:23,009 ] - Epoch 6/20 - time: 25.45 - training_loss: -16.5898 - val_loss: -16.8960
[ INFO : 2023-02-21 15:37:47,946 ] - Epoch 7: val_loss improved from -16.8960 to -16.9289, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:37:48,288 ] - Epoch 7/20 - time: 25.28 - training_loss: -16.6633 - val_loss: -16.9289
[ INFO : 2023-02-21 15:38:13,255 ] - Epoch 8: val_loss improved from -16.9289 to -16.9367, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:38:13,549 ] - Epoch 8/20 - time: 25.26 - training_loss: -16.7211 - val_loss: -16.9367
[ INFO : 2023-02-21 15:38:38,703 ] - Epoch 9: val_loss improved from -16.9367 to -16.9563, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:38:39,004 ] - Epoch 9/20 - time: 25.45 - training_loss: -16.7692 - val_loss: -16.9563
[ INFO : 2023-02-21 15:39:03,968 ] - Epoch 10: val_loss improved from -16.9563 to -16.9786, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:39:04,293 ] - Epoch 10/20 - time: 25.29 - training_loss: -16.8084 - val_loss: -16.9786
[ INFO : 2023-02-21 15:39:29,225 ] - Epoch 11: val_loss improved from -16.9786 to -17.0189, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:39:29,619 ] - Epoch 11/20 - time: 25.33 - training_loss: -16.8420 - val_loss: -17.0189
[ INFO : 2023-02-21 15:39:54,632 ] - Epoch 12: val_loss improved from -17.0189 to -17.0439, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:39:54,967 ] - Epoch 12/20 - time: 25.35 - training_loss: -16.8723 - val_loss: -17.0439
[ INFO : 2023-02-21 15:40:20,042 ] - Epoch 13: val_loss improved from -17.0439 to -17.0675, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:40:20,374 ] - Epoch 13/20 - time: 25.41 - training_loss: -16.8992 - val_loss: -17.0675
[ INFO : 2023-02-21 15:40:45,338 ] - Epoch 14: val_loss did not improve from -17.0675
[ INFO : 2023-02-21 15:40:45,338 ] - Epoch 14/20 - time: 24.96 - training_loss: -16.9228 - val_loss: -17.0407
[ INFO : 2023-02-21 15:41:10,459 ] - Epoch 15: val_loss did not improve from -17.0675
[ INFO : 2023-02-21 15:41:10,459 ] - Epoch 15/20 - time: 25.12 - training_loss: -16.9435 - val_loss: -17.0489
[ INFO : 2023-02-21 15:41:35,381 ] - Epoch 16: val_loss did not improve from -17.0675
[ INFO : 2023-02-21 15:41:35,381 ] - Epoch 16/20 - time: 24.92 - training_loss: -16.9615 - val_loss: -17.0588
[ INFO : 2023-02-21 15:42:00,339 ] - Epoch 17: val_loss improved from -17.0675 to -17.0688, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:42:00,721 ] - Epoch 17/20 - time: 25.34 - training_loss: -16.9777 - val_loss: -17.0688
[ INFO : 2023-02-21 15:42:25,670 ] - Epoch 18: val_loss improved from -17.0688 to -17.0795, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:42:25,984 ] - Epoch 18/20 - time: 25.26 - training_loss: -16.9923 - val_loss: -17.0795
[ INFO : 2023-02-21 15:42:51,008 ] - Epoch 19: val_loss did not improve from -17.0795
[ INFO : 2023-02-21 15:42:51,009 ] - Epoch 19/20 - time: 25.02 - training_loss: -17.0055 - val_loss: -17.0668
[ INFO : 2023-02-21 15:43:16,042 ] - Epoch 20: val_loss improved from -17.0795 to -17.0823, saving model to ./DGCCA.model
[ INFO : 2023-02-21 15:43:16,395 ] - Epoch 20/20 - time: 25.39 - training_loss: -17.0174 - val_loss: -17.0823
[ INFO : 2023-02-21 15:43:21,463 ] - loss on validation data: -17.0823
[ INFO : 2023-02-21 17:13:51,501 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 17:13:51,502 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2023-02-21 17:15:45,909 ] - Epoch 1: val_loss improved from 0.0000 to -15.6594, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:15:46,211 ] - Epoch 1/20 - time: 114.71 - training_loss: -15.3669 - val_loss: -15.6594
[ INFO : 2023-02-21 17:16:11,109 ] - Epoch 2: val_loss improved from -15.6594 to -16.4220, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:16:11,424 ] - Epoch 2/20 - time: 25.21 - training_loss: -15.8344 - val_loss: -16.4220
[ INFO : 2023-02-21 17:16:36,360 ] - Epoch 3: val_loss improved from -16.4220 to -16.6800, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:16:36,657 ] - Epoch 3/20 - time: 25.23 - training_loss: -16.1214 - val_loss: -16.6800
[ INFO : 2023-02-21 17:17:01,638 ] - Epoch 4: val_loss improved from -16.6800 to -16.7560, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:17:02,077 ] - Epoch 4/20 - time: 25.42 - training_loss: -16.3190 - val_loss: -16.7560
[ INFO : 2023-02-21 17:17:27,007 ] - Epoch 5: val_loss improved from -16.7560 to -16.9081, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:17:27,354 ] - Epoch 5/20 - time: 25.28 - training_loss: -16.4592 - val_loss: -16.9081
[ INFO : 2023-02-21 17:17:52,305 ] - Epoch 6: val_loss improved from -16.9081 to -16.9429, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:17:52,671 ] - Epoch 6/20 - time: 25.32 - training_loss: -16.5642 - val_loss: -16.9429
[ INFO : 2023-02-21 17:18:17,617 ] - Epoch 7: val_loss improved from -16.9429 to -16.9454, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:18:17,934 ] - Epoch 7/20 - time: 25.26 - training_loss: -16.6441 - val_loss: -16.9454
[ INFO : 2023-02-21 17:18:42,809 ] - Epoch 8: val_loss improved from -16.9454 to -16.9463, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:18:43,175 ] - Epoch 8/20 - time: 25.24 - training_loss: -16.7051 - val_loss: -16.9463
[ INFO : 2023-02-21 17:19:08,072 ] - Epoch 9: val_loss improved from -16.9463 to -17.0008, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:19:08,412 ] - Epoch 9/20 - time: 25.24 - training_loss: -16.7547 - val_loss: -17.0008
[ INFO : 2023-02-21 17:19:33,320 ] - Epoch 10: val_loss improved from -17.0008 to -17.0152, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:19:33,663 ] - Epoch 10/20 - time: 25.25 - training_loss: -16.7957 - val_loss: -17.0152
[ INFO : 2023-02-21 17:19:58,523 ] - Epoch 11: val_loss did not improve from -17.0152
[ INFO : 2023-02-21 17:19:58,523 ] - Epoch 11/20 - time: 24.86 - training_loss: -16.8320 - val_loss: -17.0109
[ INFO : 2023-02-21 17:20:23,368 ] - Epoch 12: val_loss improved from -17.0152 to -17.0336, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:20:23,720 ] - Epoch 12/20 - time: 25.20 - training_loss: -16.8631 - val_loss: -17.0336
[ INFO : 2023-02-21 17:20:48,614 ] - Epoch 13: val_loss improved from -17.0336 to -17.0533, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:20:48,972 ] - Epoch 13/20 - time: 25.25 - training_loss: -16.8906 - val_loss: -17.0533
[ INFO : 2023-02-21 17:21:13,838 ] - Epoch 14: val_loss improved from -17.0533 to -17.0680, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:21:14,145 ] - Epoch 14/20 - time: 25.17 - training_loss: -16.9152 - val_loss: -17.0680
[ INFO : 2023-02-21 17:21:38,983 ] - Epoch 15: val_loss did not improve from -17.0680
[ INFO : 2023-02-21 17:21:38,983 ] - Epoch 15/20 - time: 24.84 - training_loss: -16.9368 - val_loss: -17.0652
[ INFO : 2023-02-21 17:22:03,872 ] - Epoch 16: val_loss did not improve from -17.0680
[ INFO : 2023-02-21 17:22:03,872 ] - Epoch 16/20 - time: 24.89 - training_loss: -16.9556 - val_loss: -17.0631
[ INFO : 2023-02-21 17:22:28,723 ] - Epoch 17: val_loss improved from -17.0680 to -17.0773, saving model to ./DGCCA.model
[ INFO : 2023-02-21 17:22:29,070 ] - Epoch 17/20 - time: 25.20 - training_loss: -16.9722 - val_loss: -17.0773
[ INFO : 2023-02-21 17:22:53,957 ] - Epoch 18: val_loss did not improve from -17.0773
[ INFO : 2023-02-21 17:22:53,957 ] - Epoch 18/20 - time: 24.89 - training_loss: -16.9868 - val_loss: -17.0658
[ INFO : 2023-02-21 17:23:18,825 ] - Epoch 19: val_loss did not improve from -17.0773
[ INFO : 2023-02-21 17:23:18,826 ] - Epoch 19/20 - time: 24.87 - training_loss: -16.9993 - val_loss: -17.0328
[ INFO : 2023-02-21 17:23:43,631 ] - Epoch 20: val_loss did not improve from -17.0773
[ INFO : 2023-02-21 17:23:43,631 ] - Epoch 20/20 - time: 24.81 - training_loss: -17.0106 - val_loss: -17.0340
[ INFO : 2023-02-21 17:23:48,649 ] - loss on validation data: -17.0773
[ INFO : 2023-02-21 17:59:11,143 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 17:59:11,144 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
[ INFO : 2023-02-21 18:05:04,105 ] - DeepGCCA(
  (model_list): ModuleList(
    (0): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (1): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
    (2): MlpNet(
      (layers): ModuleList(
        (0): Sequential(
          (0): Linear(in_features=3072, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (1): Sequential(
          (0): Linear(in_features=1024, out_features=1024, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
        (2): Sequential(
          (0): Linear(in_features=1024, out_features=256, bias=True)
          (1): Sigmoid()
          (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
        )
      )
    )
  )
)
[ INFO : 2023-02-21 18:05:04,105 ] - Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    initial_lr: 0.001
    lr: 0.001
    maximize: False
    weight_decay: 1e-05
)
